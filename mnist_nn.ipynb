{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "#60 000 * 28 *28\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 41s 850us/step - loss: 1.3633 - acc: 0.6796 - val_loss: 0.8904 - val_acc: 0.8246\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.7913 - acc: 0.8272 - val_loss: 0.6572 - val_acc: 0.8546\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.6436 - acc: 0.8497 - val_loss: 0.5625 - val_acc: 0.8681\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.5717 - acc: 0.8602 - val_loss: 0.5098 - val_acc: 0.8765\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.5276 - acc: 0.8678 - val_loss: 0.4758 - val_acc: 0.8826\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4973 - acc: 0.8726 - val_loss: 0.4515 - val_acc: 0.8866\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4748 - acc: 0.8775 - val_loss: 0.4333 - val_acc: 0.8882\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4574 - acc: 0.8803 - val_loss: 0.4189 - val_acc: 0.8920\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4433 - acc: 0.8834 - val_loss: 0.4075 - val_acc: 0.8939\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4317 - acc: 0.8850 - val_loss: 0.3977 - val_acc: 0.8966\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4218 - acc: 0.8873 - val_loss: 0.3896 - val_acc: 0.8984\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4134 - acc: 0.8888 - val_loss: 0.3827 - val_acc: 0.8995\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.4060 - acc: 0.8902 - val_loss: 0.3766 - val_acc: 0.9003\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3995 - acc: 0.8918 - val_loss: 0.3712 - val_acc: 0.9013\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3936 - acc: 0.8928 - val_loss: 0.3664 - val_acc: 0.9016\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3884 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.9031\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3837 - acc: 0.8950 - val_loss: 0.3582 - val_acc: 0.9033\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3794 - acc: 0.8962 - val_loss: 0.3546 - val_acc: 0.9039\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3755 - acc: 0.8970 - val_loss: 0.3514 - val_acc: 0.9048\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3718 - acc: 0.8979 - val_loss: 0.3485 - val_acc: 0.9053\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3685 - acc: 0.8985 - val_loss: 0.3457 - val_acc: 0.9058\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3653 - acc: 0.8995 - val_loss: 0.3431 - val_acc: 0.9058\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3625 - acc: 0.8999 - val_loss: 0.3407 - val_acc: 0.9063\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3598 - acc: 0.9008 - val_loss: 0.3385 - val_acc: 0.9070\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3572 - acc: 0.9012 - val_loss: 0.3364 - val_acc: 0.9074\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3548 - acc: 0.9019 - val_loss: 0.3345 - val_acc: 0.9084\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3525 - acc: 0.9022 - val_loss: 0.3326 - val_acc: 0.9082\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3504 - acc: 0.9032 - val_loss: 0.3311 - val_acc: 0.9090\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3484 - acc: 0.9031 - val_loss: 0.3293 - val_acc: 0.9094\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3465 - acc: 0.9041 - val_loss: 0.3277 - val_acc: 0.9097\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3447 - acc: 0.9044 - val_loss: 0.3264 - val_acc: 0.9097\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3430 - acc: 0.9047 - val_loss: 0.3249 - val_acc: 0.9097\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3413 - acc: 0.9051 - val_loss: 0.3235 - val_acc: 0.9103\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3397 - acc: 0.9056 - val_loss: 0.3222 - val_acc: 0.9104\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3382 - acc: 0.9058 - val_loss: 0.3211 - val_acc: 0.9110\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3368 - acc: 0.9062 - val_loss: 0.3198 - val_acc: 0.9110\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3353 - acc: 0.9069 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3340 - acc: 0.9075 - val_loss: 0.3177 - val_acc: 0.9120\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3327 - acc: 0.9075 - val_loss: 0.3166 - val_acc: 0.9122\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3314 - acc: 0.9078 - val_loss: 0.3159 - val_acc: 0.9118\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3303 - acc: 0.9080 - val_loss: 0.3147 - val_acc: 0.9127\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3291 - acc: 0.9084 - val_loss: 0.3138 - val_acc: 0.9132\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3280 - acc: 0.9089 - val_loss: 0.3130 - val_acc: 0.9132\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3270 - acc: 0.9091 - val_loss: 0.3121 - val_acc: 0.9132\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3259 - acc: 0.9093 - val_loss: 0.3113 - val_acc: 0.9135\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3249 - acc: 0.9095 - val_loss: 0.3105 - val_acc: 0.9137\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3239 - acc: 0.9105 - val_loss: 0.3098 - val_acc: 0.9141\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3230 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3221 - acc: 0.9102 - val_loss: 0.3083 - val_acc: 0.9151\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3212 - acc: 0.9109 - val_loss: 0.3075 - val_acc: 0.9150\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3204 - acc: 0.9109 - val_loss: 0.3070 - val_acc: 0.9150\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3195 - acc: 0.9112 - val_loss: 0.3063 - val_acc: 0.9148\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3187 - acc: 0.9114 - val_loss: 0.3057 - val_acc: 0.9153\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3180 - acc: 0.9117 - val_loss: 0.3050 - val_acc: 0.9148\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3171 - acc: 0.9121 - val_loss: 0.3044 - val_acc: 0.9149\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3164 - acc: 0.9121 - val_loss: 0.3037 - val_acc: 0.9156\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3157 - acc: 0.9128 - val_loss: 0.3034 - val_acc: 0.9152\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3149 - acc: 0.9121 - val_loss: 0.3029 - val_acc: 0.9148\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3143 - acc: 0.9128 - val_loss: 0.3022 - val_acc: 0.9151\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3136 - acc: 0.9129 - val_loss: 0.3016 - val_acc: 0.9161\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3130 - acc: 0.9133 - val_loss: 0.3011 - val_acc: 0.9158\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3123 - acc: 0.9131 - val_loss: 0.3007 - val_acc: 0.9151\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3117 - acc: 0.9136 - val_loss: 0.3003 - val_acc: 0.9156\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3110 - acc: 0.9137 - val_loss: 0.2997 - val_acc: 0.9158\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3105 - acc: 0.9137 - val_loss: 0.2992 - val_acc: 0.9159\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3098 - acc: 0.9138 - val_loss: 0.2988 - val_acc: 0.9161\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3093 - acc: 0.9141 - val_loss: 0.2983 - val_acc: 0.9165\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3087 - acc: 0.9139 - val_loss: 0.2979 - val_acc: 0.9166\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3082 - acc: 0.9144 - val_loss: 0.2976 - val_acc: 0.9164\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3077 - acc: 0.9145 - val_loss: 0.2971 - val_acc: 0.9166\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3071 - acc: 0.9146 - val_loss: 0.2967 - val_acc: 0.9172\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3066 - acc: 0.9147 - val_loss: 0.2964 - val_acc: 0.9167\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3061 - acc: 0.9151 - val_loss: 0.2960 - val_acc: 0.9169\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3056 - acc: 0.9150 - val_loss: 0.2956 - val_acc: 0.9173\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3051 - acc: 0.9151 - val_loss: 0.2952 - val_acc: 0.9177\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3046 - acc: 0.9152 - val_loss: 0.2950 - val_acc: 0.9173\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3042 - acc: 0.9154 - val_loss: 0.2945 - val_acc: 0.9172\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3037 - acc: 0.9154 - val_loss: 0.2942 - val_acc: 0.9176\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3032 - acc: 0.9157 - val_loss: 0.2939 - val_acc: 0.9179\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3028 - acc: 0.9156 - val_loss: 0.2936 - val_acc: 0.9177\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3024 - acc: 0.9157 - val_loss: 0.2933 - val_acc: 0.9179\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3019 - acc: 0.9157 - val_loss: 0.2930 - val_acc: 0.9178\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.3015 - acc: 0.9160 - val_loss: 0.2926 - val_acc: 0.9182\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3011 - acc: 0.9161 - val_loss: 0.2924 - val_acc: 0.9179\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3007 - acc: 0.9165 - val_loss: 0.2920 - val_acc: 0.9184\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3003 - acc: 0.9164 - val_loss: 0.2918 - val_acc: 0.9185\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2999 - acc: 0.9165 - val_loss: 0.2914 - val_acc: 0.9185\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2995 - acc: 0.9166 - val_loss: 0.2911 - val_acc: 0.9188\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2991 - acc: 0.9167 - val_loss: 0.2909 - val_acc: 0.9191\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2988 - acc: 0.9169 - val_loss: 0.2906 - val_acc: 0.9191\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2984 - acc: 0.9168 - val_loss: 0.2903 - val_acc: 0.9192\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2981 - acc: 0.9170 - val_loss: 0.2901 - val_acc: 0.9196\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2977 - acc: 0.9171 - val_loss: 0.2898 - val_acc: 0.9195\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2973 - acc: 0.9172 - val_loss: 0.2895 - val_acc: 0.9196\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2970 - acc: 0.9174 - val_loss: 0.2894 - val_acc: 0.9196\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2967 - acc: 0.9174 - val_loss: 0.2891 - val_acc: 0.9198\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2963 - acc: 0.9176 - val_loss: 0.2889 - val_acc: 0.9197\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2960 - acc: 0.9174 - val_loss: 0.2886 - val_acc: 0.9202\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2957 - acc: 0.9176 - val_loss: 0.2884 - val_acc: 0.9202\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2953 - acc: 0.9178 - val_loss: 0.2882 - val_acc: 0.9200\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2950 - acc: 0.9179 - val_loss: 0.2879 - val_acc: 0.9201\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2947 - acc: 0.9180 - val_loss: 0.2877 - val_acc: 0.9204\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2944 - acc: 0.9180 - val_loss: 0.2875 - val_acc: 0.9202\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2941 - acc: 0.9184 - val_loss: 0.2873 - val_acc: 0.9202\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2938 - acc: 0.9183 - val_loss: 0.2871 - val_acc: 0.9206\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2935 - acc: 0.9183 - val_loss: 0.2868 - val_acc: 0.9202\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2932 - acc: 0.9186 - val_loss: 0.2867 - val_acc: 0.9206\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2929 - acc: 0.9185 - val_loss: 0.2864 - val_acc: 0.9208\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2927 - acc: 0.9185 - val_loss: 0.2863 - val_acc: 0.9206\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2923 - acc: 0.9187 - val_loss: 0.2860 - val_acc: 0.9204\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2921 - acc: 0.9184 - val_loss: 0.2858 - val_acc: 0.9210\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2918 - acc: 0.9187 - val_loss: 0.2857 - val_acc: 0.9207\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2915 - acc: 0.9189 - val_loss: 0.2854 - val_acc: 0.9210\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2913 - acc: 0.9188 - val_loss: 0.2853 - val_acc: 0.9211\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2910 - acc: 0.9189 - val_loss: 0.2852 - val_acc: 0.9205\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2908 - acc: 0.9189 - val_loss: 0.2849 - val_acc: 0.9213\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2905 - acc: 0.9193 - val_loss: 0.2847 - val_acc: 0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2902 - acc: 0.9192 - val_loss: 0.2846 - val_acc: 0.9212\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2900 - acc: 0.9191 - val_loss: 0.2844 - val_acc: 0.9212\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2842 - val_acc: 0.9212\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2895 - acc: 0.9191 - val_loss: 0.2841 - val_acc: 0.9212\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2892 - acc: 0.9192 - val_loss: 0.2840 - val_acc: 0.9212\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2890 - acc: 0.9194 - val_loss: 0.2838 - val_acc: 0.9211\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2888 - acc: 0.9197 - val_loss: 0.2837 - val_acc: 0.9210\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2885 - acc: 0.9193 - val_loss: 0.2835 - val_acc: 0.9207\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2883 - acc: 0.9197 - val_loss: 0.2834 - val_acc: 0.9217\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2881 - acc: 0.9194 - val_loss: 0.2832 - val_acc: 0.9212\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2879 - acc: 0.9194 - val_loss: 0.2830 - val_acc: 0.9210\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2876 - acc: 0.9196 - val_loss: 0.2828 - val_acc: 0.9217\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2874 - acc: 0.9197 - val_loss: 0.2826 - val_acc: 0.9216\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2871 - acc: 0.9200 - val_loss: 0.2827 - val_acc: 0.9211\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2870 - acc: 0.9197 - val_loss: 0.2824 - val_acc: 0.9213\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2868 - acc: 0.9198 - val_loss: 0.2823 - val_acc: 0.9216\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2866 - acc: 0.9199 - val_loss: 0.2822 - val_acc: 0.9214\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2863 - acc: 0.9203 - val_loss: 0.2820 - val_acc: 0.9213\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2861 - acc: 0.9196 - val_loss: 0.2818 - val_acc: 0.9215\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2859 - acc: 0.9198 - val_loss: 0.2818 - val_acc: 0.9217\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2857 - acc: 0.9203 - val_loss: 0.2815 - val_acc: 0.9218\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2855 - acc: 0.9203 - val_loss: 0.2814 - val_acc: 0.9215\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2853 - acc: 0.9201 - val_loss: 0.2812 - val_acc: 0.9216\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2852 - acc: 0.9204 - val_loss: 0.2811 - val_acc: 0.9217\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2849 - acc: 0.9201 - val_loss: 0.2810 - val_acc: 0.9217\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2848 - acc: 0.9205 - val_loss: 0.2809 - val_acc: 0.9219\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2846 - acc: 0.9208 - val_loss: 0.2808 - val_acc: 0.9217\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2844 - acc: 0.9207 - val_loss: 0.2806 - val_acc: 0.9221\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2841 - acc: 0.9206 - val_loss: 0.2806 - val_acc: 0.9220\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2840 - acc: 0.9207 - val_loss: 0.2804 - val_acc: 0.9217\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2838 - acc: 0.9209 - val_loss: 0.2803 - val_acc: 0.9218\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2836 - acc: 0.9208 - val_loss: 0.2802 - val_acc: 0.9216\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2835 - acc: 0.9210 - val_loss: 0.2800 - val_acc: 0.9225\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2833 - acc: 0.9210 - val_loss: 0.2799 - val_acc: 0.9226\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2831 - acc: 0.9211 - val_loss: 0.2798 - val_acc: 0.9222\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2829 - acc: 0.9207 - val_loss: 0.2797 - val_acc: 0.9224\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2827 - acc: 0.9209 - val_loss: 0.2796 - val_acc: 0.9222\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2826 - acc: 0.9208 - val_loss: 0.2795 - val_acc: 0.9225\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2824 - acc: 0.9210 - val_loss: 0.2794 - val_acc: 0.9224\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2822 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9224\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2821 - acc: 0.9214 - val_loss: 0.2792 - val_acc: 0.9226\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2819 - acc: 0.9214 - val_loss: 0.2791 - val_acc: 0.9226\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2817 - acc: 0.9213 - val_loss: 0.2790 - val_acc: 0.9225\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2816 - acc: 0.9214 - val_loss: 0.2789 - val_acc: 0.9222\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2788 - val_acc: 0.9227\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2812 - acc: 0.9213 - val_loss: 0.2787 - val_acc: 0.9225\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2811 - acc: 0.9216 - val_loss: 0.2786 - val_acc: 0.9225\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2785 - val_acc: 0.9227\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2807 - acc: 0.9216 - val_loss: 0.2784 - val_acc: 0.9225\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2806 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9227\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2804 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9228\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2803 - acc: 0.9216 - val_loss: 0.2782 - val_acc: 0.9227\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2781 - val_acc: 0.9227\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2800 - acc: 0.9220 - val_loss: 0.2780 - val_acc: 0.9226\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2798 - acc: 0.9218 - val_loss: 0.2778 - val_acc: 0.9231\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2797 - acc: 0.9217 - val_loss: 0.2778 - val_acc: 0.9229\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2796 - acc: 0.9217 - val_loss: 0.2777 - val_acc: 0.9227\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2794 - acc: 0.9218 - val_loss: 0.2776 - val_acc: 0.9232\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2793 - acc: 0.9220 - val_loss: 0.2775 - val_acc: 0.9232\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2791 - acc: 0.9219 - val_loss: 0.2774 - val_acc: 0.9234\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2790 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9228\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2788 - acc: 0.9221 - val_loss: 0.2773 - val_acc: 0.9232\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2787 - acc: 0.9221 - val_loss: 0.2771 - val_acc: 0.9235\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2770 - val_acc: 0.9232\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2784 - acc: 0.9220 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2783 - acc: 0.9223 - val_loss: 0.2769 - val_acc: 0.9231\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2781 - acc: 0.9223 - val_loss: 0.2768 - val_acc: 0.9230\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9233\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2779 - acc: 0.9223 - val_loss: 0.2766 - val_acc: 0.9236\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2777 - acc: 0.9224 - val_loss: 0.2766 - val_acc: 0.9233\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2776 - acc: 0.9226 - val_loss: 0.2765 - val_acc: 0.9236\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2775 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2773 - acc: 0.9225 - val_loss: 0.2764 - val_acc: 0.9235\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2772 - acc: 0.9225 - val_loss: 0.2763 - val_acc: 0.9237\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2762 - val_acc: 0.9238\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2770 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9237\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2768 - acc: 0.9226 - val_loss: 0.2761 - val_acc: 0.9236\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2767 - acc: 0.9231 - val_loss: 0.2760 - val_acc: 0.9239\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2766 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9241\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2765 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9242\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2763 - acc: 0.9231 - val_loss: 0.2758 - val_acc: 0.9236\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.2762 - acc: 0.9229 - val_loss: 0.2757 - val_acc: 0.9241\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2761 - acc: 0.9230 - val_loss: 0.2756 - val_acc: 0.9241\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = NB_EPOCH, verbose = VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def training_vis(hist):\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    acc = hist.history['acc']\n",
    "    val_acc = hist.history['val_acc']\n",
    "\n",
    "    # make a figure\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    # subplot loss\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(loss,label='train_loss')\n",
    "    ax1.plot(val_loss,label='val_loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss on Training and Validation Data')\n",
    "    ax1.legend()\n",
    "    # subplot acc\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(acc,label='train_acc')\n",
    "    ax2.plot(val_acc,label='val_acc')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracy  on Training and Validation Data')\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAEYCAYAAABBWFftAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3XecnGW99/HPb8r2mt30XknoaOiKFEWqUUAERAgKHEQRURT0USxHPR6PR8/xAeFBpTdBRFFRVDSi1CQaIEAaIWVTN9v77sxczx/Xvclksy3Jzs5O5vt+vea1c/fflL2v+d1Xuc05h4iIiIiIiOy/ULoDEBEREREROVAowRIRERERERkiSrBERERERESGiBIsERERERGRIaIES0REREREZIgowRIRERERERkiSrAk65jZH83so0O9bjqZWZWZnZzqfZvZV83sjlTEYWYnm9nr+xaliIj0ZaBz976um05m9oCZfT3V+x6obNqfOMwsbGbNZjZl3yKVkUoJ1gHOzNaZ2XvTHce+MrPfByefZjPrMrPOpOl9KgCcc6c75x4c6nVHoqCg/Esv88cG7+fcvdmfc+7fnXPXDEFcETNzZjYtad+LnHOH7O++eznWrOBY3d+brWb2GzM7bS/2caWZLRrq2EQOZGa2yMzqzCw33bFkEjP7ctL5qt3M4knT+3QRam/O3UN1nk8XM3u3mTWZWUEvy14zs716bUNZNpnZP8xsYdK+4865IufchqHYf49jVZlZW/C9qTez58zsajOzQW4/y8x0s9x9pARLRjTn3JnByacIeBD4Xvd0bwWAmUWGP8oR7T7gpF6ujl0M/NM5tyINMaVF0vfoKOAvwJNmdmmawxI5IAUXT94NOOADw3zsjC4HnHPfSTpfXQO8kFTu7fFDP9Nf71Bzzv0d2AaclzzfzI4EZgM/T0dcaXJm8D2aBvwX8GXgzrRGlCWUYGUxM7vKzNaYWa2ZPWlmE4L5ZmY/NLPtZtZgZq+a2aHBsrPM7I3g6tAmM7uxj32HzOwrZrY+2M99ZlYaLJsW1ChcbmYbzGyHmf2ffXwN7w1q6b5sZluBn5hZhZk9ZWbVwdXT35jZxKRtdl5BCmom/ha83nozW2tmp+/jujOD9ZvMNy283czu6SPuwcT4DTN7PtjfH8xsVNLyhcF7u8PMbu7r/XHOrQeeBXomEpcB9wb7mm1mfzWzmmB/93d/Vr3E/a3k19RfHGZ2vJm9GLxXW8zsR2YWDRY/G/x9Pbi6dn73Z5m0/SHB+11v/qrj2UnLHgj29/vg/XnBzKb39T70eE+2OOd+CPw78D0zfzUv+L6uDfb3upl9IJh/GHAr8O4g1h3B/A+Y2bJg/Q1m9tXBHF8kS1wGvAjcA1yevMDM8s3sv4NzR0NwvssPlr0rOO/Vm9nGpPPvIjO7MmkfC83sH0nTzsw+ZWargdXBvP8N9tFoZkvN7N1J64eDcuOt4H94qZlNNrPbzOy/e8T7GzP77GBedHBeeD2If5GZzUtats7MbjRfpjaY2c/NLG+wb2jSfrpbAFxrZmuAFcH8W83XWjSa2WIzOyFpm53nbttVq39ZsH518vl7L9ctCM7H9eZ/G9ycfB7vJfaBYnw42F+TmS03s3ckLX9n0jn3YaC/mtH78N/BZJcBTzrn6sz/RvmF+RYNe3xWPWLuWTb1GYf1U7ab2X8CxwN3BGXJ/1iP1hxmVha8/urg+/Ils51lVL+/QfrjnKt3zv0Kf3H1Exa0XrH+y7Fng3W6a0+Ptr34vZDtlGBlKTM7FfgP4EJgPLAeeCRYfDpwEjAHKAM+AtQEy34G/Jtzrhg4FF8T0JuFweMUYAZQhP+RmuxdwEHAacAtfZ3cBmFSsP8pwLX47/VPgumpQBfwv/1sfwLwGlAB/BD/Gvdl3YeB54Jl32LPpCbZYGK8BP/DZCxQCHwOdvvBfwkwEZgAjOvnWPeSVNCY2SHAIez6vC2IdzxwMP7zGjBZGEQcMeB6oBI4ETgD+Ldg2UnB30OCq7KP99h3DvBb4HfAaOAG4OdmNitptUuCOEcBG/AJ0974Jf41d+9zVRBnKfBt4CEzG+ucew34NPD3INbKYP1m/GdcCpwLXG9m5+xlDCIHqsvwrQ4eBN5vZmOTln0feCf+fDoK+CKQMF/T/nvg/+L/748Elu3FMT8IHIs/jwEsDvYxCngIeCwpofkc/sfmWUAJ8HGgFX++vNjMQgBmVokvox4e6OBmNidY77NB/E8BvwnOZ90uxJ8LpwOH48vJffUB4GjgsGD6pWCfo4Bf4F9vf0nICfjz3/uBb5jZ7H1Y95v4c/+0YNlArQIGivGDwP343x6/B34EEKzza+CuYNtfB+v25T7glKTkJoz/vO9LWue3+BqtccDy4Lj9GkQcfZbtzrmbgBeAa4KypLek/cdAAb4cPhX4BLsninvze2UPzrkXgK342mXovxw7Kdimu/Z0Mfv4eyErOef0OIAfwDrgvb3M/xm+uV33dBH+RDAN/0+9CjgOCPXYbgP+R3LJAMd9Brg2afqgYP+R4BgOmJS0/GXgogH2eQ/wrR7z3gu0Azn9bDcfqE6a/gewMHh+JbAiaVlJEFvl3qyLP8l0APlJyx8B7hnk59RbjDcnTX8G+G3w/JvAAz0+uzhwch/7LsKfRI8Jpv8TeLyfWC4AFidNV3XvG39ivWcf47gReCx4Hgneu2k9Pst1wfNTgE2AJS1/DPhK8PwB4I6kZR8Alvdx3FmA6+N9ccCxfWy3HDg76bNfNMBneCvwX4P939RDjwP1gb941pV0Hl0B3BA8DwFtwBG9bPcl4Ik+9rkIuDJpeiHwj6RpB5w6QFx13ccFVgIL+ljvTeB9wfNPA08N8nV/FXg0aToUnMdODqbXAZcmLf9e8nmsj33u9jqDed3nz5P62c6AJvxFrJ7n7lnB9uOS1v8ncME+rLsBOC1p2TXd5/FBvF+9xfiHpOWHA83B81OBjT3KhJeBr/ez/0XAF4PnZ+KbDUb6WLcyeJ2FwfQD3ftm97Jpr+Kgn98fPT7LaUAUf2FyTtLyTwF/Dp73+3ull2PvLLt7zF8C3NTHNjvLMfooO3usv9vvBT12PVSDlb0m4GutAHDONeNrqSY65/6C/ye7DdhmZneaWUmw6vn4K37rg6rq4wez/+B5BF8b021r0vNW/A/efbHNOdfZPWFmhWb206C6uxFfy1bZ9+Z7xEE/sfS17gSgxjnXlrR8Y18HHGSMfb0/E5L3HXx2tX0dK1j+OHBZcFX2EoLmgUEs48zsUfNNPhvxiWx/71e3fuMws7lm9rugCUYjPiEbzH67973BBWfwwHp8TVm3/f3+dO+rNoh3oZm9EjS9qAfm9hev+SaQi4KmHA34wm+wr0/kQHY58Efn3I5g+iF2NROsBPKAt3rZbnIf8wdrt3OumX3ezN403xyvHn+Vvvt/tL9j3cuumphLGUTNRqBnuZoIYhrK81aynq/3i2a2Ijgf1eFbPvR5TnLODTqWftYd3yOOPsu9QcbY8ziFwfMJQFUvZUJ/kltvfAx40DkXC+IIm9n3gmZ2jcCaYL2BzuH9xrEPvz+SjQHC7Pnbqb/vD+xb2ddd7u1VObYfvxeyjhKs7LUZX30N+JMCvsp5E4Bz7kfOuXfim5LNAb4QzF/snFuAPxH8Cnh0MPvHV5fH8FeQhlrPUW6+iG9+cYxzrgR/xSnVtgAVtnt7+sn9rL8/MW5J3reZFeGbKvTnXuAifBOOPHzTi27/ia99OyyIZSH+yuL+xvH/8LVAs4L93pK034FGJtoMTO5uex6YQvD9HCIfwhdWa8xsBnA78EmgwjlXhr/q3l+8j+AT18nOuVLgpwzufRM5YJnvS3Uh8J7g4spWfBPfI8zsCGAHvtXBzF4239jHfIAWfNOpbr01i975f2q+v9VNQSzlwf90A7v+R/s71gPAgiDeefiybjB6lquGP0cO5XkrWfLrPQXf7PF8fPO6cnzLhVSfk7bim+l367Pc288Yt/Q4DvgyoT+PAdPN7D3AAnZvHngZ/mLxqfjEu7up+ECxDBTHQGV7f2XfdnwrkJ6/nYbs+2Nmx+EvdHf3X+yvHOst1n39vZB1lGBlh6iZ5SU9IvgrileY2ZFBm+LvAC8559YFHRmPNT8gQQu+MIybWY6ZfdTMSp1zXUAj/mTQm4eBG8xsevDD+zvAz7uvHqVYMf7KTp2ZVeB/2KeUc+4tfLvorwXv07uAs/vZZH9ifAxf+B8ffHbfYuCE5a/4z/J24KHg80uOpQVoMLPJ+KZ8QxFHMf4HTYv5/nXd/a9wzsXxNaYz+tj38/iE/PNmFjXfZ/As+k7oB838EPWfAb6Cbybh2NVcsNqvYlfia7C6bQMm2a5BOrpfX61zrj0otC7a39hEDgAfxJcLB+P7Px2JT1L+DlwW1OrcBfzAzCYENQnd55AHgfea2YXmO/9XmB/5DXxfrPPMD6owC983pT/F+HNINRAxs1vwTaq6/RT4d/Od9s3MDg/OxTjnqvD9t+7HN6duY3AeBc42s9OCc8Xn8T9Gnx/k9vuj+/XuwDc1+zq7an9S6VHgy+YHZ5iEb9LWl/2J8R9AyMw+HXw3Pgy8o78NglYVv8RfYFzjnEvuz1eM/2xq8In7t4cojoHK9m30Ue4F5fIvgO+YWZH5gZtuwCf8+8XMSs0P3PQQvvnnm0nx9lWObQdccAGSpPX35fdC1lGClR2ewrd573583Tn3DL69+OP4KzIz2fWPVYLvpFmHr56uwXdKBl/Nvi6oGr6Gvju03oUvnJ4F3sYnadcN6avq2w/wV6Rq8AXb7/tffchcjO8UWgN8DT8UbEcf6+5zjM65V/GDRzyKv7K1ld2bDfS2jcN/HlPZ/SoeQazH4JOhJ/HfiaGI4/P4ZkFN+NqsnkPjfg0/kES9me02nK5zrgPf4XYBvjD+EXCJc27VYGLrjQUjIQGv4mvyznPO3Zf0Wn6Eb0u/BZ9cvZS0+Z/wI5NtC67Ig6/t+g8za8IPfbvfyZ/IAeBy4G7n3Abn3NbuB77Z+UeDC3w34i9ILcY3VfpPfH/fDfgLKZ8P5i8Djgj2+0OgE/8D9V58Mtafp/Hn1VX4cqyd3Zuv/QD/P/tH/MXCnwH5ScvvxQ8eMdjmgTjnVuLLxP+LP2+dC5yb3IQ9hZ4C/ow/T63Dv6Ytw3Dcr+E/k3X49/JR+i739jnGoEz4EHAV/rfJeQyuZvFeei/37sbXOG4GXmeQSfAg4hiobP8f/CAq9Wb2g14OcS3+e/428Lcg/p6x743fB+XeBuBm/FDtVyYt77Mcc8414QdDeymIdz77+HshG9nuzUhFZKiY2ePAMufc3o5uJyIiaWRmJ+FrDqYFtW4yCGZ2HfBB59ygb+QuciBSDZbIEDGzY4ImkSEzOws4Bz+Eq4iIZIiged/1wE+VXPXPzCaa2QlBuTcP36TtiXTHJZJuuvu3yNCZgK8uH4UfHvWqoOmZiIhkgCBJWAK8AlyR5nAyQS6+S8E0fJO5h/FNwkWympoIioiIiIiIDBE1ERQRERERERkiKWsiaGZ34fugbHfOHdrPekcDLwIfcc79YqD9VlZWumnTpg1ZnCIikh5Lly7d4Zwbne44Uk3llojIgWGw5VYq+2Ddgx+Wtc/hJc0sjB+e9enB7nTatGksWbJkv4MTEZH0MrP16Y5hOKjcEhE5MAy23EpZE0Hn3LP4+1j05zr8oADbUxWHiIiIiIjIcElbHywzm4i/Wdsd6YpBRERERERkKKVzkIv/AW5yzsUHWtHMrjazJWa2pLq6ehhCExERERER2XvpvA/WfOARMwOoBM4ys5hz7lc9V3TO3QncCTB//nyNKy8iw6Krq4uqqira29vTHUpGy8vLY9KkSUSj0XSHIiIiknJpS7Ccc9O7n5vZPcBve0uuRETSpaqqiuLiYqZNm0ZwMUj2knOOmpoaqqqqmD59+sAbiIiIZLhUDtP+MHAyUGlmVcDXgCiAc079rkRkxGtvb1dytZ/MjIqKCtS8W0REskXKEizn3MV7se7CVMUhIrI/lFztP72HIiKSTdLZBystXlpbw47mTs4+fHy6QxERERERkcGKdUB7A0TyIKcIQiGId0HdemitgY5GvzyvDKYeD+2NsPVVSMQgFIU5pw9LmFmXYP188UYWr69VgiUiIiIiI1NXO3S1Qm4xhHsZICiR8H9DoV3THQ3gHIQifptwrl+eiENDFUQLIK8EMGivh8bNkF8G+eXQtBVadkBXG4Qjft1ovv8byYOaNbBlmU9qisdB0Tho3gZVL0NLtU9q2hshrxRmnAwuAdUrYcdK6GiGyjlQWAmRXAjn+EckF9rqoXatf7TsgOJxuFCERH0VJLogFMXCEXBxrLUW62za+RY4DJdbjHW2Yi6251tkEUJJ8zujJeT8n41D9xn1I+sSrHDIiMc1EKGIjHz19fU89NBDXHvttXu13VlnncVDDz1EWVnZXm23cOFCzjnnHC644IK92k5EZNi0N4KL+x/90fy9375xM2x4ASYdDWVT/Lzmatj4kk9MikbDuMP98/Z6qN/o/+YWQ26Jf3Q2QUuNrzFp3QGttT55KJ8G8U7obPFJRWdT8LfZJz6lE33825b76dwiKJnkk56uNp/sYLD0btj8Lx9bKAKlkyEWJFz5o/wxmrb4JMbCPpmKd/n3JYkLRUkUj8daawh1tezX296XuEVoiZTTFi6kI1xESeeblL35JABtVsCmyGRaLZ/xW/5MEc3kuC7C7IozToitjKbKxlHLNCoaa7FEjKrEZDpclKjFiBIngVHr5lHjSmigkDw6KbZWSmKttJHLW4kJVFNKkyugkQLGWy0nhF6nxhWzLDGLdnLJI8rjKXkX9pR1CVYkbHQllGCJyMhXX1/Pj3/84z0SrHg8Tjgc7nO7p556KtWhiUg2a6vzCUaiy9dm5JX5BKFhI9StC2pKSv26zdt97URhJRSP9zUptWv9epFcP694HHQ0wdpFYOYTlcbNUL/BJxXF42HKcfDWX2HTkl1xRAvAgiZiia5d81zCJyS5Jf64BRX+EYrAqj/4BAWgcIyPs/Ytv023SD6Ewj4xGiIJQoTwx6iLjiUeipIfb6YwVr/HupvCk3gm9xLaQoWUxOsY27iFSF4xOUUFhNvr6HQRGkpPpjUGXV2dhF2M5oSxqbOAOCEixIkSp9haGV9bQ6M7iDfdVHLoopg2/7GQz1ZXTom1Uk4TW90oaiihK5SHi8fJtw7y6SCfTkoinewIVfJc+wxy6WKs1THW6mh0BbziZuLCeRTlRYh3OKIh47DiGogWUB8ehZkRDYcozY9S39bFpro2xhVHmFwaJd+6iEXyIZRDyIxQyAiHoDAnwoSyfKLhEM3xBF3xBB2xBLG4oyBsVETD5OeEiYSM9q4404py+eDkMjbXt7G2uoXRxbmML8tjfEk+nfEE9a2d5ERC5Of0XW4OtaxLsMIhI64ES0T20jd+8zpvbG4c0n0ePKGEr517SJ/Lb775Zt566y2OPPJIotEoRUVFjB8/nmXLlvHGG2/wwQ9+kI0bN9Le3s7111/P1VdfDcC0adNYsmQJzc3NnHnmmbzrXe/i+eefZ+LEifz6178mP3/gq77PPPMMN954I7FYjKOPPprbb7+d3Nxcbr75Zp588kkikQinn3463//+93nsscf4xje+QTgcprS0lGeffXbI3iMRGWKJBDRv9YmOS/jEqLUGat/2SU8o5BORjmaf9HQ2+34tsU7flKy1xjf72l/F432i01qza17lHN90bP0LuJIJdJVMIZRbSKhmDaFn/4vYqDk0HHMj29pz6GhrJj/WQNgcFo5COIeQQTjehrMwcYsSb6sn0VKD7dhBUWIlxbSwouws/pJzMofyFlPjGyhINFI17l28YEfR6cIUtm5iQtMrRID2kolsoZJtXfm4jhboaKCQNmLhAsoqx7MtXsQb9VE2deQx2uqZZNV0uByayaOVPJpdHi3k00YOBszIbSQnv5DaRDFdsQSd8QSheDul4U7KSkoodk3kxJpoLZlDaWEu8YQjGg4RCRtrtjezeUcblcW55EXDdMUTlBVFKS/IIRwyinIjTB9dSGl+lJAZLR0xOuMJGqJhCnIjHB8J0dQeozOWYMqoAsoKdjU7DIWMisIcKopyKcwJ09IZZ3N9G/GEoyg3wsSyfEIhoyueoL61i+aOGPGEIy8aorwgh4Kc8IgYzGhCWT7zp43aY/7o4txhjyXrEqxIKEQsnhh4RRGRNPvud7/L8uXLWbZsGYsWLeLss89m+fLlO+8ndddddzFq1Cja2to4+uijOf/886moqNhtH6tXr+bhhx/mJz/5CRdeeCGPP/44l156ab/HbW9vZ+HChTzzzDPMmTOHyy67jNtvv53LLruMJ554ghUrVmBm1Nf7K6/f/OY3efrpp5k4ceLOeSKyH5zzfWIKKiCSs2t+Iu6ToMZNvvYlmgcNm4IO/GHY/qbvFzNqpm/WVrXYJ0nxLr9OdyIVa+v9uEXjguPEfPO13GLIKfbzw1Fcez2UT8WO+AiUTqYzAU3VVXQ019EVyqU1bwxN+VNIdLbR0VLHloZ2auKFNBRMpixRT37HDl5tLmZVRwUukk80L0RuYReh1mpiCSMndxLtXXE20862Te24ql2hFdJGy+Y82Lx3P+RDBpNHFdCVSLC5oZ3i9ggTy/N5rGkSNS2+JqswJ8zE8nxyI2HyC2YyddIZdMQSbKpvIy/qa19K86OMKc5jYlk+r1TV8/zmRsqLo8yfns/CCaVUFOUSDfvamkjIiISNupYutja2M7EsnznjiplQmjciEpGBFOVGmDO2eI/50XCI0cW5aUlYMk0WJliqwRKRvddfTdNwOeaYY3a7We+PfvQjnnjiCQA2btzI6tWr90iwpk+fzpFHHgnAO9/5TtatWzfgcVauXMn06dOZM2cOAJdffjm33XYbn/70p8nLy+PKK6/k7LPP5pxzzgHgxBNPZOHChVx44YWcd955Q/FSRQ4cnS0+Wepq3dV0rr3B97FZ9bRPkibO90lN/QY/kMCWV3wzvHCOr9XJLfZ9d2rWQLxjgAMa4H/nxIon4gpGE4pESViErsJJtI49kaaCyTTkjKM1HqIlFqKWErZbJbWxHFo6YjR3xGjuiNPc3kVLQzyY9o9IyBi7KY+WjliQoBzU4/hxIAcYS2FOmLKCHDrjCbriRYSsmBmVhUwZm0tX3NEZT5BIRCgtmwkGW+rbyI2EedfsSiaW5VNeEKWtK0HCOXIjIXKjYYpyw8weU8yYklxicUdX0ISsM+Z2+31nBmUFUUYX55Ib8U3DWjtj5Ed31bY45+iIJciNhPYq8bnw6MmDXleyU9YlWOGwEVOCJSIZqLCwcOfzRYsW8ec//5kXXniBgoICTj75ZNrb2/fYJjd315XGcDhMW1sfV66TONf7OTISifDyyy/zzDPP8Mgjj3Drrbfyl7/8hTvuuIOXXnqJ3/3udxx55JEsW7Zsj0RPZMTqbIW2WiiZ6H+VA9S85Yd2rpjlE5xI8H/UuBleuM0nQCUTfb+guef4WqUty3wfpJYaX3vU0QhN26Bm9e79e5IVVPq+Q/+8z0+HojD2YJj3ARh7CIn6KuLVK0l0tBDLH0fzvBNpKJpOfXQcieZq4h0t7AiNJmZRwq6LlfHxvNVaSF7zBlbWJlhZXTLYNwGzzRTlRCjMjVCU5/8W50YYXZxLUW6UotwwRXkRuuKOrQ3tFOZGmFiWx/jSfCqKcsiNhMmJGDnhMDmREMV5EcaPsBqbgpzdf/aaGXnR4euXI9kj6xIs1WCJSKYoLi6mqamp12UNDQ2Ul5dTUFDAihUrePHFF4fsuHPnzmXdunWsWbOGWbNmcf/99/Oe97yH5uZmWltbOeusszjuuOOYNWsWAG+99RbHHnssxx57LL/5zW/YuHGjEixJv+btvoncxpdhw4t+5LZQj+GnDd+sLhHzzeBKJvhkq27drv2EIj7RirX7EeUAJhwJb/8NXn0EfvvZpHWjUDja11LlleAqZtI08xxai6ZQWFhEbX0D1Tu2UxvLZyPjeJVZ1LZ0EW3ayNY2Y0tnATk1ubgd0LwkRmvnJOC4Pl7gpD3mjCp0jC+NUZI3lcMPzufSyX4k0eb2GIW5YYpydyVO3YlUUa5/5EfDhEIjJxkSyWRZl2CFQyFiCYdzbkRdVRER6amiooITTzyRQw89lPz8fMaOHbtz2RlnnMEdd9zB4YcfzkEHHcRxx/X1I2zv5eXlcffdd/PhD3945yAX11xzDbW1tSxYsID29nacc/zwhz8E4Atf+AKrV6/GOcdpp53GEUccMWSxiOyhq90nS9vf8H2LAHBBbdEan1g1bPDN7cAnSOOPhCMvCbZv9QM7dLX5QRZmvdcPuFC12N+Tp2wyHHsNTD6WRO3buK3LCe9YgYsW0DTnQ9TO/jCbGcvrmxooa3yD6fUv8kbnGP7eNo2tiTI64tDZmqCzMUFju29Wt0s+4Ps5FedGGFXUxKjCHCoqpnNoYQ7H5ERo7YwRMtuVDAW1Sd2JUHJS1L1OyCDu3M6mcCKSXtZXU5CRav78+W7JkiUDr9iHHz2zmh/8aRVrvn0mkXBoCCMTkQPNm2++ybx589IdxgGht/fSzJY65+anKaRhs7/lVlZqq4PNy2DHaj/iXWcLlE31gzv8835/Q9WeLOTXKZkARWNh4jv8vY7GH9Hv/ZKcczS0dbGpvo23d7SwbkcLnXHHtoZ2/vjGVjpiCU6YWcGa7c2sq2ntdR+jCnM4aGwx+TlhcsIhopEQOeGQ7y80tpiS/CjVTR1MKM3j0ImljCnJVTIkkoEGW25lYQ2Wr7WKJRw6t4mIiKRJ93DhnS0+mVr3d1+r1LwV1j/vm+2Bv5FqNN8PF25hOHgBHPIhGHeYb+bXLa/UDxgRqG/tZF1NK+FtnVQWQ0E0wqJV21m5tYl4wlFV38aabc1U1bXS0hmnp6LcCKfNG0NhboS/r65m6qhCrjhxOuWFOZTlRzl0YikFOWGaO2JUFOaoVYyI7JR1CVY07E+A6oclItnqU5/6FM8999xu866//nquuOKKNEUkB6R4zN/Addty2Pa6b8IXjvh+SrF2WPMMNG3etX5eKeSPgpwiOOEJ4fBzAAAgAElEQVQ6mHEKjJ4LRWP88pYdfhCKwso9DtXcEaO6oYPtjTVsb+pg6fo6Hlm8gfauPQeXCIfMj4RXksfsMUUcP7OCSeX5TCzLZ0pFATMqi/bqhqQaJEFEesq6BCsc8s0CNZKgiGSr2267Ld0hyIGoaimsfhpyS6B6Bbzx5K6mfBb2iVIi5vtNmcHUE2HG53xiNWqmHzgi1Hey4gorqW/tYv3GetbXtPDW9mZefLuW1zc17FEDFQkZHzxqIu8/xPd32t7UTl1LJ8fPrOCoyeUazEFEUirrEqxISDVYIiIi+805PwLf2kWw8infxK9bThHMOxemvwfGHgKjD9o11PkgbK5v409vbGPRyu3UtXbREUtQVdtKU9KAEWZw6IRSPjx/MuNL8xhdnMuY4jzGlOQyrjSPkrzoEL5YEZHBy7oEa1cfrD7uSSEiIiK7OAf162HDS9DV4u/ptOEln1i1bPfrVMyC930T5n8cEnGI5O3WH6oviYRj9fZm1tW0sL2pg9XbmvjnhjqWb2oEYEZlIRPL86kIhzh6WjlTRhUwtaKQqRUFTC4v2KumfCIiwyXrEizVYImIiAzAOZ9ALXvQDzjRuGn35YWjYcbJ/jH9PX5o8wHsaO5g1bYmVm9rZvX2JlZta+bNLY00te+qlSrMCXPIhFJuOmMu7zt4LLPGFA3hixIRGR5Zl2DtrMGKK8ESEREB/Ih+25YHN+d9GV5/Anas8oNOzHiP7y815Xg/wES8C0omQqj/W52s2tZEfWsX2xrb+enf1/JK1a6h1YvzIswZW8y5R0zgnVPKmTO2OGjil6v+USKS8bIuwYqEdw3TLiJyICkqKqK5ubnXZevWreOcc85h+fLlwxzVyGNmZwD/C4SBnzrnvttj+VTgLmA0UAtc6pyrCpZdDnwlWPVbzrl7hy3wVGnYBE/8264+VBbyydSJn4XDLhh036nOWILXNtXz1vYWfvmvKl5cW7tz2bSKAr505lwOnlDCnLHFjCnO1bDmInLAyr4EK7jiFlcfLBGRrGNmYeA24H1AFbDYzJ50zr2RtNr3gfucc/ea2anAfwAfM7NRwNeA+YADlgbb1g3vqxgC1avg+R9B1RKoe9uP8nfm92D8kVAxs9eh0HuzcmsTL71dw7IN9fz5zW00Bs39xpXk8ZWz5zF3XAnRsDF/2qidLUhERA50WZhgqQZLRPbB72+Gra8N7T7HHQZnfrfPxTfddBNTp07l2muvBeDrX/86Zsazzz5LXV0dXV1dfOtb32LBggV7ddj29nY++clPsmTJEiKRCD/4wQ845ZRTeP3117niiivo7OwkkUjw+OOPM2HCBC688EKqqqqIx+N89atf5SMf+ch+vew0OwZY45xbC2BmjwALgOQE62DghuD5X4FfBc/fD/zJOVcbbPsn4Azg4WGIe/8lEn4Y9SV3w+o/+oEoZp7qH0d/widWA+7CsWR9Hf/cUMef3tjG0vU+tywviPLeeWM5/ZBxzBtfzKTyAiVUIpK1si7BUh8sEckUF110EZ/97Gd3JliPPvoof/jDH7jhhhsoKSlhx44dHHfccXzgAx/Yq+ZW3ffBeu2111ixYgWnn346q1at4o477uD666/nox/9KJ2dncTjcZ566ikmTJjA7373OwAaGhr623UmmAhsTJquAo7tsc4rwPn4ZoQfAorNrKKPbSf2dhAzuxq4GmDKlClDEvh+WfMM/OlrsO01KBoHJ30Bjv23QddU1bV08teV27nz2bWs2NoEwKwxRXzl7Hmcedh4JpTmqcmfiEgg6xKs7j5YGkVQRPZKPzVNqXLUUUexfft2Nm/eTHV1NeXl5YwfP54bbriBZ599llAoxKZNm9i2bRvjxo0b9H7/8Y9/cN111wEwd+5cpk6dyqpVqzj++OP59re/TVVVFeeddx6zZ8/msMMO48Ybb+Smm27inHPO4d3vfneqXu5w6S0L6Fkg3AjcamYLgWeBTUBskNv6mc7dCdwJMH/+/PQVOM7BC7fCH78C5dPhQ3fCoedBeOB7RHXE4vxh+VZ+vngjL6ytwTmYMbqQ/7rgcN47byzlhTnD8AJERDJP1iVY4aAPlpoIikgmuOCCC/jFL37B1q1bueiii3jwwQeprq5m6dKlRKNRpk2bRnt7+17t07nez3+XXHIJxx57LL/73e94//vfz09/+lNOPfVUli5dylNPPcWXvvQlTj/9dG655ZaheGnpUgUkjyk+CdicvIJzbjNwHoCZFQHnO+cazKwKOLnHtotSGex+W/Rd+Nt34eAFPrkaxL2p6ls7uX3RWzy6ZCN1rV1MKs/nulNmccrcMRw+qUxN/0REBpB1CdbOPlhxDXIhIiPfRRddxFVXXcWOHTv429/+xqOPPsqYMWOIRqP89a9/Zf369Xu9z5NOOokHH3yQU089lVWrVrFhwwYOOugg1q5dy4wZM/jMZz7D2rVrefXVV5k7dy6jRo3i0ksvpaioiHvuuWfoX+TwWgzMNrPp+Jqpi4BLklcws0qg1jmXAL6EH1EQ4GngO2ZWHkyfHiwfmd78rU+ujrgEFtzW77DqiYRj8bpanluzg/tfXE9DWxdnHDqOi4+ZwokzKzV0uojIXsjaBEtNBEUkExxyyCE0NTUxceJExo8fz0c/+lHOPfdc5s+fz5FHHsncuXP3ep/XXnst11xzDYcddhiRSIR77rmH3Nxcfv7zn/PAAw8QjUYZN24ct9xyC4sXL+YLX/gCoVCIaDTK7bffnoJXOXycczEz+zQ+WQoDdznnXjezbwJLnHNP4mup/sPMHL6J4KeCbWvN7N/xSRrAN7sHvBhxti6HX30SJhwF5/yw3+SqobWLzzzyL/62qhozOHFmJV8+ax4HTygZxoBFRA4c1ldTkZFq/vz5bsmSJfu8/dL1tZx/+wvc9/FjOGnO6CGMTEQONG+++Sbz5s1LdxgHhN7eSzNb6pybn6aQhs3+llt7bf0L8NBHIKcQPvFHKJvc62rxhOO3r27m+39cydaGdr581jzOe8ckSvMH7p8lIpKNBltuZV0NVnjnfbAyK7EUEREZUNNWePACKB4HH3uiz+Rq5dYmrn/kX6zY2sScsUU8cvVxvHPqqGEOVkTkwJSyBMvM7gLOAbY75w7tZflHgZuCyWbgk865V1IVTzfdB0tEDmSvvfYaH/vYx3abl5uby0svvZSmiGRYLfoPiHXARx+Dsj2Hh3fOcd8L6/nOU29SnBfh1kuO4qxDx6uPlYjIEEplDdY9wK3AfX0sfxt4j3OuzszOxA9n2/NeJEMuvLMPlga5EJGBOecy6v4+hx12GMuWLUt3GLvJtKboGat6JfzzfjjmKhg1Y4/FNc0dfPEXr/LMiu2cfNBo/uuCIxhdnJuGQEVEDmwpS7Ccc8+a2bR+lj+fNPkifrjblOuuwerSjYZFZAB5eXnU1NRQUVGRUUnWSOKco6amhry8gYcHl/1QvxF+eRVEC/xNhHv4++pqPvfoKzS0dvG1cw9m4QnT9J0WEUmRkdIH6xPA7/taaGZXA1cDTJmyZ5OHvRHWKIIiMkiTJk2iqqqK6urqdIeS0fLy8pg0aViuoWWnLa/AfQsgHoPz7oTCyt0W/2P1Dq64ezHTKwu57+PHMG+8RgcUEUmltCdYZnYKPsF6V1/rOOfuxDchZP78+fuVGUXDutGwiAxONBpl+vTp6Q5DpG/OwR++BKEoXPkMVMzcbfHyTQ1c88BSZo0p4tFrjqckTyMEioikWt83xhgGZnY48FNggXOuZjiOqT5YIiJywHjrL7D+OXjPF/dIrn7zymYu/H8vUJwX4e4rjlZyJSIyTNKWYJnZFOCXwMecc6uG67gaRVBERA4IzsFf/h1Kp8A7Lt9t0a+XbeK6h//FweNL+NWnTmR8aX6aghQRyT6pHKb9YeBkoNLMqoCvAVEA59wdwC1ABfDjoKNtbDhuOKk+WCIickDY+DJs/hec+78Qydk5e1tjO1/91XLeMaWMh646jpxIWhuriIhknVSOInjxAMuvBK5M1fH7EgluNBzTKIIiIpLJXv8lhHPhkPN2znLOcdPjr9IZT/DfFx6p5EpEJA2y7swbDnc3EVQfLBERyVCJOLz+BMw5HfJ2jQr4s3+8zaKV1dx8xlymVxamMUARkeyVdQmW+mCJiEjGW/8cNG/brfbqnxvq+O7vV/D+Q8Zy+QnT0hebiEiWy9oEK64mgiIikqmWPw7RQpjzfsA3Dbzl18sZW5LH9y44QjcRFhFJo6xLsMKqwRIRkUzmHKx6Gma/D3J8M8Cl6+tYvqmRT50yi9J8DccuIpJOWZdgmRnhkGkUQRERyUy1a6FpC0w/aeesu59fR0lehA8eNSGNgYmICGRhggW+Fks1WCIikpHW/cP/nfZuALY0tPGH5Vu56JgpFOSkbHBgEREZpKxMsCIhI65RBEVEJBOtfw4Kx0DlbAAefHEDCef42HFT0xyYiIhAliZY4ZDRpUEuREQk0zjna7CmngBmtHfFefjlDZw2dyyTRxWkOzoRESFLE6yI+mCJiEgmqlsHjZtg2rsA+O2rW6hp6eSKE6elNSwREdklOxOscEh9sEREJPPs7H/1Lpxz3Pv8OmaPKeKEmRXpjUtERHbKzgRLfbBERCQTbVkGuSUwei6rtjXz2qYGLj1uqu57JSIygmRlgqVRBEVEJCPtWA0Vs8CMv6+uBuB9B49Nc1AiIpIsKxMs9cESEZGMtGM1VM4B4G+rqpk1pogJZflpDkpERJJlZYKlGiwREck4HU3QtBkqZ9HeFeflt2t59+zKdEclIiI9ZGWCFQmFiMXVB0tERDJIzRr/t3IOi9fV0hFLcNLs0emNSURE9pCVCVZYTQRFRCTT7NiVYD27qpqccIhjZ4xKb0wiIrKHrEywomE1ERQRkQyzYxVYCEbN4Lk1NbxzajkFOZF0RyUiIj1kZYKlGiwREck4NauhbCrtLsKqbU28Y2pZuiMSEZFeZGWC5ftgKcESEZEMsmM1VM5mxdYmYgnHoRNK0x2RiIj0IisTLNVgiYhIRkkkoOYtqJzDa5saADh0ohIsEZGRKCsTrEjYiCU0iqCIiGSIxiqItUHFLF7f1EBZQZRJ5br/lYjISJSVCZbugyUiIhmlocr/LZ/Ka5saOGxiKWaW3phERKRXWZlgRUKmPlgiIlnKzM4ws5VmtsbMbu5l+RQz+6uZ/cvMXjWzs4L508yszcyWBY87hi3otjoAOnNKWbWtSc0DRURGsKwc3zUSCqkPlohIFjKzMHAb8D6gClhsZk86595IWu0rwKPOudvN7GDgKWBasOwt59yRwxkzAG31/uDNUbrijsOUYImIjFhZWYMVVh8sEZFsdQywxjm31jnXCTwCLOixjgNKguelwOZhjK93QQ3W8powgBIsEZERLCsTrIhGERQRyVYTgY1J01XBvGRfBy41syp87dV1ScumB00H/2Zm7+7rIGZ2tZktMbMl1dXV+x91Wx1YmFX1kBcNaYALEZERLCsTLA1yISKStXobGaJngXAxcI9zbhJwFnC/mYWALcAU59xRwOeAh8yshF445+50zs13zs0fPXr0/kfdVgf5ZVQ3dzK6OFcDXIiIjGBZmWBpkAsRkaxVBUxOmp7Enk0APwE8CuCcewHIAyqdcx3OuZpg/lLgLWBOyiMGn2DllVHd3MHootxhOaSIiOyblCVYZnaXmW03s+V9LDcz+1EwitOrZvaOVMXSUzgUUg2WiEh2WgzMNrPpZpYDXAQ82WOdDcBpAGY2D59gVZvZ6GCQDMxsBjAbWDssUbfXQ345O5p8DZaIiIxcqazBugc4o5/lZ+ILp9nA1cDtKYxlN74Plga5EBHJNs65GPBp4GngTfxoga+b2TfN7APBap8HrjKzV4CHgYXOOQecBLwazP8FcI1zrnZYAm+rg/xyX4OlBEtEZERL2TDtzrlnzWxaP6ssAO4LCq0XzazMzMY757akKqZukbD6YImIZCvn3FP4wSuS592S9PwN4MRetnsceDzlAfamrY7EqFnUtnRSqSaCIiIjWjr7YA1mJCdg6Edj0iiCIiKSUdrqaIsUA6gGS0RkhEtngjWYkZz8zCEejUl9sEREJGMk4tDeSLMFCZZqsERERrR0JliDGckpJVSDJSIiGaO9AXA0UgSoBktEZKRLZ4L1JHBZMJrgcUDDcPS/An8frHjC4bt/iYiIjGBtdQDUJgoB1AdLRGSES9kgF2b2MHAyUGlmVcDXgCiAc+4OfAfjs4A1QCtwRapi6SkS8q0TYwlHNKybNYqIyAjWVg9AdbwAUA2WiMhIl8pRBC8eYLkDPpWq4/cnHCRV8YQjGk5HBCIiIoMU1GBt68ynOC9CngouEZERLZ1NBNMmGvIvWwNdiIjIiNfua7A2deap9kpEJANkZYIVDpoIxuNKsEREZIQLarA2tOaq/5WISAbIygQrEu7ug5VIcyQiIiIDCBKs9S1R1WCJiGSArEywdtZgqYmgiIiMdG11kFPM1ua47oElIpIBsjLB6h5FsEsJloiIjHRtdbi8Upo6YqrBEhHJAFmZYIWDQS7UB0tEREa8tnq6csoAVIMlIpIBsjLBiqoPloiIZIq2OjqiJQCUF+akORgRERlIViZY6oMlIpL5zOzTZlae7jhSrq2OWG4pADmRrCy2RUQySlaeqbv7YOk+WCIiGW0csNjMHjWzM8zM0h1QSrTV7Wwi2F1+iYjIyJWVCdbOPlhKsEREMpZz7ivAbOBnwEJgtZl9x8xmpjWwoXbu/7J15oUAhA7QHFJE5ECSlQmWarBERA4MzjkHbA0eMaAc+IWZfS+tgQ2luWfRMOowYNd9HEVEZOSKpDuAdOjugxWLa5ALEZFMZWafAS4HdgA/Bb7gnOsysxCwGvhiOuMbSt0tLlSDJSIy8mVlgqUaLBGRA0IlcJ5zbn3yTOdcwszOSVNMKdGdYKkPlojIyJd9TQRfupOZS74BqA+WiEiGewqo7Z4ws2IzOxbAOfdm2qJKge7yKqwES0RkxMu+BGvLMso3/hlQDZaISIa7HWhOmm4J5h1wlGCJiGSO7EuwovmEYm0AxHWjYRGRTGbBIBeAbxrIAdr0PaYmgiIiGSOrE6xYXDVYIiIZbK2ZfcbMosHjemBtuoNKhYRTDZaISKbIwgSrgFC8HSOhPlgiIpntGuAEYBNQBRwLXJ3WiFKk+4KgEiwRkZHvgGxK0a9oAQC5dNGlBEtEJGM557YDF6U7juEQVw2WiEjGGFSCZWYzgSrnXIeZnQwcDtznnKtPZXApESRYBXSoD5aISAYzszzgE8AhQF73fOfcx9MWVIpokAsRkcwx2CaCjwNxM5sF/AyYDjyUsqhSKZoPQD4d6oMlIpLZ7gfGAe8H/gZMAprSGlGKxJRgiYhkjMEmWAnnXAz4EPA/zrkbgPGpCyuFggQrzzrVB0tEJLPNcs59FWhxzt0LnA0cluaYUiKxcxTB7Os6LSKSaQZ7pu4ys4uBy4HfBvOiqQkpxYImgvl06D5YIiKZrSv4W29mhwKlwLT0hZM6O2uwTDVYIiIj3WATrCuA44FvO+feNrPpwAOpCyuFcroTLNVgiYhkuDvNrBz4CvAk8Abwn+kNKTW6+wyHw0qwRERGukENcuGcewP4DEBQmBU7576bysBSpnuQC+ugK65BLkREMpGZhYBG51wd8CwwI80hpVR3caUbDYuIjHyDqsEys0VmVmJmo4BXgLvN7AepDS1Fuvtg0aEaLBGRDOWcSwCf3tftzewMM1tpZmvM7OZelk8xs7+a2b/M7FUzOytp2ZeC7Vaa2fv3NYa90V2DFVITQRGREW+wTQRLnXONwHnA3c65dwLvTV1YKRTd1URQfbBERDLan8zsRjObbGajuh8DbWRmYeA24EzgYOBiMzu4x2pfAR51zh2Fv9fWj4NtDw6mDwHOAH4c7C+lVIMlIpI5BptgRcxsPHAhuwa5GND+XCFMme5h2k01WCIiGe7jwKfwTQSXBo8lg9juGGCNc26tc64TeARY0GMdB5QEz0uBzcHzBcAjzrkO59zbwJpgfym1swZLCZaIyIg3qD5YwDeBp4HnnHOLzWwGsLq/DZKuEL4PqAIWm9mTQX+ubt1XCG8Prgo+RapHgFINlojIAcE5N30fN50IbEyargKO7bHO14E/mtl1QCG7Wm1MBF7sse3EfYxj0GIJp9orEZEMMdhBLh4DHkuaXgucP8BmO68QAphZ9xXC5ASrryuEqRMkWIXWufOKoIiIZB4zu6y3+c65+wbatLfNekxfDNzjnPtvMzseuD8YCn4w22JmVwNXA0yZMmWAcAYWd043GRYRyRCDHeRikpk9YWbbzWybmT1uZpMG2Ky3K4Q9r/J9HbjUzKrwtVfXDTLufReOgoUpCOk+WCIiGe7opMe78WXKBwaxXRUwOWl6Ente4PsE8CiAc+4FIA+oHOS2OOfudM7Nd87NHz169GBeS7/icSVYIiKZYrB9sO7G32NkAj5J+k0wrz97c4VwEnAW/grhHjGZ2dVmtsTMllRXVw8y5L6iMogWUGidxOJKsEREMpVz7rqkx1XAUUDOIDZdDMw2s+lmloMftOLJHutsAE4DMLN5+ASrOljvIjPLDe4JORt4eWheUd9UgyUikjkGm2CNds7d7ZyLBY97gIEuye3PFcLdDPWVQKL5FFgnHbH4/u9LRERGilZ8wtMv51wMP8T708Cb+L7Ar5vZN82suwbs88BVZvYK8DCw0Hmv48utN4A/AJ9yzqW8MIknlGCJiGSKwQ5yscPMLsUXMuBrnmoG2GbnFUJgE/4K4SU91um+QnhPjyuEqZVTQEl7F03tsZQfSkREUsPMfsOulhEh/JDrjw5mW+fcU/im6cnzbkl6/gZwYh/bfhv49j6EvM80yIWISOYYbIL1ceBW4If4wux54Ir+NnDOxcys+wphGLir+wohsMQ59yT+CuFPzOyGYL8LnXOpb7cXLaAo3EVjW1fKDyUiIinz/aTnMWC9c64qXcGkUkI1WCIiGWOwowhuoEfHYTP7LPA/A2y3z1cIUyqaT6F10KgaLBGRTLYB2OKcawcws3wzm+acW5fesIZeLOEImxIsEZFMMNg+WL353JBFMdyiBRRYp2qwREQy22NA8v024iTdUuRAEk84wmElWCIimWB/EqzMPdNH88mnk8Z2JVgiIhks4pzr7J4Ing9mFMGME084IqH9KbJFRGS47M/ZOnPHOI8WkEsHjW1qIigiksGqk0b9w8wWADvSGE/KxBMOdcESEckM/fbBMrMmek+kDMhPSUTDIVpAruugrStOZyxBTkRXBUVEMtA1wINmdmswXQVclsZ4UkY1WCIimaPfBMs5VzxcgQyraD45iXYAGtu7qCzKTXNAIiKyt5xzbwHHmVkRYM65pnTHlCqxhCOkKiwRkYyQnZfDovlEuhMsDXQhIpKRzOw7ZlbmnGt2zjWZWbmZfSvdcaVCPJHQfbBERDJEliZYBUTibYDTUO0iIpnrTOdcffeEc64OOCuN8aRM3KH7YImIZIjsTLByCgDIRTcbFhHJYGEz29nG28zygQOyzXc8kVCCJSKSIQZ1o+EDTtQnWPl0aKh2EZHM9QDwjJndHUxfAdybxnhSJp5wSrBERDJEliZYfgDEfDo1VLuISIZyzn3PzF4F3osf3fYPwNT0RpUaGkVQRCRzZOfZOqjBKrB21WCJiGS2rUACOB84DXgzveGkRizhiIRVgyUikgmyugarKKQ+WCIimcbM5gAXARcDNcDP8cO0n5LWwFIooSaCIiIZI0sTLF+DVZEbVw2WiEjmWQH8HTjXObcGwMxuSG9IqRVLOMKmBEtEJBNkdRPBUdGY+mCJiGSe8/FNA/9qZj8xs9PwfbAOWBrkQkQkc2RpguWbCJbnqAZLRCTTOOeecM59BJgLLAJuAMaa2e1mdnpag0uRuPpgiYhkjCxNsHwNVnlUfbBERDKVc67FOfegc+4cYBKwDLg5zWGlRDzhCKmJoIhIRsjSBMvXYJVGumhQgiUikvGcc7XOuf/nnDs13bGkQtw5ImoiKCKSEbIzwcrxNVgl4RiN7eqDJSIiI1ss7ggpwRIRyQjZmWAFTQSLwmoiKCIiI5+/0bASLBGRTJCdCVY4ByxEUaiTjliC9q54uiMSERHpU9w5wqHsLLJFRDJNdp6tzSBaQKF1ANCkZoIiIjKC+WHa0x2FiIgMRvaervNHUZxoBNBQ7SIiMqL5JoLZW2SLiGSS7D1bF4+lsHMHgPphiYjIiKZh2kVEMkf2JlhFY8nvqAagtqUzzcGIiIj0LZZI6EbDIiIZInsTrOJx5Lb7BKuqri3NwYiIiPQtkYCwRhEUEckI2ZtgFY0j1F5PSTTGxtrWdEcjIiLSp1giQVhNBEVEMkL2JljFYwE4rLSDjXVKsEREZGRyzpFwqsESEckU2ZtgFY0DYF5RKxtr1URQRERGpnjCAehGwyIiGSKlCZaZnWFmK81sjZnd3Mc6F5rZG2b2upk9lMp4dhPUYM3Mb1YNloiIjFixIMEKKcESEckIkVTt2MzCwG3A+4AqYLGZPemceyNpndnAl4ATnXN1ZjYmVfHsIajBmhJtoqk9RkNrF6UF0WE7vIiIyGAknGqwREQySSprsI4B1jjn1jrnOoFHgAU91rkKuM25/9/enYfHUZ35Hv++Vd3aLFnW4g0LbywGG2+x2QOEOAOEAAbCTQyEIZkJPCzDlpkkTMgNTEJyk8zCwECckIFhDJ4QhglLMmwhZglmSQzY2AaDF2wsbGRb8iJZkqXuPvePKsltWfLare6Sfp/nqaeqTi39qtTqo7fPqVNuM4BzbkMW49nVgGowj2HeFgC1YomI9BN7611hZneY2cJw+sDMtqRtS6Zte7I34u1owdI9WCIi0ZC1FixgBLA2bb0WOL7LPkcCmNl8wAduc8490/VEZnYlcCXAyJEjMxOd58OAIVSmNgOwtqGZY5ikN/8AACAASURBVEaUZ+bcIiKSl/ald4Vz7qa0/a8DpqadosU5N6W34gVIJpVgiYhESTZbsLqrCVyX9RhwBPAZ4GLg381s0G4HOXevc266c2764MGDMxdh2VBKE/WAWrBERPqJfeldke5i4Fe9ElkPkuoiKCISKdlMsGqBQ9PWa4B13ezzhHOu3Tn3IfA+QcLVO0qHEW+uo6woppEERUT6h+56V4zobkczGwWMAealFReZ2QIze93Mzu/pRczsynC/BRs3bjyogJMa5EJEJFKymWD9GTjCzMaYWQEwC+jaX/1x4HQAM6sm6DK4Kosx7apsKDTWcWhFiVqwRET6h33pXdFhFvCocy6ZVjbSOTcduAT4VzM7rLsDM9nzQsO0i4hES9YSLOdcAvgb4FngPeAR59xSM/u+mZ0X7vYsUG9m7wIvAN90ztVnK6bdlA6D7RsZVVHA2gYlWCIi/cC+9K7oMIsu3QOdc+vC+SrgRXa9PysrOluwTAmWiEgUZHOQC5xzTwFPdSn7XtqyA74RTr2vbCjgOHrgDv7wQQuJZIqY33+fvSwi0g909q4APiZIoi7pupOZjQMqgNfSyiqAZufcjrDXxcnAT7MdcMcogjFfCZaISBT072wifBbWpEGttCVSfFDXlOOAREQkm/axdwUEg1s8HH4R2OFoYIGZLSLodfHj9NEHsyXZOUx7/66yRUSiIqstWHmvvAaA8UWbgVIW1W5h/CEDcxuTiIhk1d56V4Trt3Vz3KvAxKwG143OBEtdBEVEIqF/fx02eByYx+DmFZQXx1m0dsvejxEREelFST1oWEQkUvp3ghUvhsqx2IZ3mXzoIBYqwRIRkTyjUQRFRKKlfydYAEPGw4Z3mVJTzgd1jTS3JXIdkYiISKdEKgWoBUtEJCqUYA2dAA0fMnV4ASkHSz7eluuIREREOqWcugiKiESJEqyhEwDHlKI6AN2HJSIieSWRVIIlIhIlSrCGjAegonE5IwYV8+aazTkOSEREZCcNciEiEi1KsCrGQLwENrzLqUcO5o/LN7Ijkcx1VCIiIgAknQa5EBGJEiVYngeDj4K6pfzF+CFsb0vy+qqGXEclIiICQCJswfKUYImIRIISLICh46FuKSeNraI47vP8u3W5jkhERASAlIZpFxGJFCVYADXHQvMmirau5NQjq3n+vTpc2CVDREQklzpbsEwJlohIFCjBAhh7ejBfOY/PHT2U9VtbNVy7iIjkhc4HDftKsEREokAJFkDFKKg6vDPBivvGb96uzXVUIiIiOxMsdREUEYkEJVgdDvssrH6FikLH548ZzqNv1tLclsh1VCIi0s8l1UVQRCRSlGB1OOyz0N4Ma9/gshNH0dia4LeL1uU6KhER6ed2tmCpyhYRiQJ9WncY/WnwYrDiD0wfVcG4oWU8+PoaDXYhIiI51fmgYd2DJSISCUqwOhSWwaiT4d3HMef4y5NGseTjbfxx+aZcRyYiIv1YxyiCvroIiohEghKsdFMuhc2r4aNXuWhaDTUVxfz02WWdzyARERHpbcmwJ4WvQS5ERCJBCVa6o8+FwoHw9kMUxny+8RdHsuTjbfzv4vW5jkxERPqpZDIFKMESEYkKJVjpCkrgmAth6ePQuo2ZU0Zw1LAyfvz0Mpp2aERBERHpfZ1dBJVgiYhEghKsrqZeBokWWPhf+J5x+/nHsG5rCz95elmuIxMRkX4o5fQcLBGRKFGC1dWIaTDyJHjlDmhvYfroSr560mgefH0Nr67QgBciItK71IIlIhItSrC6MoPP3gJNn8CC+wH45pnjGDt4ANc//Dbrt7bkOEAREelPUkqwREQiJZbrAPLS6E/DmFODVqypX6GkqJx7L5vGzLvnc9VDb/HrK0+gKO7nOkoREekHNEy7iOyv9vZ2amtraW1tzXUokVRUVERNTQ3xePyAjleC1ZPP/QP8+wx4/jY45w4OH1LGv3x5Clc99CbXzH2LX1w2jbivBkAREcmuZMphBp5asERkH9XW1lJWVsbo0aMxfTmzX5xz1NfXU1tby5gxYw7oHMoQejLiU3D81UE3wdXzAThzwjBuP/8Y5i3bwI2/XkhbIpXjIEVEpK9LppwGuBCR/dLa2kpVVZWSqwNgZlRVVR1U658SrD357C0waBQ8fhVsrwfg0uNHccvZR/O/76zn63MWsF3Dt4uISBYlUw5P/ySJyH5ScnXgDvbaZTXBMrOzzOx9M1thZjfvYb+LzMyZ2fRsxrPfCgbARf8BjXXw6FchGSRTV5w6lh9fOJFXlm/kop+/xtqG5tzGKSIifZZasEREoiVrCZaZ+cA9wOeB8cDFZja+m/3KgOuBN7IVy0GpmQbn3gkfvgy/uxHC55HMOm4k93/1WD7e3My5d7/CU4vX5zhQERHpixIppxEERUQiJJstWMcBK5xzq5xzbcDDwMxu9vsB8FMgf4c5mXIxnPotePtBePY7nUnWZ8YN4bfXfZpDK0q4Zu5bXDv3LeqbduQ4WBER6UuSSrBEJGK2bNnCz372s/0+7uyzz2bLli1ZiKh3ZXMUwRHA2rT1WuD49B3MbCpwqHPud2b2dz2dyMyuBK4EGDlyZBZC3QenfwfamuD1n8GObXDOv4IfZ1TVAB675iR+8fIq7nx+Oa+tque7Xzia86eM0IhPIiJy0JLO4Xu6ZVpEDsw//HYp767bltFzjj9kILeeO6HH7R0J1jXXXLNLeTKZxPd7ftTRU089lbEYcymbn9jdZReuc6OZB9wB/O3eTuScu9c5N905N33w4MEZDHE/mMGZP4LTvg1vPwQPXQhNGwGI+R7Xnn542JpVzDceWcTZd/2RF97fgHNuLycWERHpWTLp0FNBRCRKbr75ZlauXMmUKVM49thjOf3007nkkkuYOHEiAOeffz7Tpk1jwoQJ3HvvvZ3HjR49mk2bNrF69WqOPvporrjiCiZMmMAZZ5xBS0tLj6/3y1/+kmOPPZbJkyfzxS9+kebmYHyEuro6LrjgAiZPnszkyZN59dVXAZgzZw6TJk1i8uTJXHbZZRn/+S1bCYCZnQjc5pw7M1z/ewDn3P8L18uBlUBTeMgwoAE4zzm3oKfzTp8+3S1Y0OPm3rHwv+B3N0HRILjwFzD2M52bUinH7xav55+fe5819c0cN6aSq04by2eOHKIWLRGRNGb2pnMuvwY3yoKDrbf+9pFFvL6qnvk3fzaDUYlIX/bee+9x9NFH5+z1V69ezTnnnMOSJUt48cUX+cIXvsCSJUs6nyvV0NBAZWUlLS0tHHvssbz00ktUVVUxevRoFixYQFNTE4cffjgLFixgypQpfOlLX+K8887jK1/5SrevV19fT1VVFQDf/e53GTp0KNdddx1f/vKXOfHEE7nxxhtJJpM0NTVRW1vLhRdeyPz586muru6MpavuruG+1lvZ/E7sz8ARZjbGzAqAWcCTHRudc1udc9XOudHOudHA6+wlucobUy6Br/8BCstgzkz47Q3Q3AAED4I8b/Ih/P6m0/j+zAmsbWjmrx5YwOfueImHXl+jYd1FRHJsbyPcmtkdZrYwnD4wsy1p2y43s+XhdHlvxJtyugdLRKLtuOOO2+WhvXfddReTJ0/mhBNOYO3atSxfvny3Y8aMGcOUKVMAmDZtGqtXr+7x/EuWLOGUU05h4sSJzJ07l6VLlwIwb948rr76agB836e8vJx58+Zx0UUXUV1dDdBtcnWwspZgOecSwN8AzwLvAY8455aa2ffN7LxsvW6vGXYMXPVHOOk6eGsO3DUVXv03SASDXBTEPP7yxNG8/K3TuXPWFEoLY3z38SUc98Pn+eZ/L+JPHzao+6CISC/blxFunXM3OeemOOemAP8G/CY8thK4leB+4uOAW82sItsxaxRBEYm6AQMGdC6/+OKLPP/887z22mssWrSIqVOndvtQ38LCws5l3/dJJHpupPjqV7/K3XffzeLFi7n11lv3+JBg51zWnxGW1V7dzrmnnHNHOucOc879MCz7nnPuyW72/UwkWq/SxYvhjNvhqlegZjo89124e3pwj1Z78IuN+x4zp4zgiWtP5n+uPpFzJh3CU4vX86VfvMap//gCP3rqPd76aDOplJItEZFesK8j3Ha4GPhVuHwm8HvnXINzbjPwe+CsrEZL0PVcCZaIRElZWRmNjY3dbtu6dSsVFRWUlJSwbNkyXn/99YN+vcbGRoYPH057eztz587tLJ8xYwazZ88GggE2tm3bxowZM3jkkUeor68Hgu6KmZbNUQT7j6ET4Cv/AytfgOdvhSeuhedvg+l/BdP/GsqGYmZMG1XJtFGV3HreeJ5e/Am/fWcd/zH/Q+59eRXDBhZx2pGDOeXIaj59eDWDSgpy/VOJiPRFex3htoOZjQLGAPP2cOyIHo7N2Oi3iVRKDxoWkUipqqri5JNP5phjjqG4uJihQ4d2bjvrrLP4+c9/zqRJkxg3bhwnnHDCQb/eD37wA44//nhGjRrFxIkTO5O7O++8kyuvvJL77rsP3/eZPXs2J554IrfccgunnXYavu8zdepUHnjggYOOIV3WBrnIlrwY5GJPnAseSvz6bPjgGfB8OOIMmHwxHHkmxAp32X1rSzvzltXx3NI6XlmxicbWBGYwqWYQpx5RzXFjKpk6soLSQuXCItK35GKQCzP7P8CZzrmvh+uXAcc5567rZt9vAzUd28zsm0Chc+72cP3/As3OuX/e02sebL319f/8M+u2tPLUDacc8DlEpH/J9SAXfcHBDHKh/9ozzQzGnhZM9Svhzf+Adx6B95+C4goY9wU48gwYezoUDaS8OM4FU2u4YGoNiWSKRbVb+ePyjfxx+SZ+9uJK/m3eCjyDo4YNZProCqaNqmByzSBGVpZoVEIRkf1XCxyatl4DrOth31nAtV2O/UyXY1/MYGzdSqYcMV+f9yIiUaEEK5uqDgvu0ZpxG6x6Ad75NSz7LSx8CLw4jD4ZjjgTDjsdqscR8z2mjQqSqBs/dySNre28/dEWFqzZzJtrGnj0zVrmvLYGgLLCGEcfMpAJhwzkmEPKmTBiIIcPLiWmh6WIiOxJ5wi3wMcESdQlXXcys3FABfBaWvGzwI/SBrY4A/j77IYbDHLhZfmGbBGRKLj22muZP3/+LmU33HADX/va13IUUfeUYPUGPwZH/EUwJROw9o2g++Dy5+DZsG4uroRRJwXTyBNh2CTKiuKceuRgTj0yeLhyIpli2SeNLPl4K0vXbWPJuq386k8f0dqeAoKRC8dWD+CwIaUcNriUwwYPCOelFBf0/NRsEZH+wjmXMLOOEW594P6OEW6BBWmDMF0MPOzS+tE75xrM7AcESRrA951zmb87uouUc7oHS0QEuOeee3Idwj5RgtXb/FjQcjX6ZDjjB7B5NayeD2tehTXzYdnvgv1iRTBkPAyfDMMnwfDJxIaM55gR5RwzorzzdMmU48NNTSz5eBvvrt/Gyg1NLP14K08vXk/6wIQjBhVz2JBSxlSVUFNRwqGVxdRUlFBTUUx5cTzrw1WKiOQL59xTwFNdyr7XZf22Ho69H7g/a8F1I5HUKIIiIlGiBCvXKkYH09RLg/Vt64Jka93bsH4RLP1NcB8XgHnBvkPGw+CjYMjR+IPHcXjlWA4fMoLzp+4czKq1Pcma+mZWbmxi5YYmVmxsYsWGJt5es5nGLg87Li2MUVNRHE5B0jViUDFDy4sYOrCIIWWFxNX1UEQkJ5IpR0FMn8EiIlGhBCvfDDwEJl4UTBCMSrjloyDZ2vBuOC2D958Gl0w7bkRwz1fV4VA5lqJBIxlXfijjxo6EYw4PBt8IbW1uZ+3mZmo3t1DbZf7aynq2tyV3CckMqgYUMHRgUdpUyLBwubq0kKrSAioHFFAUV1dEEZFMSjq1YImIRIkSrHxnBhWjgmn8eTvLEzugfgVsfB8aVgYjFtavgCW/gdYtu54jXgLlNVB+KAw6lPLyQykfOIJjyobBkGFQOjwY4dAM5xxbW9qp3dzChsZW6rbt4JOtrbssv1O7hU1Nbd2GW1YYo6q0gKrSQqoGBPPq0oLO5aoBBQwqKWBQSZxBJXGK4766J4qI7EFSDxoWEYkUJVhRFSsMHnA8dMLu21o2w5a1sHVt2vyjYL5+ITTX736MF4fSIdiAwQwqHcKgAUOgdDCUDoVhg2FsFZRUQvEwKKmkzYrYuL2Num2t1De1Ud+0g/rtbWxq2hGsb9/BRw3NvPXRFhq279jlfrB0BTGPQcVBsjWouIDykjgVJXEGlRRQXhynvDhOWVGMgUVxSotilBXFKC2MUVYUp7Qwpn86RKTPSyQ1yIWI9G2lpaU0NTXlOoyMUYLVFxVXBNPwSd1vb2uGxvXQ+Ekwb6qDpg3BtH1DUP7JYti+EVKJbk9R4BcyoqSSEcWVYeJVEc4roSqcF1dA0UBSBYPZ5kqoTxSysb2QLa2OrS1tbG5uZ0tze7C8vZ0tLW2sbWhmcW2w3DE64p4MKPCDZCst+RpYFN81EQu3lYXrZUUxSotilBT4lBQEc91jJiL5KqUugiJyMJ6+Ofi/LpOGTYTP/ziz5+xDlGD1RwUl4f1ah+15v1Qq6G7YtCFo9WppgOaGLvPNwbaNy8KyzbveGwZ4wKBwOgwgPgAKy6BoIBQO3DkvD+cFpVBQQrtXTLMV0ewK2e4K2O4KaUwW0JgsYHMiztZkAQ1tcba1QeOOdhpbEzS2Jli3pYWmHcFyc5f7yXq8JL5HcYHPgAI/mBfGKI4H8yAR25mM7dzmU1wQ6zymOB7Mi2I750UFHgW+p26QInLAEuoiKCIR8+1vf5tRo0ZxzTXXAHDbbbdhZrz88sts3ryZ9vZ2br/9dmbOnLnXczU1NTFz5sxuj5szZw7/9E//hJkxadIkHnzwQerq6rjqqqtYtWoVALNnz+akk07K3g/bDSVY0jPPC1qlSir3/ZhUCnZsC5Kvls3Qug12NAZlrdvS5luD8tZt0Lo16MrYsS3RAkAcKA+nPccZD5LG+IBgPqAEKoIkLRUrIeEX0+YVscOKaLWizqQtmApoTsbY7mJsT/g0JX2aEjG2JTy2JWJs3epR1+6xZYdPY7tje1uKZE/9HXtgRlrS5VGUnoTFPYrjPoXxIEErintp23wKY144+RTGg+WCjvUu5R1lBeExeui0SN+QSjl8T3/PInKActDSNGvWLG688cbOBOuRRx7hmWee4aabbmLgwIFs2rSJE044gfPOO2+vX0IXFRXx2GOP7Xbcu+++yw9/+EPmz59PdXU1DQ3BYwmvv/56TjvtNB577DGSyWROuh4qwZLM8jwoHhRMByqVhPbmoCtj+/Zg3rZ953J7x3r6Ptt33b+9GZob8NprKWjbTkHbdkrbmyHReuBxmYcbUAR+Ac4vJBVOCa+AhFdA0gppt4JwitNGMO0gTquL00qc1lScFhejNeXR3BajtcVne9KnOemxPeGzKeXR2O6zPWm0uRg7iNNOjDYXI4lPO/5uc+j+g8n3rNsErTDmdyZhHQlZ3A9a2jqW4+FygW+dy3HfIx7zKPQ94jHrPCYe63ps8Lrp50k/v76JF9k/iZTuwRKRaJk6dSobNmxg3bp1bNy4kYqKCoYPH85NN93Eyy+/jOd5fPzxx9TV1TFs2LA9nss5x3e+853djps3bx4XXXQR1dXVAFRWBg0C8+bNY86cOQD4vk95+V6/qs84JViSfzw/6EJYWJb5c3dN3tpbghEZEzuC5Ku7eXLnsqXNvXCfgs7jd0Ciqedzpdr3LUY/nPaRwyPl+aQshjOfpMVI4ZM0nySxcO6TSIST80jg0+582vFod0FZW+e6R1vKp9157EgF+ybxacOnBY8EsfAcMRJ4uyd+4fkT3SSDSeeTMg/nxfFiMcyLY34cz4+BH8Pz43ixoMy8GObHwu1xfD9olYt5RswLEjnfCxK9mGf4vhH3vLDM0vY1fN8j7qWV+dZ5nmA5mAfnTN9n19fxveA1YmllnqEuoJJVyZTD03tMRCLmoosu4tFHH+WTTz5h1qxZzJ07l40bN/Lmm28Sj8cZPXo0ra17/+K7p+Occ3lb/yrBkv4lm8nb3qRSQaKV3AHJdki2hQlc+86yxI6gvGNK355KBAlisj1cbodUEku246cS+KlEWJ4I90mm7ZeAZGLX9V3O1bpz/85j23Gd64nOMuth4JMDuybhtA+5ZzJM5pL4JMJEL4lHOz4J54dJY3cJX7B/Co8URhIPh0cSjzaMVmy3bSnXsRzMU537pO+3cxvmY+YFDwP3/GBuHuZ5YDHwvGC752Oeh1kwx/PxzMN8PyjzfbxwW8ey53t4fgzzPHzPx/PC/cJzeZ1lMXzPwzwfz/cxz8cP9/PCMs/z8P0Ynhec0/eDct/38b0YXszD92L4fqwzofUsSEa9jmQ1nGKe5W3F1tck1YIlIhE0a9YsrrjiCjZt2sRLL73EI488wpAhQ4jH47zwwgusWbNmn86zdevWbo+bMWMGF1xwATfddBNVVVU0NDRQWVnJjBkzmD17NjfeeCPJZJLt27czcODAbP6ou1GCJdJbPC+4R4ySXEeyz7r9l865tORtZ6LXNfFjl+Ssu+Svnb0ngzvP54eJ5B7PlUrgkkFi6BLhPNkOyQTOpSCVxLlEOE9BZ1kqGJwl5TCXDMrTJnNJrGOZFObC/UjhuRRG2n15LpwA9m2MlbyVdDuTz455W1pyufH8hxk39ZRch9nnJVIOTwmWiETMhAkTaGxsZMSIEQwfPpxLL72Uc889l+nTpzNlyhSOOuqofTpPT8dNmDCBW265hdNOOw3f95k6dSoPPPAAd955J1deeSX33Xcfvu8ze/ZsTjzxxGz+qLtRgiUi+8cMwi59+cjo6a60LHIunJKdSRsdSZtL7UxK08tSuydyux+XClo+wzKXSpJKJUkmkySTKVLJBKlUklQyiXNBeSqZwrkkqWQCl0qRTAVzlwyOdalUeJ5UkICmUji3syxIOJPBPNzWsYxLpiWqKQ6p3nO/ecmML02v4ajhvfvtq4hIJixevHN4+Orqal577bVu99vTQBR7Ou7yyy/n8ssv36Vs6NChPPHEEwcQbebk539IIiJRYhZMZHekN2O/b9GTPuBbZ+3bt7wiIpIflGCJiIiIiEhOLV68mMsuu2yXssLCQt54440cRXTglGCJiIiIiPQx+TzKXncmTpzIwoULcx0GEFy7g6EnF4qIiIiI9CFFRUXU19cfdKLQHznnqK+vp6io6IDPoRYsEREREZE+pKamhtraWjZu3JjrUCKpqKiImpqaAz5eCZaIiIiISB8Sj8cZM2ZMrsPot9RFUEREREREJEOUYImIiIiIiGSIEiwREREREZEMsaiNLmJmG4E1B3maamBTBsLpDVGJNSpxQnRijUqcEJ1YoxInRCfWg4lzlHNucCaDyUf9rN6KSpwQnVijEidEJ9aoxAnRiTUqcUIv1FuRS7AywcwWOOem5zqOfRGVWKMSJ0Qn1qjECdGJNSpxQnRijUqcUReV6xyVOCE6sUYlTohOrFGJE6ITa1TihN6JVV0ERUREREREMkQJloiIiIiISIb01wTr3lwHsB+iEmtU4oToxBqVOCE6sUYlTohOrFGJM+qicp2jEidEJ9aoxAnRiTUqcUJ0Yo1KnNALsfbLe7BERERERESyob+2YImIiIiIiGScEiwREREREZEM6XcJlpmdZWbvm9kKM7s51/F0MLNDzewFM3vPzJaa2Q1h+W1m9rGZLQyns3MdK4CZrTazxWFMC8KySjP7vZktD+cVOY5xXNp1W2hm28zsxny5pmZ2v5ltMLMlaWXdXkML3BW+b98xs0/lOM5/NLNlYSyPmdmgsHy0mbWkXduf91ace4i1x9+3mf19eE3fN7Mzcxznr9NiXG1mC8PynF3TPXwu5d37tK/K1zoLolVvRaHOCmNSvZW9OPOu3opKnbWHWFVv9cQ5128mwAdWAmOBAmARMD7XcYWxDQc+FS6XAR8A44HbgL/LdXzdxLsaqO5S9lPg5nD5ZuAnuY6zy+/+E2BUvlxT4FTgU8CSvV1D4GzgacCAE4A3chznGUAsXP5JWpyj0/fLk2va7e87/PtaBBQCY8LPBj9XcXbZ/s/A93J9TffwuZR379O+OOVznbWX90defMZ2iTVSdVba71/1VubizLt6Kyp1Vk+xdtmueitt6m8tWMcBK5xzq5xzbcDDwMwcxwSAc269c+6tcLkReA8Ykduo9ttM4D/D5f8Ezs9hLF3NAFY659bkOpAOzrmXgYYuxT1dw5nAHBd4HRhkZsNzFadz7jnnXCJcfR2o6Y1Y9qaHa9qTmcDDzrkdzrkPgRUEnxFZt6c4zcyALwG/6o1Y9mQPn0t59z7to/K2zoI+UW/lc50FqrcyGmc+1ltRqbNA9db+6m8J1ghgbdp6LXlYGZjZaGAq8EZY9Ddhs+X9+dCFIeSA58zsTTO7Miwb6pxbD8EbHBiSs+h2N4td//Dz8ZpCz9cwn9+7f0Xw7U+HMWb2tpm9ZGan5CqoLrr7fefrNT0FqHPOLU8ry/k17fK5FMX3aRRF5npGoN6KWp0FqreyKd/rrSjVWaB6azf9LcGybsryapx6MysF/ge40Tm3DZgNHAZMAdYTNMHmg5Odc58CPg9ca2an5jqgnphZAXAe8N9hUb5e0z3Jy/eumd0CJIC5YdF6YKRzbirwDeC/zGxgruIL9fT7zstrClzMrv9U5fyadvO51OOu3ZTlwzWNqkhcz4jUW5Gps0D1VjZFoN6KWp0Fqrd2098SrFrg0LT1GmBdjmLZjZnFCd4Mc51zvwFwztU555LOuRTwS3qxOXhPnHPrwvkG4DGCuOo6mlXD+YbcRbiLzwNvOefqIH+vaaina5h3710zuxw4B7jUhR2Zw64L9eHymwR9xI/MXZR7/H3n4zWNARcCv+4oy/U17e5ziQi9TyMu769nVOqtiNVZoHorK6JQb0WpzgLVWz3pbwnWn4EjzGxMS7tlAAAAA91JREFU+O3QLODJHMcEdPZfvQ94zzn3L2nl6f1ALwCWdD22t5nZADMr61gmuHF0CcG1vDzc7XLgidxEuJtdvlnJx2uapqdr+CTwl+FoNycAWzuaunPBzM4Cvg2c55xrTisfbGZ+uDwWOAJYlZsoO2Pq6ff9JDDLzArNbAxBrH/q7fi6+BywzDlX21GQy2va0+cSEXmf9gF5W2dBdOqtCNZZoHor46JSb0WszgLVW91zORjhI5cTwWghHxBk07fkOp60uD5N0CT5DrAwnM4GHgQWh+VPAsPzINaxBCPZLAKWdlxHoAr4A7A8nFfmQawlQD1QnlaWF9eUoPJcD7QTfIPy1z1dQ4Im7HvC9+1iYHqO41xB0Ge5473683DfL4bviUXAW8C5eXBNe/x9A7eE1/R94PO5jDMsfwC4qsu+Obume/hcyrv3aV+d8rXO2sv7Iy8+Y9PijEydFcaleis7ceZdvRWVOqunWMNy1VvdTBaeXERERERERA5Sf+siKCIiIiIikjVKsERERERERDJECZaIiIiIiEiGKMESERERERHJECVYIiIiIiIiGaIESyTDzCxpZgvTppszeO7RZpZPzz8REZGIU70lklmxXAcg0ge1OOem5DoIERGRfaR6SySD1IIl0kvMbLWZ/cTM/hROh4flo8zsD2b2TjgfGZYPNbPHzGxROJ0Unso3s1+a2VIze87MisP9rzezd8PzPJyjH1NERPoI1VsiB0YJlkjmFXfpavHltG3bnHPHAXcD/xqW3Q3Mcc5NAuYCd4XldwEvOecmA58ieCo6wBHAPc65CcAWgiemA9wMTA3Pc1W2fjgREelzVG+JZJA553Idg0ifYmZNzrnSbspXA591zq0yszjwiXOuysw2AcOdc+1h+XrnXLWZbQRqnHM70s4xGvi9c+6IcP3bQNw5d7uZPQM0AY8DjzvnmrL8o4qISB+geksks9SCJdK7XA/LPe3TnR1py0l23kv5BeAeYBrwppnpHksRETlYqrdE9pMSLJHe9eW0+Wvh8qvArHD5UuCVcPkPwNUAZuab2cCeTmpmHnCoc+4F4FvAIGC3byNFRET2k+otkf2kbwpEMq/YzBamrT/jnOsY8rbQzN4g+HLj4rDseuB+M/smsBH4Wlh+A3Cvmf01wTd+VwPre3hNH3jIzMoBA+5wzm3J2E8kIiJ9meotkQzSPVgivSTsyz7dObcp17GIiIjsjeotkQOjLoIiIiIiIiIZohYsERERERGRDFELloiIiIiISIYowRIREREREckQJVgiIiIiIiIZogRLREREREQkQ5RgiYiIiIiIZMj/B6dm8mpEkbl0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d9cd6ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_vis(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2773858563244343 0.9227\n"
     ]
    }
   ],
   "source": [
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 1.5210 - acc: 0.6090 - val_loss: 0.7768 - val_acc: 0.8377\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.6144 - acc: 0.8474 - val_loss: 0.4609 - val_acc: 0.8807\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.4454 - acc: 0.8794 - val_loss: 0.3784 - val_acc: 0.8962\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.3836 - acc: 0.8925 - val_loss: 0.3394 - val_acc: 0.9053\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3489 - acc: 0.9008 - val_loss: 0.3155 - val_acc: 0.9101\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3247 - acc: 0.9076 - val_loss: 0.2966 - val_acc: 0.9163\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.3066 - acc: 0.9127 - val_loss: 0.2816 - val_acc: 0.9196\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2911 - acc: 0.9177 - val_loss: 0.2702 - val_acc: 0.9229\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2778 - acc: 0.9212 - val_loss: 0.2600 - val_acc: 0.9261\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2665 - acc: 0.9244 - val_loss: 0.2499 - val_acc: 0.9277\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2558 - acc: 0.9277 - val_loss: 0.2413 - val_acc: 0.9310\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2464 - acc: 0.9299 - val_loss: 0.2346 - val_acc: 0.9335\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2378 - acc: 0.9328 - val_loss: 0.2267 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2297 - acc: 0.9348 - val_loss: 0.2202 - val_acc: 0.9386\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2224 - acc: 0.9362 - val_loss: 0.2145 - val_acc: 0.9393\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.2154 - acc: 0.9386 - val_loss: 0.2090 - val_acc: 0.9414\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2091 - acc: 0.9406 - val_loss: 0.2052 - val_acc: 0.9431\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2027 - acc: 0.9423 - val_loss: 0.2007 - val_acc: 0.9440\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1973 - acc: 0.9437 - val_loss: 0.1949 - val_acc: 0.9453\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1919 - acc: 0.9457 - val_loss: 0.1917 - val_acc: 0.9460\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1868 - acc: 0.9466 - val_loss: 0.1853 - val_acc: 0.9477\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1818 - acc: 0.9478 - val_loss: 0.1824 - val_acc: 0.9487\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1772 - acc: 0.9489 - val_loss: 0.1781 - val_acc: 0.9503\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1728 - acc: 0.9503 - val_loss: 0.1746 - val_acc: 0.9506\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1684 - acc: 0.9515 - val_loss: 0.1710 - val_acc: 0.9525\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1643 - acc: 0.9529 - val_loss: 0.1683 - val_acc: 0.9533\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1603 - acc: 0.9540 - val_loss: 0.1663 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1566 - acc: 0.9557 - val_loss: 0.1625 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1532 - acc: 0.9566 - val_loss: 0.1604 - val_acc: 0.9546\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1497 - acc: 0.9576 - val_loss: 0.1572 - val_acc: 0.9561\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1464 - acc: 0.9584 - val_loss: 0.1542 - val_acc: 0.9564\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1431 - acc: 0.9587 - val_loss: 0.1523 - val_acc: 0.9577\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1400 - acc: 0.9605 - val_loss: 0.1500 - val_acc: 0.9585\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1372 - acc: 0.9610 - val_loss: 0.1479 - val_acc: 0.9591\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1345 - acc: 0.9624 - val_loss: 0.1455 - val_acc: 0.9594\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1314 - acc: 0.9628 - val_loss: 0.1437 - val_acc: 0.9597\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1288 - acc: 0.9634 - val_loss: 0.1415 - val_acc: 0.9607\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1263 - acc: 0.9641 - val_loss: 0.1395 - val_acc: 0.9603\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1239 - acc: 0.9651 - val_loss: 0.1383 - val_acc: 0.9615\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1216 - acc: 0.9663 - val_loss: 0.1361 - val_acc: 0.9617\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1191 - acc: 0.9666 - val_loss: 0.1345 - val_acc: 0.9607\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1170 - acc: 0.9674 - val_loss: 0.1327 - val_acc: 0.9617\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1147 - acc: 0.9680 - val_loss: 0.1313 - val_acc: 0.9623\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1126 - acc: 0.9681 - val_loss: 0.1311 - val_acc: 0.9623\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1106 - acc: 0.9692 - val_loss: 0.1285 - val_acc: 0.9627\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1085 - acc: 0.9697 - val_loss: 0.1271 - val_acc: 0.9632\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.1066 - acc: 0.9704 - val_loss: 0.1251 - val_acc: 0.9642\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1047 - acc: 0.9711 - val_loss: 0.1250 - val_acc: 0.9640\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1030 - acc: 0.9711 - val_loss: 0.1232 - val_acc: 0.9645\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1012 - acc: 0.9714 - val_loss: 0.1224 - val_acc: 0.9642\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0994 - acc: 0.9725 - val_loss: 0.1211 - val_acc: 0.9651\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0977 - acc: 0.9728 - val_loss: 0.1207 - val_acc: 0.9643\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0961 - acc: 0.9733 - val_loss: 0.1186 - val_acc: 0.9651\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0944 - acc: 0.9740 - val_loss: 0.1183 - val_acc: 0.9654\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0927 - acc: 0.9744 - val_loss: 0.1173 - val_acc: 0.9664\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0914 - acc: 0.9745 - val_loss: 0.1159 - val_acc: 0.9657\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0899 - acc: 0.9755 - val_loss: 0.1142 - val_acc: 0.9673\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0884 - acc: 0.9758 - val_loss: 0.1144 - val_acc: 0.9663\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0870 - acc: 0.9762 - val_loss: 0.1133 - val_acc: 0.9670\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0857 - acc: 0.9763 - val_loss: 0.1123 - val_acc: 0.9674\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0844 - acc: 0.9766 - val_loss: 0.1122 - val_acc: 0.9671\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0831 - acc: 0.9769 - val_loss: 0.1110 - val_acc: 0.9677\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0818 - acc: 0.9774 - val_loss: 0.1105 - val_acc: 0.9683\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0805 - acc: 0.9780 - val_loss: 0.1097 - val_acc: 0.9683\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0792 - acc: 0.9782 - val_loss: 0.1083 - val_acc: 0.9678\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0781 - acc: 0.9786 - val_loss: 0.1085 - val_acc: 0.9679\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0770 - acc: 0.9789 - val_loss: 0.1077 - val_acc: 0.9681\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0758 - acc: 0.9792 - val_loss: 0.1074 - val_acc: 0.9683\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0748 - acc: 0.9791 - val_loss: 0.1070 - val_acc: 0.9687\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0736 - acc: 0.9795 - val_loss: 0.1050 - val_acc: 0.9696\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0725 - acc: 0.9799 - val_loss: 0.1041 - val_acc: 0.9696\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0715 - acc: 0.9805 - val_loss: 0.1037 - val_acc: 0.9694\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0704 - acc: 0.9808 - val_loss: 0.1032 - val_acc: 0.9699\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0694 - acc: 0.9809 - val_loss: 0.1032 - val_acc: 0.9702\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0685 - acc: 0.9812 - val_loss: 0.1023 - val_acc: 0.9702\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0673 - acc: 0.9817 - val_loss: 0.1027 - val_acc: 0.9699\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0666 - acc: 0.9818 - val_loss: 0.1015 - val_acc: 0.9700\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0655 - acc: 0.9822 - val_loss: 0.1021 - val_acc: 0.9692\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0647 - acc: 0.9825 - val_loss: 0.1002 - val_acc: 0.9704\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0638 - acc: 0.9828 - val_loss: 0.0998 - val_acc: 0.9709\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0629 - acc: 0.9828 - val_loss: 0.0994 - val_acc: 0.9712\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0620 - acc: 0.9833 - val_loss: 0.0985 - val_acc: 0.9712\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0612 - acc: 0.9832 - val_loss: 0.0988 - val_acc: 0.9709\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0603 - acc: 0.9838 - val_loss: 0.0984 - val_acc: 0.9721\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0594 - acc: 0.9840 - val_loss: 0.0981 - val_acc: 0.9718\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0586 - acc: 0.9842 - val_loss: 0.0985 - val_acc: 0.9722\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0579 - acc: 0.9845 - val_loss: 0.0972 - val_acc: 0.9716\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0570 - acc: 0.9848 - val_loss: 0.0967 - val_acc: 0.9713\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0564 - acc: 0.9849 - val_loss: 0.0963 - val_acc: 0.9726\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0556 - acc: 0.9852 - val_loss: 0.0972 - val_acc: 0.9719\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0549 - acc: 0.9855 - val_loss: 0.0958 - val_acc: 0.9723\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0542 - acc: 0.9856 - val_loss: 0.0959 - val_acc: 0.9725\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0535 - acc: 0.9859 - val_loss: 0.0954 - val_acc: 0.9722\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0527 - acc: 0.9862 - val_loss: 0.0959 - val_acc: 0.9716\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0521 - acc: 0.9862 - val_loss: 0.0948 - val_acc: 0.9717\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0515 - acc: 0.9866 - val_loss: 0.0942 - val_acc: 0.9730\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0507 - acc: 0.9870 - val_loss: 0.0946 - val_acc: 0.9723\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0501 - acc: 0.9870 - val_loss: 0.0957 - val_acc: 0.9718\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0494 - acc: 0.9870 - val_loss: 0.0937 - val_acc: 0.9728\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0488 - acc: 0.9871 - val_loss: 0.0948 - val_acc: 0.9726\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0482 - acc: 0.9875 - val_loss: 0.0934 - val_acc: 0.9722\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0477 - acc: 0.9879 - val_loss: 0.0932 - val_acc: 0.9729\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0469 - acc: 0.9878 - val_loss: 0.0934 - val_acc: 0.9732\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0464 - acc: 0.9884 - val_loss: 0.0933 - val_acc: 0.9739\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0459 - acc: 0.9881 - val_loss: 0.0925 - val_acc: 0.9736\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0453 - acc: 0.9886 - val_loss: 0.0921 - val_acc: 0.9732\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0448 - acc: 0.9888 - val_loss: 0.0922 - val_acc: 0.9732\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0441 - acc: 0.9890 - val_loss: 0.0921 - val_acc: 0.9736\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0435 - acc: 0.9892 - val_loss: 0.0920 - val_acc: 0.9730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0431 - acc: 0.9891 - val_loss: 0.0919 - val_acc: 0.9732\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0425 - acc: 0.9894 - val_loss: 0.0914 - val_acc: 0.9739\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0419 - acc: 0.9897 - val_loss: 0.0916 - val_acc: 0.9732\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0414 - acc: 0.9897 - val_loss: 0.0913 - val_acc: 0.9732\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0917 - val_acc: 0.9736\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0403 - acc: 0.9902 - val_loss: 0.0920 - val_acc: 0.9736\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0399 - acc: 0.9901 - val_loss: 0.0910 - val_acc: 0.9732\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0395 - acc: 0.9903 - val_loss: 0.0919 - val_acc: 0.9737\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0390 - acc: 0.9905 - val_loss: 0.0906 - val_acc: 0.9744\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0385 - acc: 0.9907 - val_loss: 0.0908 - val_acc: 0.9741\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0380 - acc: 0.9910 - val_loss: 0.0909 - val_acc: 0.9737\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0375 - acc: 0.9911 - val_loss: 0.0912 - val_acc: 0.9729\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0371 - acc: 0.9912 - val_loss: 0.0906 - val_acc: 0.9741\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0367 - acc: 0.9912 - val_loss: 0.0898 - val_acc: 0.9742\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0362 - acc: 0.9918 - val_loss: 0.0899 - val_acc: 0.9742\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0358 - acc: 0.9916 - val_loss: 0.0899 - val_acc: 0.9747\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0352 - acc: 0.9918 - val_loss: 0.0900 - val_acc: 0.9738\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0349 - acc: 0.9919 - val_loss: 0.0913 - val_acc: 0.9732\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0345 - acc: 0.9920 - val_loss: 0.0902 - val_acc: 0.9745\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0341 - acc: 0.9922 - val_loss: 0.0898 - val_acc: 0.9735\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0336 - acc: 0.9924 - val_loss: 0.0906 - val_acc: 0.9733\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0333 - acc: 0.9923 - val_loss: 0.0899 - val_acc: 0.9734\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0329 - acc: 0.9927 - val_loss: 0.0899 - val_acc: 0.9738\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0324 - acc: 0.9927 - val_loss: 0.0901 - val_acc: 0.9742\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0321 - acc: 0.9926 - val_loss: 0.0893 - val_acc: 0.9740\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0318 - acc: 0.9928 - val_loss: 0.0901 - val_acc: 0.9749\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0313 - acc: 0.9932 - val_loss: 0.0901 - val_acc: 0.9740\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0310 - acc: 0.9932 - val_loss: 0.0901 - val_acc: 0.9744\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0306 - acc: 0.9935 - val_loss: 0.0891 - val_acc: 0.9741\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0303 - acc: 0.9933 - val_loss: 0.0888 - val_acc: 0.9742\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0300 - acc: 0.9936 - val_loss: 0.0890 - val_acc: 0.9747\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0295 - acc: 0.9937 - val_loss: 0.0886 - val_acc: 0.9748\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0292 - acc: 0.9937 - val_loss: 0.0887 - val_acc: 0.9746\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0289 - acc: 0.9940 - val_loss: 0.0891 - val_acc: 0.9746\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0890 - val_acc: 0.9743\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0282 - acc: 0.9944 - val_loss: 0.0883 - val_acc: 0.9747\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0280 - acc: 0.9942 - val_loss: 0.0887 - val_acc: 0.9748\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0275 - acc: 0.9945 - val_loss: 0.0895 - val_acc: 0.9742\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0272 - acc: 0.9946 - val_loss: 0.0886 - val_acc: 0.9748\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0269 - acc: 0.9946 - val_loss: 0.0889 - val_acc: 0.9751\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0889 - val_acc: 0.9743\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0263 - acc: 0.9950 - val_loss: 0.0893 - val_acc: 0.9746\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.0898 - val_acc: 0.9737\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0258 - acc: 0.9950 - val_loss: 0.0895 - val_acc: 0.9743\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0255 - acc: 0.9951 - val_loss: 0.0890 - val_acc: 0.9749\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0251 - acc: 0.9952 - val_loss: 0.0886 - val_acc: 0.9753\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0885 - val_acc: 0.9746\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0889 - val_acc: 0.9747\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0242 - acc: 0.9957 - val_loss: 0.0893 - val_acc: 0.9748\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0241 - acc: 0.9955 - val_loss: 0.0889 - val_acc: 0.9746\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0237 - acc: 0.9957 - val_loss: 0.0892 - val_acc: 0.9742\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0235 - acc: 0.9957 - val_loss: 0.0891 - val_acc: 0.9744\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0233 - acc: 0.9957 - val_loss: 0.0890 - val_acc: 0.9748\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0229 - acc: 0.9960 - val_loss: 0.0888 - val_acc: 0.9749\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0227 - acc: 0.9960 - val_loss: 0.0889 - val_acc: 0.9748\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0225 - acc: 0.9961 - val_loss: 0.0890 - val_acc: 0.9746\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0222 - acc: 0.9960 - val_loss: 0.0885 - val_acc: 0.9748\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0219 - acc: 0.9962 - val_loss: 0.0890 - val_acc: 0.9752\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0217 - acc: 0.9964 - val_loss: 0.0895 - val_acc: 0.9747\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0213 - acc: 0.9964 - val_loss: 0.0885 - val_acc: 0.9745\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0212 - acc: 0.9965 - val_loss: 0.0890 - val_acc: 0.9750\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0209 - acc: 0.9965 - val_loss: 0.0897 - val_acc: 0.9748\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0207 - acc: 0.9966 - val_loss: 0.0888 - val_acc: 0.9744\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0205 - acc: 0.9967 - val_loss: 0.0890 - val_acc: 0.9747\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0202 - acc: 0.9966 - val_loss: 0.0895 - val_acc: 0.9751\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0201 - acc: 0.9968 - val_loss: 0.0892 - val_acc: 0.9745\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0199 - acc: 0.9968 - val_loss: 0.0889 - val_acc: 0.9748\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0196 - acc: 0.9969 - val_loss: 0.0895 - val_acc: 0.9744\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0194 - acc: 0.9970 - val_loss: 0.0890 - val_acc: 0.9742\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0192 - acc: 0.9969 - val_loss: 0.0893 - val_acc: 0.9747\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0190 - acc: 0.9968 - val_loss: 0.0892 - val_acc: 0.9745\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0188 - acc: 0.9971 - val_loss: 0.0893 - val_acc: 0.9747\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0186 - acc: 0.9972 - val_loss: 0.0892 - val_acc: 0.9749\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0184 - acc: 0.9972 - val_loss: 0.0892 - val_acc: 0.9746\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0182 - acc: 0.9974 - val_loss: 0.0895 - val_acc: 0.9739\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0179 - acc: 0.9972 - val_loss: 0.0895 - val_acc: 0.9743\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0177 - acc: 0.9974 - val_loss: 0.0891 - val_acc: 0.9741\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0176 - acc: 0.9976 - val_loss: 0.0892 - val_acc: 0.9740\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0174 - acc: 0.9975 - val_loss: 0.0894 - val_acc: 0.9750\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0172 - acc: 0.9975 - val_loss: 0.0901 - val_acc: 0.9746\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0170 - acc: 0.9975 - val_loss: 0.0894 - val_acc: 0.9750\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0169 - acc: 0.9979 - val_loss: 0.0906 - val_acc: 0.9749\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0167 - acc: 0.9976 - val_loss: 0.0893 - val_acc: 0.9752\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0165 - acc: 0.9977 - val_loss: 0.0893 - val_acc: 0.9748\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0163 - acc: 0.9976 - val_loss: 0.0894 - val_acc: 0.9752\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0161 - acc: 0.9978 - val_loss: 0.0903 - val_acc: 0.9742\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0160 - acc: 0.9979 - val_loss: 0.0898 - val_acc: 0.9742\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0158 - acc: 0.9979 - val_loss: 0.0896 - val_acc: 0.9751\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0157 - acc: 0.9979 - val_loss: 0.0899 - val_acc: 0.9752\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0155 - acc: 0.9980 - val_loss: 0.0900 - val_acc: 0.9753\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0153 - acc: 0.9980 - val_loss: 0.0900 - val_acc: 0.9746\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "#model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = NB_EPOCH, verbose = VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/step\n",
      "0.08059975245971 0.977\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 1.6561 - acc: 0.4802 - val_loss: 0.8509 - val_acc: 0.8253\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.8819 - acc: 0.7297 - val_loss: 0.5066 - val_acc: 0.8738\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.6665 - acc: 0.7960 - val_loss: 0.4095 - val_acc: 0.8910\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.5787 - acc: 0.8252 - val_loss: 0.3631 - val_acc: 0.9005\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.5192 - acc: 0.8444 - val_loss: 0.3333 - val_acc: 0.9081\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.4827 - acc: 0.8552 - val_loss: 0.3116 - val_acc: 0.9130\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.4514 - acc: 0.8664 - val_loss: 0.2937 - val_acc: 0.9164\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.4306 - acc: 0.8723 - val_loss: 0.2805 - val_acc: 0.9206\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.4076 - acc: 0.8791 - val_loss: 0.2687 - val_acc: 0.9232\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3895 - acc: 0.8844 - val_loss: 0.2594 - val_acc: 0.9256\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.3755 - acc: 0.8899 - val_loss: 0.2483 - val_acc: 0.9282\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3591 - acc: 0.8942 - val_loss: 0.2406 - val_acc: 0.9309\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3486 - acc: 0.8963 - val_loss: 0.2340 - val_acc: 0.9313\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3392 - acc: 0.8999 - val_loss: 0.2264 - val_acc: 0.9345\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3302 - acc: 0.9032 - val_loss: 0.2210 - val_acc: 0.9360\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3191 - acc: 0.9084 - val_loss: 0.2157 - val_acc: 0.9371\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.3128 - acc: 0.9082 - val_loss: 0.2097 - val_acc: 0.9397\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.3024 - acc: 0.9114 - val_loss: 0.2046 - val_acc: 0.9404\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2949 - acc: 0.9140 - val_loss: 0.1998 - val_acc: 0.9422\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2887 - acc: 0.9157 - val_loss: 0.1962 - val_acc: 0.9425\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2842 - acc: 0.9178 - val_loss: 0.1917 - val_acc: 0.9443\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2780 - acc: 0.9182 - val_loss: 0.1883 - val_acc: 0.9448\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2690 - acc: 0.9201 - val_loss: 0.1840 - val_acc: 0.9459\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2677 - acc: 0.9210 - val_loss: 0.1813 - val_acc: 0.9468\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2592 - acc: 0.9241 - val_loss: 0.1774 - val_acc: 0.9482\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2584 - acc: 0.9234 - val_loss: 0.1746 - val_acc: 0.9490\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2494 - acc: 0.9267 - val_loss: 0.1718 - val_acc: 0.9492\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2454 - acc: 0.9275 - val_loss: 0.1689 - val_acc: 0.9504\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2432 - acc: 0.9289 - val_loss: 0.1655 - val_acc: 0.9518\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2372 - acc: 0.9319 - val_loss: 0.1635 - val_acc: 0.9523\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2343 - acc: 0.9317 - val_loss: 0.1609 - val_acc: 0.9533\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2295 - acc: 0.9329 - val_loss: 0.1592 - val_acc: 0.9539\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2296 - acc: 0.9329 - val_loss: 0.1571 - val_acc: 0.9550\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2245 - acc: 0.9341 - val_loss: 0.1545 - val_acc: 0.9552\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2217 - acc: 0.9360 - val_loss: 0.1524 - val_acc: 0.9560\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2205 - acc: 0.9355 - val_loss: 0.1506 - val_acc: 0.9566\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.2159 - acc: 0.9372 - val_loss: 0.1488 - val_acc: 0.9571\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2131 - acc: 0.9366 - val_loss: 0.1468 - val_acc: 0.9578\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2106 - acc: 0.9387 - val_loss: 0.1457 - val_acc: 0.9581\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2058 - acc: 0.9393 - val_loss: 0.1436 - val_acc: 0.9587\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.2047 - acc: 0.9398 - val_loss: 0.1416 - val_acc: 0.9593\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1984 - acc: 0.9405 - val_loss: 0.1405 - val_acc: 0.9591\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1985 - acc: 0.9423 - val_loss: 0.1383 - val_acc: 0.9602\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1964 - acc: 0.9429 - val_loss: 0.1376 - val_acc: 0.9607\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1955 - acc: 0.9426 - val_loss: 0.1365 - val_acc: 0.9614\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1888 - acc: 0.9440 - val_loss: 0.1354 - val_acc: 0.9603\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1902 - acc: 0.9443 - val_loss: 0.1338 - val_acc: 0.9611\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1882 - acc: 0.9457 - val_loss: 0.1323 - val_acc: 0.9618\n",
      "Epoch 49/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1855 - acc: 0.9444 - val_loss: 0.1310 - val_acc: 0.9622\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1846 - acc: 0.9457 - val_loss: 0.1301 - val_acc: 0.9628\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1819 - acc: 0.9463 - val_loss: 0.1288 - val_acc: 0.9633\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1823 - acc: 0.9461 - val_loss: 0.1271 - val_acc: 0.9636\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1762 - acc: 0.9470 - val_loss: 0.1267 - val_acc: 0.9637\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1750 - acc: 0.9477 - val_loss: 0.1257 - val_acc: 0.9639\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1742 - acc: 0.9490 - val_loss: 0.1238 - val_acc: 0.9648\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1720 - acc: 0.9495 - val_loss: 0.1229 - val_acc: 0.9648\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1713 - acc: 0.9485 - val_loss: 0.1219 - val_acc: 0.9651\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1688 - acc: 0.9499 - val_loss: 0.1216 - val_acc: 0.9655\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1691 - acc: 0.9507 - val_loss: 0.1214 - val_acc: 0.9653\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1667 - acc: 0.9501 - val_loss: 0.1202 - val_acc: 0.9658\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1652 - acc: 0.9515 - val_loss: 0.1186 - val_acc: 0.9668\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1607 - acc: 0.9523 - val_loss: 0.1182 - val_acc: 0.9666\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1610 - acc: 0.9519 - val_loss: 0.1177 - val_acc: 0.9668\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1613 - acc: 0.9520 - val_loss: 0.1169 - val_acc: 0.9665\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1560 - acc: 0.9541 - val_loss: 0.1159 - val_acc: 0.9667\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1568 - acc: 0.9538 - val_loss: 0.1158 - val_acc: 0.9670\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1556 - acc: 0.9536 - val_loss: 0.1145 - val_acc: 0.9674\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1549 - acc: 0.9545 - val_loss: 0.1144 - val_acc: 0.9676\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1549 - acc: 0.9547 - val_loss: 0.1137 - val_acc: 0.9680\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1517 - acc: 0.9549 - val_loss: 0.1127 - val_acc: 0.9683\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1508 - acc: 0.9545 - val_loss: 0.1118 - val_acc: 0.9690\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1499 - acc: 0.9550 - val_loss: 0.1125 - val_acc: 0.9681\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1482 - acc: 0.9565 - val_loss: 0.1107 - val_acc: 0.9687\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1475 - acc: 0.9555 - val_loss: 0.1099 - val_acc: 0.9683\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1448 - acc: 0.9567 - val_loss: 0.1091 - val_acc: 0.9685\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1461 - acc: 0.9558 - val_loss: 0.1088 - val_acc: 0.9688\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1444 - acc: 0.9568 - val_loss: 0.1085 - val_acc: 0.9685\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1424 - acc: 0.9575 - val_loss: 0.1078 - val_acc: 0.9698\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1430 - acc: 0.9581 - val_loss: 0.1070 - val_acc: 0.9690\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1384 - acc: 0.9591 - val_loss: 0.1068 - val_acc: 0.9693\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1393 - acc: 0.9592 - val_loss: 0.1062 - val_acc: 0.9697\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1362 - acc: 0.9600 - val_loss: 0.1060 - val_acc: 0.9693\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1348 - acc: 0.9590 - val_loss: 0.1059 - val_acc: 0.9703\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1356 - acc: 0.9591 - val_loss: 0.1050 - val_acc: 0.9696\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1386 - acc: 0.9591 - val_loss: 0.1046 - val_acc: 0.9701\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1352 - acc: 0.9593 - val_loss: 0.1040 - val_acc: 0.9703\n",
      "Epoch 87/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1346 - acc: 0.9598 - val_loss: 0.1038 - val_acc: 0.9708\n",
      "Epoch 88/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1349 - acc: 0.9603 - val_loss: 0.1031 - val_acc: 0.9708\n",
      "Epoch 89/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1333 - acc: 0.9605 - val_loss: 0.1029 - val_acc: 0.9711\n",
      "Epoch 90/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1297 - acc: 0.9616 - val_loss: 0.1022 - val_acc: 0.9710\n",
      "Epoch 91/250\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1300 - acc: 0.9614 - val_loss: 0.1025 - val_acc: 0.9713\n",
      "Epoch 92/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1286 - acc: 0.9609 - val_loss: 0.1020 - val_acc: 0.9718\n",
      "Epoch 93/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1292 - acc: 0.9618 - val_loss: 0.1012 - val_acc: 0.9714\n",
      "Epoch 94/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1259 - acc: 0.9622 - val_loss: 0.1010 - val_acc: 0.9712\n",
      "Epoch 95/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1244 - acc: 0.9624 - val_loss: 0.1006 - val_acc: 0.9714\n",
      "Epoch 96/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1270 - acc: 0.9614 - val_loss: 0.0999 - val_acc: 0.9717\n",
      "Epoch 97/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1245 - acc: 0.9631 - val_loss: 0.0998 - val_acc: 0.9719\n",
      "Epoch 98/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1243 - acc: 0.9629 - val_loss: 0.0992 - val_acc: 0.9716\n",
      "Epoch 99/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1225 - acc: 0.9633 - val_loss: 0.0987 - val_acc: 0.9719\n",
      "Epoch 100/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1216 - acc: 0.9637 - val_loss: 0.0991 - val_acc: 0.9714\n",
      "Epoch 101/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1238 - acc: 0.9626 - val_loss: 0.0982 - val_acc: 0.9718\n",
      "Epoch 102/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1205 - acc: 0.9641 - val_loss: 0.0975 - val_acc: 0.9717\n",
      "Epoch 103/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1198 - acc: 0.9641 - val_loss: 0.0976 - val_acc: 0.9713\n",
      "Epoch 104/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1179 - acc: 0.9646 - val_loss: 0.0976 - val_acc: 0.9723\n",
      "Epoch 105/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1192 - acc: 0.9640 - val_loss: 0.0967 - val_acc: 0.9718\n",
      "Epoch 106/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1177 - acc: 0.9650 - val_loss: 0.0965 - val_acc: 0.9717\n",
      "Epoch 107/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1179 - acc: 0.9644 - val_loss: 0.0962 - val_acc: 0.9730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1150 - acc: 0.9658 - val_loss: 0.0957 - val_acc: 0.9731\n",
      "Epoch 109/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1173 - acc: 0.9645 - val_loss: 0.0961 - val_acc: 0.9728\n",
      "Epoch 110/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1158 - acc: 0.9645 - val_loss: 0.0958 - val_acc: 0.9736\n",
      "Epoch 111/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1140 - acc: 0.9661 - val_loss: 0.0950 - val_acc: 0.9727\n",
      "Epoch 112/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1164 - acc: 0.9657 - val_loss: 0.0948 - val_acc: 0.9730\n",
      "Epoch 113/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1152 - acc: 0.9661 - val_loss: 0.0946 - val_acc: 0.9736\n",
      "Epoch 114/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1133 - acc: 0.9656 - val_loss: 0.0942 - val_acc: 0.9731\n",
      "Epoch 115/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1104 - acc: 0.9674 - val_loss: 0.0947 - val_acc: 0.9732\n",
      "Epoch 116/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1094 - acc: 0.9665 - val_loss: 0.0936 - val_acc: 0.9728\n",
      "Epoch 117/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1104 - acc: 0.9671 - val_loss: 0.0940 - val_acc: 0.9735\n",
      "Epoch 118/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1111 - acc: 0.9662 - val_loss: 0.0931 - val_acc: 0.9740\n",
      "Epoch 119/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1098 - acc: 0.9677 - val_loss: 0.0929 - val_acc: 0.9734\n",
      "Epoch 120/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1091 - acc: 0.9675 - val_loss: 0.0926 - val_acc: 0.9738\n",
      "Epoch 121/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1095 - acc: 0.9664 - val_loss: 0.0926 - val_acc: 0.9738\n",
      "Epoch 122/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1065 - acc: 0.9677 - val_loss: 0.0925 - val_acc: 0.9735\n",
      "Epoch 123/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1066 - acc: 0.9675 - val_loss: 0.0915 - val_acc: 0.9734\n",
      "Epoch 124/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1083 - acc: 0.9674 - val_loss: 0.0915 - val_acc: 0.9735\n",
      "Epoch 125/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1054 - acc: 0.9686 - val_loss: 0.0918 - val_acc: 0.9737\n",
      "Epoch 126/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1043 - acc: 0.9683 - val_loss: 0.0913 - val_acc: 0.9739\n",
      "Epoch 127/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1035 - acc: 0.9685 - val_loss: 0.0909 - val_acc: 0.9740\n",
      "Epoch 128/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1053 - acc: 0.9684 - val_loss: 0.0912 - val_acc: 0.9738\n",
      "Epoch 129/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1026 - acc: 0.9696 - val_loss: 0.0904 - val_acc: 0.9740\n",
      "Epoch 130/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1049 - acc: 0.9683 - val_loss: 0.0908 - val_acc: 0.9736\n",
      "Epoch 131/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1030 - acc: 0.9697 - val_loss: 0.0905 - val_acc: 0.9738\n",
      "Epoch 132/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1037 - acc: 0.9690 - val_loss: 0.0903 - val_acc: 0.9738\n",
      "Epoch 133/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1014 - acc: 0.9696 - val_loss: 0.0902 - val_acc: 0.9734\n",
      "Epoch 134/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1008 - acc: 0.9704 - val_loss: 0.0899 - val_acc: 0.9740\n",
      "Epoch 135/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1004 - acc: 0.9697 - val_loss: 0.0898 - val_acc: 0.9736\n",
      "Epoch 136/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0984 - acc: 0.9700 - val_loss: 0.0898 - val_acc: 0.9742\n",
      "Epoch 137/250\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0993 - acc: 0.9700 - val_loss: 0.0895 - val_acc: 0.9744\n",
      "Epoch 138/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0995 - acc: 0.9694 - val_loss: 0.0890 - val_acc: 0.9743\n",
      "Epoch 139/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0976 - acc: 0.9695 - val_loss: 0.0888 - val_acc: 0.9742\n",
      "Epoch 140/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0975 - acc: 0.9699 - val_loss: 0.0884 - val_acc: 0.9751\n",
      "Epoch 141/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0976 - acc: 0.9701 - val_loss: 0.0885 - val_acc: 0.9743\n",
      "Epoch 142/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0951 - acc: 0.9722 - val_loss: 0.0886 - val_acc: 0.9743\n",
      "Epoch 143/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0971 - acc: 0.9701 - val_loss: 0.0876 - val_acc: 0.9745\n",
      "Epoch 144/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0956 - acc: 0.9705 - val_loss: 0.0878 - val_acc: 0.9750\n",
      "Epoch 145/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0948 - acc: 0.9704 - val_loss: 0.0875 - val_acc: 0.9750\n",
      "Epoch 146/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0959 - acc: 0.9713 - val_loss: 0.0879 - val_acc: 0.9752\n",
      "Epoch 147/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0940 - acc: 0.9717 - val_loss: 0.0878 - val_acc: 0.9747\n",
      "Epoch 148/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0947 - acc: 0.9708 - val_loss: 0.0877 - val_acc: 0.9743\n",
      "Epoch 149/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0929 - acc: 0.9718 - val_loss: 0.0873 - val_acc: 0.9753\n",
      "Epoch 150/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0918 - acc: 0.9723 - val_loss: 0.0873 - val_acc: 0.9746\n",
      "Epoch 151/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0941 - acc: 0.9711 - val_loss: 0.0871 - val_acc: 0.9748\n",
      "Epoch 152/250\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0920 - acc: 0.9719 - val_loss: 0.0866 - val_acc: 0.9748\n",
      "Epoch 153/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0901 - acc: 0.9723 - val_loss: 0.0865 - val_acc: 0.9752\n",
      "Epoch 154/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0914 - acc: 0.9724 - val_loss: 0.0860 - val_acc: 0.9757\n",
      "Epoch 155/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0905 - acc: 0.9716 - val_loss: 0.0859 - val_acc: 0.9758\n",
      "Epoch 156/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0911 - acc: 0.9725 - val_loss: 0.0862 - val_acc: 0.9755\n",
      "Epoch 157/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0909 - acc: 0.9724 - val_loss: 0.0852 - val_acc: 0.9756\n",
      "Epoch 158/250\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0908 - acc: 0.9724 - val_loss: 0.0861 - val_acc: 0.9762\n",
      "Epoch 159/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0922 - acc: 0.9718 - val_loss: 0.0861 - val_acc: 0.9752\n",
      "Epoch 160/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0891 - acc: 0.9728 - val_loss: 0.0858 - val_acc: 0.9756\n",
      "Epoch 161/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0909 - acc: 0.9721 - val_loss: 0.0854 - val_acc: 0.9759\n",
      "Epoch 162/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0889 - acc: 0.9734 - val_loss: 0.0852 - val_acc: 0.9749\n",
      "Epoch 163/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0891 - acc: 0.9726 - val_loss: 0.0851 - val_acc: 0.9758\n",
      "Epoch 164/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0885 - acc: 0.9729 - val_loss: 0.0845 - val_acc: 0.9762\n",
      "Epoch 165/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0865 - acc: 0.9734 - val_loss: 0.0854 - val_acc: 0.9759\n",
      "Epoch 166/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0865 - acc: 0.9736 - val_loss: 0.0851 - val_acc: 0.9753\n",
      "Epoch 167/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0881 - acc: 0.9730 - val_loss: 0.0846 - val_acc: 0.9753\n",
      "Epoch 168/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0847 - acc: 0.9742 - val_loss: 0.0845 - val_acc: 0.9756\n",
      "Epoch 169/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0842 - acc: 0.9750 - val_loss: 0.0850 - val_acc: 0.9764\n",
      "Epoch 170/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0863 - acc: 0.9731 - val_loss: 0.0851 - val_acc: 0.9759\n",
      "Epoch 171/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0854 - acc: 0.9736 - val_loss: 0.0844 - val_acc: 0.9754\n",
      "Epoch 172/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0860 - acc: 0.9736 - val_loss: 0.0844 - val_acc: 0.9761\n",
      "Epoch 173/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0845 - acc: 0.9744 - val_loss: 0.0845 - val_acc: 0.9754\n",
      "Epoch 174/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0851 - acc: 0.9734 - val_loss: 0.0839 - val_acc: 0.9761\n",
      "Epoch 175/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0858 - acc: 0.9739 - val_loss: 0.0837 - val_acc: 0.9758\n",
      "Epoch 176/250\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0830 - acc: 0.9744 - val_loss: 0.0839 - val_acc: 0.9756\n",
      "Epoch 177/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0823 - acc: 0.9747 - val_loss: 0.0845 - val_acc: 0.9762\n",
      "Epoch 178/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0812 - acc: 0.9744 - val_loss: 0.0838 - val_acc: 0.9757\n",
      "Epoch 179/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0828 - acc: 0.9745 - val_loss: 0.0835 - val_acc: 0.9760\n",
      "Epoch 180/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0848 - acc: 0.9738 - val_loss: 0.0837 - val_acc: 0.9757\n",
      "Epoch 181/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0824 - acc: 0.9756 - val_loss: 0.0838 - val_acc: 0.9760\n",
      "Epoch 182/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0823 - acc: 0.9745 - val_loss: 0.0837 - val_acc: 0.9762\n",
      "Epoch 183/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0806 - acc: 0.9753 - val_loss: 0.0836 - val_acc: 0.9762\n",
      "Epoch 184/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0809 - acc: 0.9746 - val_loss: 0.0836 - val_acc: 0.9762\n",
      "Epoch 185/250\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0824 - acc: 0.9738 - val_loss: 0.0835 - val_acc: 0.9763\n",
      "Epoch 186/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0809 - acc: 0.9748 - val_loss: 0.0829 - val_acc: 0.9763\n",
      "Epoch 187/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0801 - acc: 0.9753 - val_loss: 0.0830 - val_acc: 0.9761\n",
      "Epoch 188/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0775 - acc: 0.9752 - val_loss: 0.0828 - val_acc: 0.9760\n",
      "Epoch 189/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0781 - acc: 0.9761 - val_loss: 0.0831 - val_acc: 0.9764\n",
      "Epoch 190/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0782 - acc: 0.9757 - val_loss: 0.0836 - val_acc: 0.9762\n",
      "Epoch 191/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0800 - acc: 0.9754 - val_loss: 0.0827 - val_acc: 0.9765\n",
      "Epoch 192/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0780 - acc: 0.9759 - val_loss: 0.0823 - val_acc: 0.9764\n",
      "Epoch 193/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0789 - acc: 0.9755 - val_loss: 0.0827 - val_acc: 0.9764\n",
      "Epoch 194/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0789 - acc: 0.9757 - val_loss: 0.0821 - val_acc: 0.9766\n",
      "Epoch 195/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0775 - acc: 0.9765 - val_loss: 0.0821 - val_acc: 0.9768\n",
      "Epoch 196/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0785 - acc: 0.9759 - val_loss: 0.0817 - val_acc: 0.9768\n",
      "Epoch 197/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0723 - acc: 0.9774 - val_loss: 0.0833 - val_acc: 0.9755\n",
      "Epoch 198/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0781 - acc: 0.9760 - val_loss: 0.0823 - val_acc: 0.9767\n",
      "Epoch 199/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0763 - acc: 0.9760 - val_loss: 0.0822 - val_acc: 0.9763\n",
      "Epoch 200/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0771 - acc: 0.9759 - val_loss: 0.0821 - val_acc: 0.9763\n",
      "Epoch 201/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0758 - acc: 0.9770 - val_loss: 0.0817 - val_acc: 0.9758\n",
      "Epoch 202/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0759 - acc: 0.9761 - val_loss: 0.0816 - val_acc: 0.9759\n",
      "Epoch 203/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0729 - acc: 0.9768 - val_loss: 0.0823 - val_acc: 0.9764\n",
      "Epoch 204/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0746 - acc: 0.9763 - val_loss: 0.0819 - val_acc: 0.9765\n",
      "Epoch 205/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0742 - acc: 0.9772 - val_loss: 0.0818 - val_acc: 0.9764\n",
      "Epoch 206/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0729 - acc: 0.9773 - val_loss: 0.0817 - val_acc: 0.9761\n",
      "Epoch 207/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0760 - acc: 0.9773 - val_loss: 0.0815 - val_acc: 0.9764\n",
      "Epoch 208/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0733 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9761\n",
      "Epoch 209/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0763 - acc: 0.9759 - val_loss: 0.0812 - val_acc: 0.9763\n",
      "Epoch 210/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0737 - acc: 0.9769 - val_loss: 0.0809 - val_acc: 0.9769\n",
      "Epoch 211/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0741 - acc: 0.9767 - val_loss: 0.0811 - val_acc: 0.9762\n",
      "Epoch 212/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0725 - acc: 0.9768 - val_loss: 0.0811 - val_acc: 0.9763\n",
      "Epoch 213/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0736 - acc: 0.9770 - val_loss: 0.0817 - val_acc: 0.9763\n",
      "Epoch 214/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0731 - acc: 0.9776 - val_loss: 0.0806 - val_acc: 0.9760\n",
      "Epoch 215/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0728 - acc: 0.9778 - val_loss: 0.0803 - val_acc: 0.9767\n",
      "Epoch 216/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0726 - acc: 0.9766 - val_loss: 0.0803 - val_acc: 0.9767\n",
      "Epoch 217/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0708 - acc: 0.9779 - val_loss: 0.0804 - val_acc: 0.9763\n",
      "Epoch 218/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0719 - acc: 0.9777 - val_loss: 0.0806 - val_acc: 0.9767\n",
      "Epoch 219/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0698 - acc: 0.9779 - val_loss: 0.0809 - val_acc: 0.9762\n",
      "Epoch 220/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0685 - acc: 0.9785 - val_loss: 0.0808 - val_acc: 0.9767\n",
      "Epoch 221/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0700 - acc: 0.9782 - val_loss: 0.0807 - val_acc: 0.9766\n",
      "Epoch 222/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0690 - acc: 0.9789 - val_loss: 0.0807 - val_acc: 0.9767\n",
      "Epoch 223/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0691 - acc: 0.9781 - val_loss: 0.0803 - val_acc: 0.9761\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0699 - acc: 0.9778 - val_loss: 0.0806 - val_acc: 0.9767\n",
      "Epoch 225/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0701 - acc: 0.9780 - val_loss: 0.0814 - val_acc: 0.9763\n",
      "Epoch 226/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0698 - acc: 0.9779 - val_loss: 0.0802 - val_acc: 0.9765\n",
      "Epoch 227/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0677 - acc: 0.9791 - val_loss: 0.0811 - val_acc: 0.9763\n",
      "Epoch 228/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0679 - acc: 0.9789 - val_loss: 0.0809 - val_acc: 0.9767\n",
      "Epoch 229/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0699 - acc: 0.9783 - val_loss: 0.0802 - val_acc: 0.9767\n",
      "Epoch 230/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0683 - acc: 0.9786 - val_loss: 0.0806 - val_acc: 0.9768\n",
      "Epoch 231/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0683 - acc: 0.9789 - val_loss: 0.0803 - val_acc: 0.9766\n",
      "Epoch 232/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0698 - acc: 0.9782 - val_loss: 0.0803 - val_acc: 0.9762\n",
      "Epoch 233/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0676 - acc: 0.9788 - val_loss: 0.0799 - val_acc: 0.9767\n",
      "Epoch 234/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0679 - acc: 0.9787 - val_loss: 0.0800 - val_acc: 0.9768\n",
      "Epoch 235/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0686 - acc: 0.9785 - val_loss: 0.0804 - val_acc: 0.9768\n",
      "Epoch 236/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0661 - acc: 0.9791 - val_loss: 0.0804 - val_acc: 0.9763\n",
      "Epoch 237/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0670 - acc: 0.9790 - val_loss: 0.0803 - val_acc: 0.9769\n",
      "Epoch 238/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0683 - acc: 0.9791 - val_loss: 0.0802 - val_acc: 0.9768\n",
      "Epoch 239/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0660 - acc: 0.9797 - val_loss: 0.0796 - val_acc: 0.9770\n",
      "Epoch 240/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0650 - acc: 0.9797 - val_loss: 0.0797 - val_acc: 0.9775\n",
      "Epoch 241/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0660 - acc: 0.9791 - val_loss: 0.0795 - val_acc: 0.9773\n",
      "Epoch 242/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0638 - acc: 0.9796 - val_loss: 0.0802 - val_acc: 0.9770\n",
      "Epoch 243/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0654 - acc: 0.9789 - val_loss: 0.0799 - val_acc: 0.9768\n",
      "Epoch 244/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0666 - acc: 0.9784 - val_loss: 0.0793 - val_acc: 0.9774\n",
      "Epoch 245/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0648 - acc: 0.9800 - val_loss: 0.0797 - val_acc: 0.9767\n",
      "Epoch 246/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0658 - acc: 0.9799 - val_loss: 0.0797 - val_acc: 0.9768\n",
      "Epoch 247/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0631 - acc: 0.9809 - val_loss: 0.0798 - val_acc: 0.9764\n",
      "Epoch 248/250\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0650 - acc: 0.9791 - val_loss: 0.0800 - val_acc: 0.9765\n",
      "Epoch 249/250\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.0639 - acc: 0.9795 - val_loss: 0.0800 - val_acc: 0.9772\n",
      "Epoch 250/250\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0649 - acc: 0.9796 - val_loss: 0.0796 - val_acc: 0.9772\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "#model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = 250, verbose = VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/step\n",
      "0.07645218165212428 0.977\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.4816 - acc: 0.8555 - val_loss: 0.1823 - val_acc: 0.9457\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.2257 - acc: 0.9333 - val_loss: 0.1374 - val_acc: 0.9593\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1784 - acc: 0.9469 - val_loss: 0.1177 - val_acc: 0.9660\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1499 - acc: 0.9556 - val_loss: 0.1071 - val_acc: 0.9685\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1329 - acc: 0.9608 - val_loss: 0.1048 - val_acc: 0.9705\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1205 - acc: 0.9645 - val_loss: 0.1058 - val_acc: 0.9702\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.1115 - acc: 0.9667 - val_loss: 0.0984 - val_acc: 0.9717\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.1057 - acc: 0.9692 - val_loss: 0.0957 - val_acc: 0.9733\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0951 - acc: 0.9714 - val_loss: 0.0963 - val_acc: 0.9735\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0934 - acc: 0.9725 - val_loss: 0.0975 - val_acc: 0.9736\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0919 - acc: 0.9731 - val_loss: 0.0939 - val_acc: 0.9767\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0857 - acc: 0.9741 - val_loss: 0.0968 - val_acc: 0.9778\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0852 - acc: 0.9754 - val_loss: 0.0978 - val_acc: 0.9773\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0801 - acc: 0.9763 - val_loss: 0.0957 - val_acc: 0.9776\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0760 - acc: 0.9782 - val_loss: 0.0984 - val_acc: 0.9782\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0768 - acc: 0.9773 - val_loss: 0.0920 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0728 - acc: 0.9787 - val_loss: 0.0997 - val_acc: 0.9773\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0726 - acc: 0.9788 - val_loss: 0.1001 - val_acc: 0.9770\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 30us/step - loss: 0.0741 - acc: 0.9789 - val_loss: 0.1002 - val_acc: 0.9772\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 31us/step - loss: 0.0686 - acc: 0.9798 - val_loss: 0.1008 - val_acc: 0.9781\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "OPTIMIZER = RMSprop()\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "#model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = 20, verbose = VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAEYCAYAAABBWFftAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xl8XHW5x/HPk31Ps3RN2yTQltJSLFB2ZJddVkU2obhwERHkXryCV9GLIm7XhQvCRUVA9kUUpIpYqSgC0kKhLRRa2rRN96TN3uy/+8fvTDpNszXN5EyS7/v1mtecfZ6ZSc6Z5/x+5znmnENERERERET2XkLYAYiIiIiIiAwXSrBEREREREQGiBIsERERERGRAaIES0REREREZIAowRIRERERERkgSrBEREREREQGiBIsGXHM7M9mdulALxsmMys3s+NjvW0z+4aZ3ROLOMzseDNb1r8oRUSkK73tt/u7bJjM7CEz+1ast93bcWlv4jCzRDOrM7PJ/YtU4pkSrGHOzMrM7OSw4+gvM/tjsAOqM7MWM2uOGu/XQcA5d4pz7uGBXjYeBQfLv3YxfWzweU7fk+05577tnLt6AOJKMjNnZiVR217gnJu5t9vu4rWmBK8V+bvZZGbPmdlJe7CNz5nZgoGOTWS4MrMFZrbdzFLDjmUoMbOvRe2rGs2sLWq8Xyeg9mS/PVD7+LCY2UfNrNbMMrqYt8TM9ui9DeRxycz+YWZzo7bd5pzLcs6tHYjtd3qtcjPbEfzdVJnZK2Z2lZlZH9efYma6Ue5eUIIlcc05d3qwA8oCHgZ+EBnv6iBgZkmDH2VcexA4toszZBcDbzrnlocQUyii/o4OAv4KPGtml4UclsiwE5w4+SjggLMH+bWH9DHAOffdqH3V1cCrUce83X7oD/X3O9Ccc38HNgPnR083s9nAVODxMOIKyenB31EJ8EPga8C9oUY0gijBGsHM7PNmttLMtpnZs2Y2IZhuZvYTM9tiZtVm9o6ZHRDMO8PM3g3OEK03sxu72XaCmX3dzNYE23nQzHKDeSVBi8IVZrbWzCrM7L/6+R5ODlrpvmZmm4BfmFmBmc0zs63BGdTnzKwoap2Os0hBy8TfgvdbZWarzOyUfi67b7B8rfmuhXeb2f3dxN2XGP/bzP4ZbO9PZpYfNX9u8NlWmNlN3X0+zrk1wMtA50TicuCBYFtTzewlM6sMtvebyHfVRdzfiX5PPcVhZkea2WvBZ7XRzO4ws+Rg9svB87LgDNsFke8yav2ZweddZf7M45lR8x4KtvfH4PN51cxKu/scOn0mG51zPwG+DfzAzJ/RC/5eVwXbW2ZmZwfTZwF3Ah8NYq0Ipp9tZouD5dea2Tf68voiI8DlwGvA/cAV0TPMLN3M/ifYb1QH+7r0YN4xwT6vyszWRe17F5jZ56K2MdfM/hE17szsi2a2AlgRTPtZsI0aM1tkZh+NWj4xOGZ8GPz/LjKzSWZ2l5n9T6d4nzOzL/flTQf7hGVB/AvMbP+oeWVmdqP542m1mT1uZml9/UCjthNp/b/GzFYCy4Ppd5pvtagxszfM7KiodTr227azRf/yYPmt0fvuPVw2I9gXV5n/XXBT9D68i9h7i/HRYHu1ZrbUzA6Omn9I1P72UaCnltEH8X+D0S4HnnXObTf/++Qp870ZdvuuOsXc+bjUbRzWw3HdzL4PHAncExxHfmqdenKY2ajg/W8N/l5uNus4PvX4+6Mnzrkq59zv8CdWP2tBzxXr+Rj2crBMpPX0UNuD3wqiBGvEMrMTgduBC4HxwBrgsWD2KcCxwDRgFPApoDKY9yvg35xz2cAB+JaArswNHicA+wBZ+B+p0Y4B9gNOAm7pbgfXBxOD7U8GrsH/Xf8iGC8GWoCf9bD+UcASoAD4Cf499mfZR4FXgnnfYfekJlpfYrwE/+NkLJAJ/Dvs8oP/EqAImACM6+G1HiDqYGNmM4GZ7Py+LYh3PDAD/331miz0IY5W4HqgEDgaOA34t2DescHzzODM7NOdtp0C/AF4HhgN3AA8bmZToha7JIgzH1iLT5j2xG/x7zmyzQ+COHOB24BHzGysc24JcC3w9yDWwmD5Ovx3nAt8HLjezM7awxhEhqPL8T0OHgZONbOxUfN+BByC35fmA/8JtJtvZf8j8L/4//nZwOI9eM1zgcPx+zCAN4Jt5AOPAE9GJTT/jv+xeQaQA3wGaMDvKy82swQAMyvEH58e7e3FzWxasNyXg/jnAc8F+7KIC/H7wVLgQPwxsr/OBg4FZgXjrwfbzAeewr/fnpKQo/D7vlOB/zazqf1Y9lb8fr8kmNdbj4DeYjwX+A3+d8cfgTsAgmV+D9wXrPv7YNnuPAicEJXcJOK/7wejlvkDvkVrHLA0eN0e9SGObo/rzrmvAq8CVwfHka6S9p8DGfhj8InAZ9k1UdyT3yq7cc69CmzCty5Dz8ewY4N1Iq2nb9DP3wojlnNOj2H8AMqAk7uY/it8d7vIeBZ+Z1CC/8f+ADgCSOi03lr8j+ScXl53PnBN1Ph+wfaTgtdwwMSo+f8CLuplm/cD3+k07WSgEUjpYb05wNao8X8Ac4PhzwHLo+blBLEV7smy+B1NE5AeNf8x4P4+fk9dxXhT1Ph1wB+C4VuBhzp9d23A8d1sOwu/Iz0sGP8+8HQPsXwCeCNqvDyybfzO9f5+xnEj8GQwnBR8diWdvsuyYPgEYD1gUfOfBL4eDD8E3BM172xgaTevOwVw3XwuDji8m/WWAmdGffcLevkO7wR+2Nf/TT30GI4P/Imzlqh96HLghmA4AdgBfKSL9W4GnulmmwuAz0WNzwX+ETXugBN7iWt75HWB94FzulnuPeBjwfC1wLw+vu9vAE9EjScE+7Djg/Ey4LKo+T+I3od1s81d3mcwLbLvPLaH9QyoxZ/A6rzfnhKsPy5q+TeBT/Rj2bXASVHzro7sw/vweXUV45+i5h8I1AXDJwLrOh0P/gV8q4ftLwD+Mxg+Hd9tMKmbZQuD95kZjD8U2Ta7Hpf2KA56+O3R6bssAZLxJyWnRc3/IvCXYLjH3ypdvHbHcbvT9IXAV7tZp+MYRjfHzU7L7/JbQY9dH2rBGrkm4FutAHDO1eFbqYqcc3/F/6PdBWw2s3vNLCdY9AL8Wb81QXP1kX3ZfjCchG+NidgUNdyA/8HbH5udc82RETPLNLNfBk3eNfhWtsLuV98tDnqIpbtlJwCVzrkdUfPXdfeCfYyxu89nQvS2g+9uW3evFcx/Grg8ODN7CUH3wCCWcWb2hPkunzX4RLanzyuixzjMbLqZPR90w6jBJ2R92W5k22tdsBcPrMG3lEXs7d9PZFvbgnjnmtnbQfeLKmB6T/Ga7wK5IOjOUY0/APb1/YkMV1cAf3bOVQTjj7Czm2AhkAZ82MV6k7qZ3le77G/N7D/M7D3z3fGq8GfpI/+fPb3WA+xsibmMPrRsBDofU9uDmAZynxWt8/v9TzNbHuyLtuN7PXS7P3LO9TmWHpYd3ymObo95fYyx8+tkBsMTgPIujgc9ie658WngYedcaxBHopn9IOhmVwOsDJbrbf/dYxz9+O0RbQyQyO6/m3r6+4H+Hfcix7w9OobtxW+FEUkJ1si1Ad+EDfgdA77ZeT2Ac+4O59wh+K5k04CvBNPfcM6dg98Z/A54oi/bxzeZt+LPIg20zpVu/hPfBeMw51wO/qxTrG0ECmzXPvWTelh+b2LcGL1tM8vCd1foyQPARfhuHGn47hcR38e3vs0KYpmLP7u4t3H8H74VaEqw3VuitttbdaINwKRI//PAZIK/zwFyHv6AtdLM9gHuBr4AFDjnRuHPvPcU72P4xHWScy4X+CV9+9xEhiXz11JdCBwXnFjZhO/e+xEz+whQge9xsG8Xq6/rZjpAPb7rVERXXaI7/kfNX2/11SCWvOD/uZqd/589vdZDwDlBvPvjj3N90fmYavj940Dus6JFv98T8N0eL8B3r8vD91qI9f5oE76LfkS3x7y9jHFjp9cBfzzoyZNAqZkdB5zDrt0DL8efKD4Rn3hHuon3FktvcfR2XO/puLcF3wOk8++mAfv7MbMj8Ce5I9cv9nQM6yrW/v5WGJGUYI0MyWaWFvVIwp9VvNLMZgf9ir8LvO6cKwsuZjzcfEGCevwBsc3MUszsUjPLdc61ADX4HUJXHgVuMLPS4If3d4HHI2eQYiwbf3Znu5kV4H/Yx5Rz7kN83+hvBp/TMcCZPayyNzE+if8BcGTw3X2H3hOWl/Df5d3AI8H3Fx1LPVBtZpPwXfkGIo5s/I+aevPX10Wuv8I514ZvMd2nm23/E5+Q/4eZJZu/ZvAMuk/o+8x8ifrrgK/ju0o4dnYX3OoXsc/hW7AiNgMTbWeRjsj72+acawwOXBftbWwiQ9y5+GPCDPz1T7PxScrfgcuDVp37gB+b2YSgJSGy/3gYONnMLjR/8X+B+cpv4K/FOt98UYUp+GtTepKN339sBZLM7BZ8l6qIXwLfNn/RvpnZgcF+GOdcOf76rd/gu1LvoG+eAM40s5OC/cR/4H+M/rOP6++NyPutwHc1+xY7W39i6Qnga+aLM0zEd2nrzt7E+A8gwcyuDf42Pgkc3NMKQY+K3+JPLq50zkVfz5eN/24q8Yn7bQMUR2/H9c10c8wLjslPAd81syzzRZtuwCf8e8XMcs0XbXoE3/3zvah4uzuGbQFccPKRqOX781thRFKCNTLMw/d7jzy+5Zybj+8z/jT+rMy+7PznysFfqLkd30Rdib8wGXxTe1nQPHw13V/Ueh/+APUysBqfpH1pQN9V936MPytViT+4/bHnxQfMxfgLQyuBb+LLwTZ1s2y/Y3TOvYMvHvEE/uzWJnbtOtDVOg7/fRSz65k8glgPwydDz+L/JgYijv/Adw2qxbdmdS6P+018IYkqM9ulpK5zrgl/0e05+APyHcAlzrkP+hJbVyyohgS8g2/JO98592DUe7kD359+Iz65ej1q9Rfx1ck2B2flwbd23W5mtfjyt3ud/IkMcVcAv3bOrXXObYo88F3OLw1O7t2IPxn1Br6r0vfx1/quxZ9E+Y9g+mLgI8F2fwI043+gPoBPxnryAn6f+gH+GNbIrt3Xfoz/f/0z/kThr4D0qPkP4ItH9LV7IM659/HHw//F77M+Dnw8uvt6DM0D/oLfR5Xh39PGQXjdb+K/kzL8Z/kE3R/z+h1jcDw4D/g8/nfJ+fStZfEBuj7m/Rrf4rgBWEYfk+A+xNHbcf2n+CIqVWb24y5e4hr83/lq4G9B/J1j3xN/DI55a4Gb8KXaPxc1v9tjmHOuFl8I7fUg3jn087fCSGW7diUVkYFiZk8Di51ze1rdTkREQmJmx+JbDkqCVjfpAzP7EnCuc67PN3EXGa7UgiUyQMzssKBLZIKZnQGchS/jKiIiQ0DQve964JdKrnpmZkVmdlRwzNsf36XtmbDjEokHugO4yMCZgG8yz8eXSP180PVMRETiXJAkLATeBq4MOZyhIBV/OUEJvsvco/ju4CIjXky7CJrZafibrCXizwZ9r9P8ufg+oZEqKXc6534Zs4BERERERERiKGYtWObvnH0X8DH82fw3zOxZ59y7nRZ93Dl3baziEBERERERGSyx7CJ4GL405ioAM3sMXxGsc4K1RwoLC11JScneRyciIqFatGhRhXNudNhxxJqOWyIiw0Nfj1uxTLCK2LUsajlweBfLXRBU7PkAuME5t9udwM3sKuAqgMmTJ7Nw4cIYhCsiIoPJzNaEHcNgKCkp0XFLRGQY6OtxK5ZVBLu6u3PnC76ew5dBPRB/f4QHutqQc+5e59wc59yc0aOH/clOEREREREZomKZYJUDk6LGJ+Jv6tbBOVcZ3LgNfCWaQ2IYj4iIiIiISEzFMsF6A5ga3BcoBbgIf+fnDmY2Pmr0bOC9GMYjIiIiIiISUzG7Bss512pm1wIv4Mu03+ecW2ZmtwILnXPPAteZ2dlAK7ANmBureERE9lRLSwvl5eU0NjaGHcqQlpaWxsSJE0lOTg47FBERkZiL6Y2GnXPzgHmdpt0SNXwzcHMsYxAR6a/y8nKys7MpKSnBrKvLSqU3zjkqKyspLy+ntLQ07HBERERiLpZdBEVEhrTGxkYKCgqUXO0FM6OgoCCuWgHN7DQze9/MVprZTV3MLzaz+Wb2jpktMLOJUfN+YGbLzOw9M7vD9MchIiKdKMESEemBfj/vvXj6DM0sEbgLOB2YAVxsZjM6LfYj4MGgwu2twO3BukcBRwMHAgcAhwLHDVLoIiIyRMS0i2A8evmDrbS1O06YPibsUEREZPAdBqx0zq0CMLPHgHOAd6OWmQHcEAy/BPwuGHZAGpCCvxVJMrB5EGIWEZE90NjSRlVDC9sbmv2jvoWWtnbOPahoUF5/xCVYd720klYlWCIiI1UREH1D+3Lg8E7LvA1cAPwMOA/INrMC59yrZvYSsBGfYN3pnOuy+q2ZXQVcBTB58uSBfQciIsOUc452B+3O0e4czkFzWzvVQbK0rb45KnFqoarTtKqGFrbVN7OjpW23bWenJinBipXSwkxefFcnHEUk/lVVVfHII49wzTXX7NF6Z5xxBo888gijRo3ao/Xmzp3LWWedxSc+8Yk9Wm+I6aq/ous0fiNwp5nNBV4G1gOtZjYF2B9/X0eAF83sWOfcy7tt0Ll7gXsB5syZ03n7IiJdcw5adkBjNTTV+OfGGmiqjhqOmt6xXDCckgF5pZBXAvmlO4fziiE5fbeXq29qpXz7DtZta6B8ewPrtu9g/fYd7Ghpo9052tr9o2PYQXvUtMj0dsduy7a7SMK0M2lyUcnTzvk7n/eEGeSmJ5OXkcKojGTG5aQxfVwOeRnJ5GWmkJeRQl5GMqMyUsjP9MODZcQlWCWFmVTWN1PT2EJOmkoGi0j8qqqq4uc///luCVZbWxuJiYndrjdv3rxu5wnlwKSo8YnAhugFnHMbgPMBzCwLuMA5Vx20Sr3mnKsL5v0ROAKfhIlIrDgHDdugdgPUbYaEZEjNgpRsSM32w8mZkBBiaYG2FmiqheY6/9wUPDfX7hxvrqN1RzU11VXU12ynqb6a1h01WHMd6a6eXBrIpIEkdm99ieYSkrDUHEjLgbRcSM3xyVRqjn+9bWWw5hUfS5SG1DFUpExgvY1jVWshy3YU8F5jPmvcWLaRDRjpyYkU5aWTmZpEokFigpFgRnJiAqlJRkKC7TI9MSEyzUgwOoYTEwwzIzEBEswvaxYZJhiPTNs5nhC1jAXrJSUYuRnJ5GekkJfpE6a8jBRy05NJTAjOmbW3Q2MV1FdAQ8XO520VsC4Ydg4++evYfP+djLwEqyATgLKKeg6cuGdnd0Vk5Prv55bx7oaaAd3mjAk5fPPjM7udf9NNN/Hhhx8ye/ZskpOTycrKYvz48SxevJh3332Xc889l3Xr1tHY2Mj111/PVVddBUBJSQkLFy6krq6O008/nWOOOYZ//vOfFBUV8fvf/5709N3PYnY2f/58brzxRlpbWzn00EO5++67SU1N5aabbuLZZ58lKSmJU045hR/96Ec8+eST/Pd//zeJiYnk5uby8stxnW+8AUw1s1J8y9RFwCXRC5hZIbDNOdeOv5XIfcGstcDnzex2fEvYccBPBytwkWGptQlqN0LNRp9A1WwMxjdEPW+CtqZeNmSQkrUz4eoYDh7R81JzIDkD2lv867c2Qmtz8NwIbZHhzvOafBwd05t8a1NznR/vgxaXQhtptLp0miydtqQsEjLG0pSSzRqXwba2VCpa0tjcnMqmphSqXSa1Lp0aMqhxmdSSTnNCGvmJqRS4FAoTUylIS6EgM5WCrBQaW9pY5xoodw3Ubt9ERt06JtkWim0zxW1bKN6xhSmJ/+JIt80HlOqf2pKzcKOKSSzcB8srgbRRkJQKSWk7nxNTosZTo+Z3MS8xxTcxRbS3Q3sruDb/3N4K7V0Nt/nvJTLdtfvPtj5ImrZU7kyg6rdCQ2WQTFX6bXclNQcyCyFncLoHwghMsEoLfYK1WgmWiMS5733veyxdupTFixezYMECzjzzTJYuXdpxP6n77ruP/Px8duzYwaGHHsoFF1xAQUHBLttYsWIFjz76KL/4xS+48MILefrpp7nssst6fN3Gxkbmzp3L/PnzmTZtGpdffjl33303l19+Oc888wzLly/HzKiqqgLg1ltv5YUXXqCoqKhjWrxyzrWa2bXAC0AicJ9zbpmZ3QosdM49CxwP3G5mDt869cVg9aeAE4El+G6Ff3LOPTfY70FkyKmvgFULoGLFrklU7Ub/w7izpHTIGQ/ZE2DSYZA93j9yxkPWOP+ju6O1qCaqtSjSehQ1XF+xa0tSe2v3cVpCLwlFGqTkQeLOcZeUSj3pbGlOZuOOZNbVJ7C6NoGV1cb21lTqSKeedEbl5TNx7BimjR/FtLHZ7Dcum2mFmSQndt/q1tbuqGpoprK+mYq6Jirrmqmsa6KirpnK+uC5rol16xqorGumrqmVBIPxuelMyk9n1rQpTMo7kEn56UzKz2BiXjpjs9NISDCfGG5fA9tXw/YyErf5Z7a+Dx/8uQ8JbR8kpvrvqr2V3Xti74W0UT5hyiiE/H1g4qF+PHO0n5ZZEDwXQkaB/64G2YhLsIoLMgAoq2gIORIRGUp6amkaLIcddtguN+u94447eOaZZwBYt24dK1as2C3BKi0tZfbs2QAccsghlJWV9fo677//PqWlpUybNg2AK664grvuuotrr72WtLQ0Pve5z3HmmWdy1llnAXD00Uczd+5cLrzwQs4///yBeKsx5ZybB8zrNO2WqOGn8MlU5/XagH+LeYAiQ11bC5S/ASvnw4fzYcNi/A9s8z+Cc8ZD7iT/wzhnws7kKXuCf04btWvrx0Bxzrc6Nfvueh0JVMfzzp/FjS1tHUlNRV1T8NiZ6FRU+WkbqxupbdyZtI3JTmW/cdnsNz2bU8dlM31cNlPGZJGRsuc/uRMTjIKsVAqyUpk2NrvX5Rtb2khMsB6Ttg7J6TBmun905lzQihfVWtcW1cLXU4te9HBbE1giJCQFj8ROz52Hk3ySGz2ekASJyT5RiiRMifF/ic+IS7DSkhOZkJvGmsr6sEMREdkjmZmZHcMLFizgL3/5C6+++ioZGRkcf/zxXd7MNzV155m7xMREduzY0evrONf1mcakpCT+9a9/MX/+fB577DHuvPNO/vrXv3LPPffw+uuv8/zzzzN79mwWL168W6InIsPc9jU+mVo5H1a/7FuWLNEnUSd8DaacBGNnQVJKaCG2OajYARuqEtlUnUpFXRNb6+qCVqHoZMq3BnUlMyWRgqxUCrNSKCnI5LDSfN8iNTabaWOzycsM7/2lJXd/be4eMdvZaif9MuISLIDigkxWK8ESkTiXnZ1NbW1tl/Oqq6vJy8sjIyOD5cuX89prrw3Y606fPp2ysjJWrlzJlClT+M1vfsNxxx1HXV0dDQ0NnHHGGRxxxBFMmTIFgA8//JDDDz+cww8/nOeee45169YpwRIZ7prroewfO1upKlf66bmT4YDzYd+ToPRYSB+cyzGcc1TWN7OxqpEN1TvYWLWDjdWNbKhu7BjeXNNIaxel6vIykinM8tcwHVCUS2GQQBUGrUeR4cKsVNJTBiiJkWFtRCZYJYWZ/GnpxrDDEBHpUUFBAUcffTQHHHAA6enpjB07tmPeaaedxj333MOBBx7IfvvtxxFHHDFgr5uWlsavf/1rPvnJT3YUubj66qvZtm0b55xzDo2NjTjn+MlPfgLAV77yFVasWIFzjpNOOomPfOQjAxaLiMQJ52Dzsp2tVGtf9d3GktKh5Bg49HMw5WQomDLg3fuaWtt897zaJrbWNrGxZmfStCF43lTdSHNb+y7rpSQmMC43jfG5aRxWms+43DQm5KYxPjedcblpjMlOJS8zpW9d6kT2gHXXFSRezZkzxy1cuHCvtnHvyx/y3XnLefuWU8gdxJr4IjK0vPfee+y///5hhzEsdPVZmtki59yckEIaNANx3JIRrL3dJzJtzf7apvaWncMd01u7X8Y5X2ig84Po6d0sE3lUfuiTqrpNPqYxM2HKib6VavKRkJy2x2+rpa29o0ve1tomtkaea5s6pkWeaxp3766XlGCMzUljwqg0xuWmB4lTGuNHpTMhSKAKMlN8QQeRAdLX49bIbMEKSrWvrqxndoYqCYqIiMggc86XH9/2IWxb5ZOYbav8o3pdUCSgOUiGQpaeB/uc4K+j2vdEyJmAc46m1nZqGluoq6qjtrGVuqZWahtbqW1s6RiOnra9obkjidre0NLlS2WlJjE6O5XRWb5YxDFTCinMSmV0tu+iV5idyvjcNAqzUnfeA0kkzozIBCtSqr2sop7Zk5RgicjI8sUvfpFXXnlll2nXX389V155ZUgRiQxTPSVR21ZBS1RF44RkyCuG/H2h+Chf5S0xxT8SknYOJ0YPJ/v1IsMdz8k717NE32XPErp9ODNqm9uprG+hoq6ZrXWtVNS3sLW+mS21LVQ0OGq3tVH7cit1Ly6nrnEptY2tXV7P1FlacgLZaclkpSaRl5FMaaEvDBGdNEUSKl3jJMPFiEywJuVnYObvhSUiMtLcddddYYcgMny0t/mb4Qb3E+o5iUqCvBKfRJUe6+/hE3nkTtqlTPhAiFy7FGk18o9GttY1doxvCZ6bWndvKUtKMEZnp5KXkUJ2WhJFo9LITssmKzWJ7LQkstKSyE5LJjs1aZdpOUFClZWWpOubZEQakQmWL9WeTpkqCYqIiEhvmhugag1sW70zkYoMV631XfkiopOoko9Cwb4xTaIiqne0sGx9NUvWV/PO+mqWrq9mTWXX9/zMz0xhdNBydGhJZkcL0ujsqEdWKrnpybqGSaQfRmSCBb6bYJlasERERIae+grYvNRXtdux3d+vJzHV3yw2cv+epKjx3uYlpkBDpU+YtgUJVPRwpLhDRGqOT6LGzIDpZ/rhvFL/HMMkKqK2sYWl62tYsr6KJetrWFJeRVlUMjUxL51ZRbmcf9BExuVGEqY0Rmf7UuRqVRKJrRGbYJUUZvDs4g0457BY3C1cRERE9k5rM1R8ECRTQUK1eRnUbY5ayICBrohskDPBJ01TTob8kiCBKoX8Ul/0YZAviVgUAAAgAElEQVR+O9Q2trBsQw1Lyn3r1NL11ayKOkFcNMonU5+cM4lZRbkcUJRLfog3uxWRkZxgFWRS09hKVUNLqHfdFhERGfGc80nTpk6JVMX70B6U6E5MhTHTfcIzdiaMPcA/ZxT4ZVqbgkejf7Q1B8NNneY1QVvUcGujT+Qy8ne2RI2a3K/S43urqbWNpetreGvtdpYE3f1WV9QTuaPOhNw0Zk3M5fyDi5g1cRQHTMihICt10OMUkZ6N2AQrUklwdWW9EiwRGRaysrKoq6vrcl5ZWRlnnXUWS5cuHeSoRDppb4ety2HDm0EiFSRUDZU7l8mZ6JOnaafCuAN8MpW/b/dd7yKV81KzBuc9DJAtNY28uXY7i9Zs5821VSwpr+64We743DQOKMrlvNlFHDAxl1lFuRQqmRIZEkZsglVcsLNU+8GT80KORkREZJhqrIH1C2Hdv2Dd61C+CJqq/bzkDBizv7+OaeysoGVqhu+CN8y0trWzfFNtR0K1aM12yrfvACAlKYEDi3KZe3QJB0/O4+DiUYzJHvwWNBEZGCM2wZqcn0GCoUIXItI3f7wJNi0Z2G2OmwWnf6/b2V/96lcpLi7mmmuuAeBb3/oWZsbLL7/M9u3baWlp4Tvf+Q7nnHPOHr1sY2MjX/jCF1i4cCFJSUn8+Mc/5oQTTmDZsmVceeWVNDc3097eztNPP82ECRO48MILKS8vp62tjW984xt86lOf2qu3LcOYc740eSSZWvcv2PIu/hop8wnUrAtg0uFQdIivrJcwPO97tL2+mbfWBa1Ta6p4u7yKhuY2AMbmpHJIcR5zjyrh4OI8Zk7IITVpeH4OIiPRiE2wUpISKMpLZ3U3JUxFRMJ20UUX8eUvf7kjwXriiSf405/+xA033EBOTg4VFRUcccQRnH322XtUrCdyH6wlS5awfPlyTjnlFD744APuuecerr/+ei699FKam5tpa2tj3rx5TJgwgeeffx6A6urqgX+jMnQ1N8CGt6D8XzuTqkhXv9RcmDgHZpwDkw7zCVVaTrjxxkhTaxsfbKpj6YZq3lyznUVrt7Nqqz+Bm5hgzJyQw4VzJnFwcR6HFOcxITdNBbZEhrERm2CBL3ShFiwR6ZMeWppi5aCDDmLLli1s2LCBrVu3kpeXx/jx47nhhht4+eWXSUhIYP369WzevJlx48b1ebv/+Mc/+NKXvgTA9OnTKS4u5oMPPuDII4/ktttuo7y8nPPPP5+pU6cya9YsbrzxRr761a9y1lln8dGPfjRWb1eGgpZG+OCPsPZ1n0xtemdnEYqCqTDtdJh0qG+hKtwPEoZfOfD6plbe21jD0vXVLNtQw9INNazYXEtru69EkZeRzCHFeVxw8EQOKc7jwIm5ZKSM6J9bIiPOiP6PLy3M5Jk316tUu4jErU984hM89dRTbNq0iYsuuoiHH36YrVu3smjRIpKTkykpKaGxsXGPtulc1yWtL7nkEg4//HCef/55Tj31VH75y19y4oknsmjRIubNm8fNN9/MKaecwi233DIQb02GmhUvwryv+PtDJWf4Fqmjr4eJh8HEQyGzIOwIB1xVQ7NPojqSqV2r+hVkpjCzKJcT9hvNzAm5zJyQQ3FBhn5TiIxwIzrBKinIpLaplcr6ZlXmEZG4dNFFF/H5z3+eiooK/va3v/HEE08wZswYkpOTeemll1izZs0eb/PYY4/l4Ycf5sQTT+SDDz5g7dq17LfffqxatYp99tmH6667jlWrVvHOO+8wffp08vPzueyyy8jKyuL+++8f+Dcp8W37Gnjha7D8D1AwBS55EvY9MeY30x1Mzjm21DaxbEM1S9fXdDyvr9rRsUzRqHRmTsjhnI8UcUBRDjMn5DI2J1XJlIjsZvjsHfshUqq9rKJeCZaIxKWZM2dSW1tLUVER48eP59JLL+XjH/84c+bMYfbs2UyfPn2Pt3nNNddw9dVXM2vWLJKSkrj//vtJTU3l8ccf56GHHiI5OZlx48Zxyy238MYbb/CVr3yFhIQEkpOTufvuu2PwLiUutTbBP++Al//H31T3pG/CkV+EpOFxvHTO8da6Kp5/ZyN/WrqpI5kyg9KCTA4uzuPTRxZzQNAypVu6iEhfWXddReLVnDlz3MKFCwdkW6sr6jnhRwv44ScO5JNzJg3INkVk+HjvvffYf//9ww5jWOjqszSzRc65OSGFNGgG8rg1aFb+Beb9J2z70BepOOU2GDX0j5ORpGreOxuZt2QjG6obSUlM4NhphRw9pZADinLZf3wOWakj+vyziHSjr8etEb0HmZiXTmKCUVapQhciIiJUrYMXbob3nvM39r3stzDlpLCj2ivOORavq2Leko3MW+JbqiJJ1Y2n7sfJM8aSk5YcdpgiMoyM6AQrOTGBSXnplKlUu4gME0uWLOHTn/70LtNSU1N5/fXXQ4pIhoTWJnj1TvjbD/34id+Ao740ZLsDOud4u7yaeUs28vw7G1lftYPkROPYqaP5j1OmKakSkZga0QkWQLFKtYtID4ZaldFZs2axePHisMPYxVDrij7irJzvqwNu+xD2/zic+l0YNTnsqPaYc453yqt5vlNS9dGpo/n3j/mkKjddSZWIxN6IT7BKCzNZWLZtyP2IEpHYS0tLo7KykoKCAu0f+sk5R2VlJWlpaWGH0sHMTgN+BiQCv3TOfa/T/GLgPmA0sA24zDlXHsybDPwSmAQ44AznXNngRT+AqsvhTzfDe89C/j5w6dMw9eSwo9ojzjmWrK/m+Xc28vySjZRv90nVMVMKueFj0/iYkioRCcGIT7BKCjKob25ja10TY7Lj5weAiIRv4sSJlJeXs3Xr1rBDGdLS0tKYOHFi2GEAYGaJwF3Ax4By4A0ze9Y5927UYj8CHnTOPWBmJwK3A5F+lw8CtznnXjSzLKB9EMMfGK3Nvjvgyz8E5+DEr8NR1w2p7oAtbe089/YG7vnbh3ywuY6kBOOYqYVcf9JUTpkxjtwMJVUiEh4lWB2l2huUYInILpKTkyktLQ07DBlYhwErnXOrAMzsMeAcIDrBmgHcEAy/BPwuWHYGkOScexHAOVc3WEEPmA9f8t0BK1fA9LPgtNuHVHfAxpY2nli4jv/72yrWV+1g+rhsvn/BLE6dOY5RGSqjLiLxYcQnWNH3wjqsND/kaEREJMaKgHVR4+XA4Z2WeRu4AN+N8Dwg28wKgGlAlZn9FigF/gLc5Jxr6/wiZnYVcBXA5MlxkMA0N8Dvr4Flz0BeKVz6FEz9WNhR9VlNYwu/eXUNv35lNRV1zRw8eRS3njOTE6ePUfddEYk7Iz7BKhqVTlKCsVql2kVERoKufo13rsJxI3Cnmc0FXgbWA634Y+ZHgYOAtcDjwFzgV7tt0Ll7gXvB3wdrYELvJ+fg91+EZb+D478GR18PyUOjx0ZFXRP3/WM1v3l1DbVNrRw7bTTXHL8vh5fmK7ESkbg14hOspMQEJudnqJKgiMjIUI4vUBExEdgQvYBzbgNwPkBwndUFzrlqMysH3orqXvg74Ai6SLDiyt//B5b9Fk7+FhxzQ29Lx4V12xr4xd9X8fgb62hua+eMA8bzheP35YCi3LBDExHp1YhPsMBfh7VaCZaIyEjwBjDVzErxLVMXAZdEL2BmhcA251w7cDO+omBk3TwzG+2c2wqcCCwctMj74/0/wl+/A7M+CUd/OexoerVicy13L/iQ37+9gQSD8w4q4t+O25d9R2eFHZqISJ8pwQJKCjJ59cNKlWoXERnmnHOtZnYt8AK+TPt9zrllZnYrsNA59yxwPHC7mTl8F8EvBuu2mdmNwHzzB4tFwC/CeB99smU5PP15GP8ROPt/IY6Pb4vXVfHzl1by53c3k56cyBVHlvD5Y0sZn5sedmgiInsspglWb/caiVruE8CTwKHOuUE/G1hamMGOljY21zQxLndo9EsXEZH+cc7NA+Z1mnZL1PBTwFPdrPsicGBMAxwIDdvg0YsgOR0uesQ/xxnnHK+srOTnC1byzw8ryU1P5rqTpjL3qBLyM1URUESGrpglWH281whmlg1cB7weq1h601GqvbJeCZaIiAxtba3w1JVQsx6u+APkFoUd0W4Wlm3j2394l7fLqxmTncp/nbE/Fx8+maxUdawRkaEvlnuyvtxrBODbwA/wVZtCUVKws1T7EfsUhBWGiIjI3nvxG7BqAZx9J0zuXIE+XM457v9nGbc9/x5jc9L47nmzuOCQIlKTEsMOTURkwMQywer1XiNmdhAwyTn3h6Bfe5difT+RCaPSSUlMUKl2EREZ2t56GF77ORx+NRz86bCj2cWO5jZu+u07/H7xBj42Yyz/c+FHyElLDjssEZEBF8sEq8d7jZhZAvAT/D1EehTr+4kkJhiT8tNVql1ERIaudf+CP3wZSo+DU24LO5pdrKms599+s4j3N9fylVP34wvH7UtCQvwW3RAR2RuxTLB6u9dINnAAsCCo3DcOeNbMzg6n0EUmZRUNg/2yIiIie69mAzx+GeRMgE/eD4nxcy3TS8u3cP1jb5GQYNx/5WEcN2102CGJiMRULPfAPd5rxDlXDRRGxs1sAXBjGMkV+Ouw/r6igvZ2p7NqIiIydLTsgMcugeZ6uPz3kJEfdkQAtLc77vjrCn42fwX7j8vh/z59CJPyM8IOS0Qk5mKWYPXxXiNxo6Qwk6bWdjbVNDJhVPyVsxUREdmNc/DsdbDhLV+Ofcz+YUcEQPWOFv798cXMX76F8w8u4rvnzSItWYUsRGRkiGkfgt7uNdJp+vGxjKU3pYU7KwkqwRIRkSHhn3fAkifghK/D9DPDjgaA9zbWcPVDi9hQtYNvnzOTy44oxuL4JsciIgMtIewA4kXkXliqJCgiIkPCihfhxW/CjHPh2NDudLKL3y9ez3k/f4UdzW08dtURfPrIEiVXIjLixM9VsCEbn5NGalKCKgmKiEj8q1gBT30Wxh0A5/4cQk5iWtrauX3ecu57ZTWHleRz56UHMSY7LdSYRETCogQrkJBgFBdksFqVBEVEJJ7tqIJHL4LEZH/dVUpmqOFsqW3k2kfe4l+rt3Hl0SV87Yz9SU5UBxkRGbmUYEUpLshUC5aIiMSv9jZ4+rOwvQyueA5GTQ41nEVrtnPNw4uo3tHCzy6azTmzi0KNR0QkHugUU5TSwkzWbGugvX3A72UsIiKy9/7yLVj5Fzjjh1B8VGhhOOf4zWtruOjeV0lLTuSZa45WciUiElALVpSSgkyaW9vZUL2DiXm6V4eIiMSRtx/3VQPnfBbmfCa0MBpb2vivZ5by9JvlnLDfaH76qYPIzUgOLR4RkXijBCtKSaFPqsoqGpRgiYhI/Fi/CJ79EhQfA6d/P7QwttQ0cuX9b/Duxhq+fPJUrjtxKgkJqhIoIhJNXQSjlKpUu4iIxJvaTfDYpZA1Fi58wBe3CEFjSxuff3Ahqyvq+dUVc/jyydOUXImIdEEtWFHGZqeRlqxS7SIiEidaGuHxy6CxGj77Z8gsDCUM5xw3/3YJb5dXc++nD+HE6WNDiUNEZChQC1aUhASjRJUERUQkXrS3QEYBnHcPjJsVWhj3vryKZ95az42nTOOUmeNCi0NEZChQC1YnJQWZfLClNuwwREREIDUbLn4s1BsJv7R8C9/703LOOnA8XzxhSmhxiIgMFWrB6qSkMJN12xpobWsPOxQREZFQk6uVW2q57tG3mDE+hx9+4iNYiLGIiAwVSrA6KS3MoKXNsaGqMexQREREQlPd0MLnHlhIanICv7h8DukpiWGHJCIyJCjB6qSkwFcSLFMlQRERGaFa29q59tE3WV+1g//79CFMGJUedkgiIkOGEqxOSgqVYImIyMj23XnL+fuKCm47dxaHFOeHHY6IyJCiBKuTMdmpZKQkslqVBEVEhiUzO83M3jezlWZ2Uxfzi81svpm9Y2YLzGxip/k5ZrbezO4cvKgHzxNvrOO+V1bzmaNLufDQSWGHIyIy5CjB6sTMKFapdhGRYcnMEoG7gNOBGcDFZjaj02I/Ah50zh0I3Arc3mn+t4G/xTrWMCws28Z//W4JH51ayNfOmB52OCIiQ5ISrC6UFmZQVtkQdhgiIjLwDgNWOudWOeeagceAczotMwOYHwy/FD3fzA4BxgJ/HoRYB9X6qh1c/dAiikalc+fFB5OUqJ8IIiL9ob1nF0oKVKpdRGSYKgLWRY2XB9OivQ1cEAyfB2SbWYGZJQD/A3yltxcxs6vMbKGZLdy6desAhB1bO5rbuOrBhTS1tPPLK+aQm5EcdkgiIkOWEqwulBRm0truKN++I+xQRERkYHV1IyfXafxG4Dgzews4DlgPtALXAPOcc+vohXPuXufcHOfcnNGjR+9tzDHlnOPGp97m3Y013HHxQUwZkx12SCIiQ1pS2AHEo9KgkuDqyvqOqoIiIjIslAPRlRsmAhuiF3DObQDOBzCzLOAC51y1mR0JfNTMrgGygBQzq3PO7VYoYyi566WVPP/ORm4+fTonTB8TdjgiIkOeEqwudNwLq6Ie9gs5GBERGUhvAFPNrBTfMnURcEn0AmZWCGxzzrUDNwP3ATjnLo1aZi4wZ6gnV39etokf/fkDzjuoiKuO3SfscEREhgV1EexCYVYKWalJqiQoIjLMOOdagWuBF4D3gCecc8vM7FYzOztY7HjgfTP7AF/Q4rZQgo2x5ZtquOHxxXxk0ihuP38WZl31nhQRkT2lFqwumBklhRmsViVBEZFhxzk3D5jXadotUcNPAU/1so37gftjEN6g2FbfzOcfXEhmahL3fvoQ0pITww5JRGTYUAtWN4oLMllTqRYsEREZXlra2rnm4UVsrmni3svnMDYnLeyQRESGFSVY3SgtyKR8+w5aVKpdRESGkVufe5fXVm3j+xfMYvakUWGHIyIy7CjB6kZJYSZt7Y5129RNUEREhoeHXlvDb15bw78dtw/nHTQx7HBERIYlJVjdKC3MAKBM3QRFRGQYeG1VJd96dhkn7Dea/zx1etjhiIgMW0qwuhEp1b66Qi1YIiIytNU0tnDNw29SXJDBzy4+iMQEVQwUEYkVJVjdyM9MITtNpdpFRGToW7a+hm31zXzjrBnkpCWHHY6IyLCmBKsbZkZpYaa6CIqIyJAXOZZNGZMVciQiIsOfEqwelBRkslotWCIiMsSVVdaTkpjA+Nz0sEMRERn2lGD1oKQwkw1VO2hqbQs7FBERkX4rq6hnckGGrr0SERkESrB6UFqYQbtDpdpFRGRIW1PZQElBRthhiIiMCEqweqBKgiIi8cvMrjWzvLDjiHft7Y6yyvqOY5qIiMSWEqweRA5Ga1ToQkQkHo0D3jCzJ8zsNDNT/7cubKltorGlneJCJVgiIoNBCVYP8jJTyE1PVqELEZE45Jz7OjAV+BUwF1hhZt81s31DDSzORI5hpWrBEhEZFEqwelGiUu0iInHLOeeATcGjFcgDnjKzH4QaWByJ9MIo1jVYIiKDQglWL0oLMijTNVgiInHHzK4zs0XAD4BXgFnOuS8AhwAXhBpcHFkdlGifMEol2kVEBkNME6ygT/z7ZrbSzG7qYv7VZrbEzBab2T/MbEYs4+mPksJMNlTvoLFFpdpFROJMIXC+c+5U59yTzrkWAOdcO3BWuKHFjzUVDUzKT1eJdhGRQRKzBMvMEoG7gNOBGcDFXSRQjzjnZjnnZuPPQP44VvH0V2lhJs7BWpVqFxGJN/OAbZERM8s2s8MBnHPvhRZVnCmrrKdUBS5ERAZNLFuwDgNWOudWOeeagceAc6IXcM7VRI1mAi6G8fTLzlLtug5LRCTO3A3URY3XB9Mk4Jwv0V6sAhciIoMmKYbbLgLWRY2XA4d3XsjMvgj8O5ACnNjVhszsKuAqgMmTJw94oD0pCc76lSnBEhGJNxYUuQB810Azi+VxbcjZXONLtJeoBUtEZNDEsgWrq87eu7VQOefucs7tC3wV+HpXG3LO3eucm+OcmzN69OgBDrNnuenJ5GemqJKgiEj8WRUUukgOHtcDq8IOKp5Ejl0lqiAoIjJoYplglQOTosYnAht6WP4x4NwYxtNvJQUZ6iIoIhJ/rgaOAtazs5fEVaFGFGcivS9K1EVQRGTQxLIrxRvAVDMrxR/8LgIuiV7AzKY651YEo2cCK4hDJQWZvLqqMuwwREQkinNuC/7YIt0oq2xQiXYRkUHWpwTLzPYFyp1zTWZ2PHAg8KBzrqq7dZxzrWZ2LfACkAjc55xbZma3Agudc88C15rZyUALsB24Yu/eTmyUFGby27fWs6O5jfSUxLDDERERwMzSgM8CM4G0yHTn3GdCCyrOlFXUq0S7iMgg62sXwaeBNjObAvwKKAUe6W0l59w859w059y+zrnbgmm3BMkVzrnrnXMznXOznXMnOOeW9fN9xFTk4uA129RNUEQkjvwGGAecCvwN3xW9treV+nCPxmIzm29m75jZAjObGEyfbWavmtmyYN6nBvj9DLiyynp1DxQRGWR9TbDanXOtwHnAT51zNwDjYxdWfCktUCVBEZE4NMU59w2g3jn3AL6r+ayeVujjPRp/hO+lcSBwK3B7ML0BuNw5NxM4DfipmY0asHczwJxzrKlsUAVBEZFB1tcEq8XMLsZ34ftDMC05NiHFn5JCX31pdYVuNiwiEkdagucqMzsAyAVKelmn13s04hOv+cHwS5H5zrkPItcNO+c2AFuAwS1tuwe21Daxo6VNFQRFRAZZXxOsK4Ejgducc6uDwhUPxS6s+JKdlkxhVopasERE4su9ZpaHv8XHs8C7wPd7WaerezQWdVrmbeCCYPg8INvMCqIXMLPD8Pdv/LCrFzGzq8xsoZkt3Lp1a1/ey4CLVL9VC5aIyODqU5EL59y7wHUAwcEs2zn3vVgGFm9KCjJZrXthiYjEBTNLAGqcc9uBl4F9+rpqF9M636PxRuBOM5sbbHs90Br12uPx139d4Zxr7+pFnHP3AvcCzJkzZ7d7QA6GNZUq0S4iEoY+tWAFF/nmmFk+/szer83sx7ENLb6UFGaqBUtEJE4Eic21/Vi113s0Ouc2OOfOd84dBPxXMK0awMxygOeBrzvnXutP7INldUUDyYmmEu0iIoOsr10Ec51zNcD5wK+dc4cAJ8curPhTWpjJltom6ptae19YREQGw4tmdqOZTTKz/Mijl3U67tFoZin4+2g9G72AmRUGLWQANwP3BdNTgGfwBTCeHNi3MvDWVNYzKT9DJdpFRAZZXxOspKBLxIXsLHIxokS6WJSpm6CISLz4DPBFfDe+RcFjYU8rBBVxI/dofA94InKPRjM7O1jseOB9M/sAGAvcFky/EDgWmGtmi4PH7AF+TwNmdUV9RxVcEREZPH26BgtfpvYF4BXn3Btmtg+wInZhxZ9IJcGyigZmTsgNORoREXHOlfZzvXnAvE7Tbokafgp4qov1HmKIFHiKlGg/at/CsEMRERlx+lrk4kngyajxVeyssDQiFKsFS0QkrpjZ5V1Nd849ONixxJtIifbSQpVoFxEZbH1KsIK72P8vcDS+2tI/gOudc+UxjC2uZKUmMTo7VYUuRETix6FRw2nAScCbwIhPsCLHqmJ1ERQRGXR97SL4a+AR4JPB+GXBtI/FIqh4VVqQqRYsEZE44Zz7UvS4meXiy6ePeJFjVanugSUiMuj6WuRitHPu18651uBxP3F89/pYKSnMYHVFQ9hhiIhI1xqAqWEHEQ/KKn2J9vG5aWGHIiIy4vS1BavCzC4DHg3GLwYqYxNS/CopzKSirpzaxhay05LDDkdEZEQzs+fYeZPgBGAG8ER4EcWPsgpfoj0psa/nUUVEZKD0NcH6DHAn8BP8weyfwJWxCipeRcrdrqls4IAiVRIUEQnZj6KGW4E1I+na4J6UVTZ03F5EREQGV59ObTnn1jrnznbOjXbOjXHOnYu/6fCIUhL0ZV+tQhciIvFgLfC6c+5vzrlXgEozKwk3pPD5Eu31SrBEREKyN30H/n3AohgiOm42rARLRCQePAm0R423EXVLkZFqa20TDc1tHfdvFBGRwbU3CZYNWBRDRHpKIuNy0litSoIiIvEgyTnXHBkJhlNCjCcuRHpZqAVLRCQce5Ngud4XGX5KCjPUgiUiEh+2mtnZkREzOweoCDGeuLCm0le7VYIlIhKOHotcmFktXSdSBqTHJKI4V1KQyZ/f3Rx2GCIiAlcDD5vZncF4OXB5iPHEhdWV9SQnGhNGqUS7iEgYekywnHPZgxXIUFFSmMm2+maqd7SQm65S7SIiYXHOfQgcYWZZgDnnasOOKR6sqaxnUp5KtIuIhEV73z1U0lGqXd0ERUTCZGbfNbNRzrk651ytmeWZ2XfCjitsqysaOqreiojI4FOCtYdKVapdRCRenO6cq4qMOOe2A2eEGE/oIiXaiwtUQVBEJCxKsPZQ5KBVVtEQciQiIiNeopmlRkbMLB1I7WH5YS9Sor1ULVgiIqHp8Ros2V1aciITctMoUxdBEZGwPQTMN7NfB+NXAg+EGE/oyoIKgsWqICgiEholWP1QUpipLoIiIiFzzv3AzN4BTsZXt/0TUBxuVOGK3EakVAmWiEho1EWwH0oKM9WCJSISHzYB7cAFwEnAe+GGE66yynqSElSiXUQkTGrB6ofSgkyqGlqoamhmVEZK2OGIiIwoZjYNuAi4GKgEHseXaT8h1MDiQFllPZPzVaJdRCRM2gP3Q4kqCYqIhGk5vrXq4865Y5xz/wu0hRxTXCiraFAFQRGRkCnB6ofSwqCSoLoJioiE4QJ818CXzOwXZnYS/hqsEc05R1llve6BJSISMiVY/TAxLwMzWLa+JuxQRERGHOfcM865TwHTgQXADcBYM7vbzE4JNbgQba3zJdpLVOBCRCRUSrD6IS05kdNmjuP+f5axaM22sMMRERmRnHP1zrmHnXNnAROBxcBNIYcVmsj9GdWCJSISLiVY/fS9Cw6kKC+dax5+k621TWGHIyIyojnntjnn/kbwiggAACAASURBVM85d2Jvy5rZaWb2vpmtNLPdEjIzKzaz+Wb2jpktMLOJUfOuMLMVweOKgX4feyPSbb1E12CJiIRKCVY/5aYnc/elh1C9o4UvPfomrW3tYYckIiK9MLNE4C7gdGAGcLGZzei02I+AB51zBwK3ArcH6+YD3wQOBw4DvmlmeYMVe2/KKnyJ9qJR6WGHIiIyoinB2gszJuTw3fNm8dqqbfzwhffDDkdERHp3GLDSObfKOdcMPAac02mZGcD8YPilqPmnAi/+f3t3Hl9lfeb//3Wdk5OcLGSHENYEAZVNQERRq1amFjdcfk7Fsda2jsuodeky2up0OrXz7T5THRWr1VIoaq0t6li07jqiooAICAoCAcOaBQLZyPb5/XHfWQgJhHCSc054Px+P+3Hu9Zwrd07OJ9f53Pf18XvLdgEvAzN6IeYu2VRWzVCVaBcRiTp9Ch+hSycP4aunDOO3b23ghZXboh2OiIgc3GDg8zbLxf66tj7Cq1QIcAnQz8xyungsAGZ2nZktMbMlJSUlEQn8UDaWVunyQBGRGKAEKwL+7YIxTByayfeeXsH6kspohyMiIp3rqJy7a7f8XeBMM/sQOBPYAjR08VhvpXMPO+emOOem9O/f/0ji7RLnHJvKqhiuCoIiIlGnBCsCkhKCPHjlZBITAtwwbylV+xqiHZKIiHSsGBjaZnkIsLXtDs65rc65S51zk4C7/HUVXTk2Wkoq91FV10ihKgiKiESdEqwIGZSZzP9cMYn1JZXc8ZcVONfhl5oiIhJdHwCjzKzQzBKBWcBzbXcws1wza24fvw885s//HTjHzLL84hbn+OuiblOZV6J9uC4RFBGJOiVYEXTayFy+++VjeX7FNn6/qCja4YiISDvOuQbgZrzEaA3wlHPuYzP7sZnN9Hc7C/jUzNYCecB/+seWA/fgJWkfAD/210XdxlKvRLt6sEREoi+hJ5/czGYA9wJB4HfOuZ+12/5t4J/xrm0vAb7pnNvUkzH1tH858xg+3Lyb/7dwDeOHZHBSQXa0QxIRkTaccwuBhe3W/bDN/NPA050c+xitPVoxY1OZSrSLiMSKHuvB6uJYIx8CU/yxRp4GftFT8fQWM+PXXzmBIVnJ3DR/GTv31kY7JBER6eOKSlWiXUQkVvTkJ/Ehxxpxzr3unKv2F9/Du2E47qWHQzx01Ynsqa3n5sc/pF6DEIuISA8qKqvS/VciIjGiJxOsLo8X4rsGeKGjDdEYT+RIHTcwnZ9dOoH3N5bz8xc+iXY4IiLSRznnKCqtokAl2kVEYkJPJlhdHi/EzL4KTAF+2dH23h5PJFIunjSYq6cN53dvb+RvKzQIsYiIRF5pZR1VdY0aZFhEJEb0ZILVpfFCzOwf8MYZmemc29eD8UTFXeePYfKwTL739Ed8tnNvtMMREZE+pqjMqyBYoAqCIiIxoScTrK6MNTIJ+C1ecrWzB2OJmsSEAA9cOZmUxCDXz1tKpQYhFhGRCCryS7TrEkERkdjQYwlWF8ca+SWQBvzZzJab2XOdPF1cy89I5r4rJrGxtIo7ntYgxCIiEjlFfon2IVkq0S4iEgt6dBysLow18g89+fqx5NRjcrljxnH89IVPmPR2Jv/8hRHRDklERPqAorJqhmQlq0S7iEiM0KdxL7rujBHMGDuQn77wCYs3lEU7HBER6QOKSqt0/5WISAw5+hIs56CxPiovbWb88h8nMDw7hZse/5AdezQIsYiIdJ9zjk1l1br/SkQkhhxdCVZTE/z1WvjfW71EKwr6+YMQV+1r4Kb5yzQIsYiIdFtpZR2V+xpUol1EJIYcXQlWIADZI2D5fHjvwaiFMTqvHz+/bAJLNu3ipws1CLGIiHTPJr9E+3BdIigiEjN6tMhFTDrzTti5Gl66G/ofCyOjU2dj5gmD+HDzLh5btJGslBA3fXEkgUBHYzOLiIh0bKNfor1QlwiKiMSMo6sHC7xerIsfggFj4M/fhNJ1UQvlB+cdz8wTBvHrl9dy7dwlVFRH594wERGJT5vKqgkGjMEq0S4iEjOOvgQLICkNrngCgiF4YhbU7IpKGKFggHtnTeQ/Zo7lrXUlXHD//7FqS0VUYhERkfizsayKoVnJhFSiXUQkZhy9n8iZw+DyebBrEzx9DTQ2RCUMM+PqUwv40/XTaGh0XDr7Hf70weaoxCIiIvFlU1kVw3V5oIhITDl6EyyA4afC+b+G9a/CK/8e1VAmD8vi+W+dztSCbO74y0r+9emPqK1vjGpMIiISu5xzFJVWU6gCFyIiMeXoTrAATrwapl4P794PH86Paig5aUn84ZtTueXskTy1pJhLH3ynpUKUiIhIW2VVXon24SrRLiISU5RgAXz5/8GIs+D522Dz4qiGEgwY3z7nWB77+hS27K7hgv95m5dX74hqTCIiEnuK/AqCBerBEhGJKUqwAIIJcNnvIWMI/OmrUFEc7Yg4+7g8nv/W6RTkpHLt3CX84sVPaNCgxCIi4isqqwagQPdgiYjEFCVYzVKy4Yonob4GnrgC6qqjHRFDs1P48w3TuGLqMB58Yz1XPfo+JXv3RTssERGJAUWlVQQDxhCVaBcRiSlKsNrqfyxc9hhsXwnP3gjORTsiwqEgP710PL+8bALLNu/igv/5P5YUlUc7LBERibKisiqGqES7iEjM0adye6PPgS/9B3y8AN76VbSjafGPU4ay4MbTCIeCzHr4PR59eyMuBhJAERGJjqKyKl0eKCISg5RgdeTUW2DCLHj9J7Dmf6MdTYsxg9J57ubTOfu4Adzz/GpufvxDKvdFZ/wuEZF4ZWYzzOxTM/vMzO7sYPswM3vdzD40sxVmdp6/PmRmfzCzlWa2xsy+3/vRe5xzbCqtpkAVBEVEYo4SrI6YwYX3wuAT4a/Xw/ZV0Y6oRUZyiN9edSLfP/c4Xli1jZn3v83aHXujHZaISFwwsyDwAHAuMAa4wszGtNvtbuAp59wkYBbwoL/+H4Ek59x44ETgejMr6I242yurqmPvvgZVEBQRiUFKsDoTCsOsxyGc7hW9qCqNdkQtzIzrzzyG+f98CntqGrjo/kU88+EWXTIoInJoU4HPnHMbnHN1wJPARe32cUC6P58BbG2zPtXMEoBkoA7Y0/MhH6h5jERdIigiEnuUYB1Mv4Ewaz5U7YSnvgYNddGOaD/Tjslh4S2nM25wOrf9aTkXP7CIF1Zuo7FJiZaISCcGA5+3WS7217X1I+CrZlYMLAS+5a9/GqgCtgGbgV8556JSdWhjqV+iXT1YIiIxRwnWoQw+EWbeD5sWwQvfi4nKgm0NSA/z+LWn8JOLx7G7pp5/mb+M6b9+g/mLN1Fb3xjt8EREYo11sK79B/sVwBzn3BDgPGCemQXwer8agUFAIfAdMxvR4YuYXWdmS8xsSUlJSeSi920qU4l2EZFYpQSrKyb8I5z+bVg6Bz74XbSjOUAoGOCrpwznte+cxYNXTiY9OcRdC1Zx+s9f44HXP6Oiuj7aIYqIxIpiYGib5SG0XgLY7BrgKQDn3LtAGMgF/gl40TlX75zbCSwCpnT0Is65h51zU5xzU/r37x/hHwE2lqpEu4hIrNInc1ed/W8w+lx44Q7Y8Ga0o+lQMGCcNz6fZ286jcevPZmxgzL45d8/ZdrPXuWe51ezdXdNtEMUEYm2D4BRZlZoZol4RSyea7fPZmA6gJkdj5dglfjrzzZPKnAK8EmvRd7GprJqhuv+KxGRmKQEq6sCAbj0YcgdDX++Gso3RDuiTpkZpx6Tyx++OZWFt3yBL48dyJx3ijjjF6/z7aeW8+l2VR0UkaOTc64BuBn4O7AGr1rgx2b2YzOb6e/2HeBaM/sIeAL4uvOqCD0ApAGr8BK13zvnVkThZ6CotIpClWgXEYlJCdEOIK6E0+GKJ+CRL3qVBa952VsXw8YMSue/L5/Id84ZzaNvb+TJ9z/nr8u28MVj+3P9mcdwcmE2Zh3dkiAi0jc55xbiFa9ou+6HbeZXA6d1cFwlXqn2qCr3S7SrB0tEJDapB+twZRfCV+ZC6Tp46DT46Eloiv1iEkOyUvj3C8fyzp1n850vjWZFcQWzHn6PSx58hxdXqfKgiEi8KPJLtBeqgqCISExSgtUdhWfA156F5CxYcD089AX49MWYqzDYkazURL41fRSL7jybey4eR3lVHTf8cRlf+q83eeL9zao8KCIS44r8Eu3DdYmgiEhMUoLVXYVfgGvfgMt+Dw018MTl8PtzYfN70Y6sS8KhIFedMpzXv3sW9//TJFKTEvj+X1cy7aev8v2/rmTRZ6U0NDZFO0wREWmnqKVEuxIsEZFYpHuwjkQgAOMuheMvhA/nwRs/h8e+7FUbnP5vkDc22hEeUjBgXDBhEOePz+ed9WU8+cHnPLt8C0+8v5mc1ERmjBvI+RPyObkwh2BA92qJiERbUVk1gzOTSUzQd6QiIrFICVYkBEMw5ZswYRYsfgje/g3MPg1OmAVnfR+yhkc7wkMyM04bmctpI3OpqWvkzbU7eX7FNv66bAvzF28mN81PtsYPYmphtpItEZEoKSqtokD3X4nIQdTX11NcXExtbW20Q4lL4XCYIUOGEAqFunW8EqxISkyBL3wbTvw6LPoNLP4trHwaTroGvvBdSIv8YJM9ITkxyIxx+cwYl09NXSOvf7qTv63cxl+WbuGP720mNy2J88YP5Pzx+UwpULIlItJbnHMUlVUxaVhmtEMRkRhWXFxMv379KCgoULXow+Sco6ysjOLiYgoLC7v1HEqwekJKNnzpxzD1enjz5/D+I/DhH2HazXDqzZDUL9oRdllyYpDzxudz3vh8qusaeP2TEv62citPLfmcue9uYkC/JM4dN5DzJwxiyvAsAkq2RER6THlVHXtrGyhQiXYROYja2lolV91kZuTk5FBSUtLt51CC1ZMyBsPM+7zE6rV74M2fwQePwBnf8y4pTEiKdoSHJSUxgfMn5HP+hHyq9jXw2ic7+duKbTz5wef84d1N5KUnce64fC6YkM/kYUq2REQirajMqyBYkKsCFyJycEquuu9Iz50SrN7QfzRcPg+2LIVXfgQv3gnvPghf/AFM+AoEgtGO8LClJiVw4QmDuPCEQVTta+DVT3bytxVbefz9zcx5p4js1ESmFmRz8ohsphZmc/zAdCVcIiJHqKjUGwNLPVgiIrFLCVZvGnwifO052PC6l2g9cwO8cx9MvBJGnQO5oyAOv21ITUpg5gmDmHnCICr3NfDqmh28tbaUxRvLePHj7QCkhxOYWuglWycX5jB2UDoJQVXAEhE5HJvKqggYKtEuIhLDlGD1NjM45mwoPAtWPwP/92t46S5vyirwEq1R50DB6RBKjna0hy0tKYGLJg7moomDAdiyu4b3N5axeEM5728s55U1OwFITQxyYkE2Jxd604QhmSo5LCJyCBvLqhmSlaLPSxGJabt37+bxxx/nxhtvPKzjzjvvPB5//HEyM+O7kI8SrGhpHkNr3KWwezOse9mbPvwjvP8wJISh8Aw/4fqSl3zFocGZyVwyaQiXTBoCwM49tSze6CVbizeW8cu/fwpAOBRg0tAsTh7h9XBNGpZJOBR/l06KiPSkTWVVDM9R75WIdN1//O/HrN66J6LPOWZQOv9+Yefjve7evZsHH3zwgASrsbGRYLDz/+8WLlwYsRijSQlWLMgc5pVyP+kaqK+FTYv8hOslWPddb5/c0a3J1rBTISExujF304D0cMu9W+BVxGpOtt7fWM69r67DuXUkBgOcMDSDcYMzGDkgjWP6e1NuWqJu2hSRo5Jzjo2lVVwyaXC0QxEROag777yT9evXM3HiREKhEGlpaeTn57N8+XJWr17NxRdfzOeff05tbS233nor1113HQAFBQUsWbKEyspKzj33XE4//XTeeecdBg8ezLPPPktycsdXdz3yyCM8/PDD1NXVMXLkSObNm0dKSgo7duzghhtuYMOGDQDMnj2bU089lblz5/KrX/0KM2PChAnMmzcvoj+/Oeci+oQ9bcqUKW7JkiXRDqP3lK1vTbaK3obGfZCYBiPO8pKtkV/yqhX2ERU19SzdVM7iDeUs3ljOp9v3UlPf2LI9IznEMf1TvYRrQBoj/cehWcm6p0skzpjZUufclGjH0dMi1W6VV9Ux+Z6X+bcLxnDN6d0bm0VEjg5r1qzh+OOPj9rrFxUVccEFF7Bq1SreeOMNzj//fFatWtUyrlR5eTnZ2dnU1NRw0kkn8eabb5KTk7NfgjVy5EiWLFnCxIkT+cpXvsLMmTP56le/2uHrlZWVkZOTA8Ddd99NXl4e3/rWt7j88suZNm0at912G42NjVRWVlJcXMyll17KokWLyM3NbYmlvY7OYVfbLfVgxbqcY7zplBugrgo2/p/fs/USfPK8t0/eOC/hSsuDcEYHUyaE0yHYvdGoe1NGcoizj8vj7OPyAGhqcmzbU8v6nZWsL/Gmz3ZW8sbaEv68tLjluFDQKMhJbe3tGpDa0uuVmqS3uYjEv41+BcFClWgXkTgzderU/Qbtve+++1iwYAEAn3/+OevWrWtJkJoVFhYyceJEAE488USKioo6ff5Vq1Zx9913s3v3biorK/nyl78MwGuvvcbcuXMBCAaDZGRkMHfuXC677DJyc3MBOkyujlSP/udpZjOAe4Eg8Dvn3M/abT8D+A0wAZjlnHu6J+OJe4mpcOwMb3IOSj5tTbYW/xaa6g9+fCjVS7Q6TML8KSHZ6yVr8Ke28+2XO9xWBw21kJwJ+RNh0CRvyp/QrQGWAwFjcGYygzOTOWN0//22VdTUs8FPuNaXVLG+pJJPd+zlpdU7aGxq7ZnNzwgztTCbLx47gDNG9yc7NT4vrxSRo9umMi/BGq4S7SISZ1JTWz+33njjDV555RXeffddUlJSOOuss6itrT3gmKSk1vFig8EgNTU1nT7/17/+dZ555hlOOOEE5syZwxtvvNHpvs65Hr/dpMcSLDMLAg8AXwKKgQ/M7Dnn3Oo2u20Gvg58t6fi6LPMYMBx3nTaLV7CVVcJtRVdnyp3Qum61mXXeODrJIQhmOTd85UQhqD/mJDorw97iVnL+iRvvqoENr8Hq5pzZvPuI2tOuAZNhIHjvaSxmzKSQ0walsWkYVn7ra9raGJzeRWf7fSTru17eXtdKc8u34oZTByayVmjB/DF4/ozblCGxucSkbhQVOqVaB+qEu0iEuP69evH3r17O9xWUVFBVlYWKSkpfPLJJ7z33ntH/Hp79+4lPz+f+vp65s+fz+DB3u0z06dPZ/bs2S2XCFZVVTF9+nQuueQSbr/9dnJycjq9RPBI9GQP1lTgM+fcBgAzexK4CGhJsJxzRf62ph6M4+hg5vUQJfWDjCGHf7xz3iWIDbWtyVIwdOTjclXuhK3LYeuH3rThdVjxpB9zAPof1ybpmuRd7hgKH9FLJiYEGDmgHyMHtPaYNTU5Vm6p4PVPd/LGpyX85tW1/Pcra8lNS+SM0f293q1R/clIif3LKCXOOQeuCZoavUfX5H250bLO7b+u/b7Nz4Hr/mNTk9fj3dTgT41t5hugsWH/5ZZ92h0z9TpIifylFdKxorJqBmclq0S7iMS8nJwcTjvtNMaNG0dycjJ5eXkt22bMmMFDDz3EhAkTOPbYYznllFOO+PXuueceTj75ZIYPH8748eNbkrt7772X6667jkcffZRgMMjs2bOZNm0ad911F2eeeSbBYJBJkyYxZ86cI46hrR4rcmFmlwEznHP/7C9fBZzsnLu5g33nAM935RLBo67IRV+0Z1trwtU8VZd62wIJMOB4/7LCiV55+pRsSM6C5GwvgYxAt25Z5T7eWlfCG5+W8ObaEnZX1xMwmDwsiy8eN4Czju3PmPx0VSw8FOegsd77xzshDIEolNZvaoJ9FVBd7k015dBYBzmjvPsXe/vew5pd3nt6y7LWx6qS1qSpL7l5KeSO7PbhKnJxeGbe/zYZySHmXXNyBKISkb4s2kUu+oJYLXLR0X+m3crmzOw64DqAYcOGHUlMEgvS873puPO8Zedgz5b9E641/wvL5h54bCDBT7b8hCs5q00CltVuuc16s/2+qc9pauCSwiYuGZ5B49mpfLJ1F0s37mRZ0ae8/NIHvPZSI7kpQU4cmsakwf0Ym59CStD595vVQn2N99hQ65XW32++xtvvgH389QCp/aHfQG9KGwj98qBfvleopN9ASB0AwQj9eTY1Qe1uPwEphapSqC7z5qvLvUtLG+q8e+ga61rnO13nTw3+trZ/1s33+SWld/KY4SXJnW1LTIHaPV6S1JwsVZd78daUQ/WuA9fV7Grt2WkvmOhdmjrgeBgwxpvyxkDG0Igk6tTXwLYVsHUZbFnqJVPl61u3Zx8DBad5vcoW9HptA/6jWeu6/dYHDr4O82O31p9hv+UuPAYSvOcOJHgJaNvlTqc224MhL/aAelJ6S3OJ9osn9p2qsSIifVVPJljFwNA2y0OArd15Iufcw8DD4H0TeOShSUwx8/4BzRgCx1/orXPOG4B5z1bvH+jmf6Sry/dfriiG7Su95frqbr18EBjrT18DaL6nshEo8qeDaLIgjYEkmoJJNAXDuAT/3rRQMoGEMJaYQTA1j0BiCoFQ2MtHKnd4SeWWpV7Cc8B3D+YnYXl+AtacjPmJWL+BXvJQXeZN+yVNZVBVtn8S1VnPSSjFK/ufkOT909x8v10wyVsXyvQvGU3cf9sB6xK9ZKN2j9ebtG+vN1+72/s97tvjLTd0foPqISWEvaS5OYHOG+vPZ+//mJLjJSKla2Hnati5Bja9Cyv/3Ppcif38pOv41qRrwBhIze389RsboGRNayK1dRnsWN16bvsNgsGTYeI/eY+DJnlxikTArup69tY2UJCrAhcicvS66aabWLRo0X7rbr31Vr7xjW9EKaKO9WSC9QEwyswKgS3ALOCfevD1pC8xg6zh3tRV9bV+8tUuIavd7W3v6Jv4Q3xb30CAtSW1LNm8h5XbKtlVF6SiPsju+iC76oPsrgvQcBh/RuFQgJTEBJJDQZITgySHg6T1c+QF9jAwsJv+7CbXlZPVVE5mUznpdaWk7Sgm9fPlhPeVYRzsdkVrTTBScr3L44ad3LqckgOpbeZTcrweo97UWN+ahNXu8RKv5mRs3x6vNy0pvYPEqRuxDp68/3JthZdsNSddO1bDmudg2R9a90kd4CVdeWO9x4RwazK1bUVrghjO9BKo02/3k6nJXq+sSA8p8isIFuSowIWIHL0eeOCBaIfQJT2WYDnnGszsZuDveJ0EjznnPjazHwNLnHPPmdlJwAIgC7jQzP7DOTe2p2KSPi4UhlB+RP/RTQDGFMKYqR1vb2py1DY0UrWvkZq6RqrqGqiua6S6zWPbba2PTdTWe9tr6hvZWJfB6vo0quvy/fWN1NQ30vYWyQBN5FDBANtNnu0iRAN7Axmk5wwkd8AghgwaxKiBGYzO68fgzOTYrI4YDHlJXmrOofeNtHAGDDvFm5o55xVi2flxa/K1YzUsndPaI5qQDPknwJRvwOATvcQqe0RkLjEU6aIifwws9WCJiMS+Hh0Hyzm3EFjYbt0P28x/gHfpoEhcCgSMlMQEUhIj/6fknGNfQxM1frJVXde4X/K1o6KWtTv2snZnJa9s2sv2letajk1JDDJqQBqj8voxOi+N0Xn9GJ3Xj/yMsAp3tGXm3/+WB8ec3bq+qQl2b/Iue8wdHbn74SQmdGGMxmHAH4BMf587/fYMM5sA/BZIB5qAk5xzBw7gEmFFZdUq0S4iEif0X4NIjDIzwqEg4VCQrtzJU1FTz2c797J2hzf217qde3lzbQlPLy1u2adfUgKj/IRrVF4/js3rR05aIsGAETAjGDCCZgQCtJk3EgLeY9DfJ9DySN9M2AIByC489H4Sd7o4RuPdwFPOudlmNgbvi8ICM0sA/ghc5Zz7yMxygEOM8B4ZRaVVKtEuIhInlGCJ9BEZySFOHJ7NicP3H5doV1VdS0/Xuh17+XT7Xl5avYMnP/g8Iq8bMEgIBBiQnkRBTirDclIoyElhWHYqw3NSGJ6T0iM9fCLddMgxGvGqzqT78xm0Fmg6B1jhnPsIwDlX1isR492DVZCjywNFROKB/usR6eOyUhM5eUQOJ4/Y/76n0sp9rN2+l4qaehqdo7HJ0eQcjU3evWXN65qnpuZl57ztTbTM1zc1sb2ilqKyal5YuY1d1ft/qd+/X1JL0lWQk+InYV4ClpmS2JunQ2Qw0PbbhWKg/cBSPwJeMrNvAanAP/jrRwPOzP4O9AeedM79omfDVYl2Een70tLSqKysjHYYEaMES+QolZuWRO7IpEPv2A0VNfVsLqumqKyKzeXVFJVWsam8mrc/K+Evy/btt29GcojhOSkMy/Z6uwZlJjMoI5mBGWHyM8JkJIf65mWIEi1dGaPxCmCOc+7XZjYNmGdm4/DazNOBk4Bq4FV/0MlXD3iRCI7f2FyifbgqCIpId7xwpzekTSQNHA/n/uzQ+x2llGCJSMRlJIcYPySD8UMyDthWU9fI57u8pGtzuZeEbSqrZkVxBS+s2k5j0/7/6yaHguRnhP2EK7llflBmmIHp3nJmipIw6bKujNF4DTADwDn3rpmFgVz/2Dedc6UAZrYQmAwckGBFcvzG5hLthaogKCJx4o477mD48OHceOONAPzoRz/CzHjrrbfYtWsX9fX1/OQnP+Giiy465HNVVlZy0UUXdXjc3Llz+dWvfoWZMWHCBObNm8eOHTu44YYb2LBhAwCzZ8/m1FNP7bkftgNKsESkVyUnBluqGrbX0NhESeU+tlXUsr2ilq27a9heUcu2ilq2VdTw7vpSduzdd0ASFg4FyM9IZmB6mPzMMDmpiTQ2QUNTE/WNTdQ3Ohr8R2+5iYYmR12D99jQ2ERdyz7+/k1NJAQCDGxO7tLDrfMZYQZmJDOgXxKhoIoOxJmujNG4GZgOzDGz44EwUII32acHwAAADgtJREFU7Mi/mlkKUAecCfx3TwfcXKJ9uO7BEpHuiEJP06xZs7jttttaEqynnnqKF198kdtvv5309HRKS0s55ZRTmDlz5iG/IA2HwyxYsOCA41avXs1//ud/smjRInJzcykvLwfglltu4cwzz2TBggU0NjZG5dJDJVgiEjMSggG/lyq5030amxwle/exraLGT7xq2V5Rw1Y/KVu8oZyyqn0kBAKEgkZCMEBiMEBC0KuGGAoG/MnbFg4FCIUTSAgESEwwEgLevonBAPsavHvLVm/dw6trdlBbv/9Az2bepZb5GWHy0sMtvWsD01t73Aamh0lODEb0PDWX8G8u219d5421Vl3XQHV9I7X+uur6Rmr8Mdlq/HV1DU3kZ4YZ0T+NEbmpjOifelQVIenKGI3Ad4BHzOx2vMsHv+6cc8AuM/svvCTNAQudc3/r6ZhbSrRnd/53ISISSyZNmsTOnTvZunUrJSUlZGVlkZ+fz+23385bb71FIBBgy5Yt7Nixg4EDBx70uZxz/OAHPzjguNdee43LLruM3NxcALKzvSJfr732GnPnzgUgGAySkXHg1TQ97ehpVUWkTwgGrKUnaVIvvq5zjj01DWzb4/WqbW9J7mrZvqeWzWXVLN5Qxp7ahgOOTfBL22Ne1cWAGYb/6Je6D7R7bN7ethR+XWPruGjte/EOJTEhQEpikIRAgLKqffsNYp2fEabQT7ZG5KYxon8qx/RPY1BmMsFYHLD6CHVhjMbVwGmdHPtHvFLtvaaotIpBmckkJUQ2URcR6UmXXXYZTz/9NNu3b2fWrFnMnz+fkpISli5dSigUoqCggNraQw8j2NlxzrmYvT1ACZaISBeYGRkpITJSQhw3ML3T/arrGloSsO17vCSsal8DDmhyDpz32OTA+fPOuZbt3jovoXNt98WRlBAgOZRAcmKAlMQEkkNBUhKDJCcGW5a9+db1ySFvSmhzKWNtfSMbS6vYUFLFxtJKNpRUsb60imeXb2VvmwQxMSFAQU5KS9I1or//mJuq6o+9aFNZle6/EpG4M2vWLK699lpKS0t58803eeqppxgwYAChUIjXX3+dTZs2del5KioqOjxu+vTpXHLJJdx+++3k5ORQXl5OdnY206dPZ/bs2dx22200NjZSVVVFenrn7XZPUIIlIhJBKYkJfiKSFu1QOhUOBTk+P53j8/dvcJxzlFXVsaGkig0llWwsrWJ9SRVrd+7llTU7aGjTa5admshT109j5IDY/Tn7guYS7TMnDop2KCIih2Xs2LHs3buXwYMHk5+fz5VXXsmFF17IlClTmDhxIscdd1yXnqez48aOHctdd93FmWeeSTAYZNKkScyZM4d7772X6667jkcffZRgMMjs2bOZNm1aT/6oB1CCJSIigNdLl5uWRG5aElML9x+wur6xieJdNWwo8Xq8NpRWMjAjHKVIjx4NTY5ZU4dxUkH2oXcWEYkxK1e2lofPzc3l3Xff7XC/gxWiONhxV199NVdfffV+6/Ly8nj22We7EW3kKMESEZFDCgUDFOamUpibyvTjox3N0SMUDPCD83TCRUTiiRIsERERERGJqpUrV3LVVVftty4pKYnFixdHKaLuU4IlIiIiItLHxHKVvY6MHz+e5cuXRzsMwDt3R0IjZIqIiIiI9CHhcJiysrIjThSORs45ysrKCIe7f5+xerBERERERPqQIUOGUFxcTElJSbRDiUvhcJghQ4Z0+3glWCIiIiIifUgoFKKwsDDaYRy1dImgiIiIiIhIhCjBEhERERERiRAlWCIiIiIiIhFi8VZdxMxKgE1H+DS5QGkEwulN8RZzvMUL8RdzvMUL8RdzvMUL8RXzcOdc/2gH0dOO0nYr3uKF+Is53uKF+Is53uKF+Is53uLtUrsVdwlWJJjZEufclGjHcTjiLeZ4ixfiL+Z4ixfiL+Z4ixfiM2Y5tHj7vcZbvBB/McdbvBB/McdbvBB/McdbvF2lSwRFREREREQiRAmWiIiIiIhIhBytCdbD0Q6gG+It5niLF+Iv5niLF+Iv5niLF+IzZjm0ePu9xlu8EH8xx1u8EH8xx1u8EH8xx1u8XXJU3oMlIiIiIiLSE47WHiwREREREZGIU4IlIiIiIiISIX06wTKzGWb2qZl9ZmZ3drA9ycz+5G9fbGYFvR9lSyxDzex1M1tjZh+b2a0d7HOWmVWY2XJ/+mE0Ym0XU5GZrfTjWdLBdjOz+/xzvMLMJkcjTj+WY9ucu+VmtsfMbmu3T9TPsZk9ZmY7zWxVm3XZZvayma3zH7M6OfZqf591ZnZ1lGP+pZl94v/eF5hZZifHHvQ91Ivx/sjMtrT53Z/XybEH/Vzp5Zj/1CbeIjNb3smxvX6OpXvUbvU8tVs9EmdctVvx1mYdJOaYbbeO+jbLOdcnJyAIrAdGAInAR8CYdvvcCDzkz88C/hTFePOByf58P2BtB/GeBTwf7XPbLqYiIPcg288DXgAMOAVYHO2Y27w/tuMNGBdT5xg4A5gMrGqz7hfAnf78ncDPOzguG9jgP2b581lRjPkcIMGf/3lHMXflPdSL8f4I+G4X3jcH/VzpzZjbbf818MNYOceauvU7VrvVO3Gr3Yp8bHHVbsVbm3WQmGO23Tra26y+3IM1FfjMObfBOVcHPAlc1G6fi4A/+PNPA9PNzHoxxhbOuW3OuWX+/F5gDTA4GrFE2EXAXOd5D8g0s/xoBwVMB9Y75zZFO5D2nHNvAeXtVrd9r/4BuLiDQ78MvOycK3fO7QJeBmb0WKBtdBSzc+4l51yDv/geMKQ3YumKTs5xV3Tlc6VHHCxm/3PrK8ATvRGL9Bi1W7FB7dZhird2K97aLIi/dutob7P6coI1GPi8zXIxB37wt+zj/1FVADm9Et1B+Jd8TAIWd7B5mpl9ZGYvmNnYXg2sYw54ycyWmtl1HWzvyu8hGmbR+R92rJ1jgDzn3Dbw/qkBBnSwT6yea4Bv4n0j3JFDvYd6083+5SGPdXI5S6ye4y8AO5xz6zrZHkvnWDqndqt3qN3qHfHcbsVLmwXx2W71+TarLydYHX2j174mfVf26VVmlgb8BbjNOben3eZleJcGnAD8D/BMb8fXgdOcc5OBc4GbzOyMdttj8RwnAjOBP3ewORbPcVfF3LkGMLO7gAZgfie7HOo91FtmA8cAE4FteJcvtBeT5xi4goN/Exgr51gOTu1W71C7FTti8VzHS5sF8dtu9fk2qy8nWMXA0DbLQ4Ctne1jZglABt3rfo0IMwvhNVLznXN/bb/dObfHOVfpzy8EQmaW28thto9pq/+4E1iA1xXdVld+D73tXGCZc25H+w2xeI59O5ovUfEfd3awT8yda/+G5QuAK51/YXV7XXgP9Qrn3A7nXKNzrgl4pJM4YvEcJwCXAn/qbJ9YOcdySGq3eoHarV4Td+1WPLVZfgxx124dLW1WX06wPgBGmVmh/83PLOC5dvs8BzRXrLkMeK2zP6ie5l+P+iiwxjn3X53sM7D5Wnszm4r3+yvrvSgPiCfVzPo1z+PdILqq3W7PAV8zzylARfMlA1HU6TcnsXaO22j7Xr0aeLaDff4OnGNmWf5lAuf466LCzGYAdwAznXPVnezTlfdQr2h3j8UlncTRlc+V3vYPwCfOueKONsbSOZZDUrvVw9Ru9aq4arfirc3yY4jHduvoaLMOtypGPE14lYDW4lVPuctf92O8Px6AMF53+2fA+8CIKMZ6Ol6X7QpguT+dB9wA3ODvczPwMV4FmPeAU6N8fkf4sXzkx9V8jtvGbMAD/u9gJTAlyjGn4DU8GW3WxdQ5xmtEtwH1eN88XYN3j8WrwDr/MdvfdwrwuzbHftN/P38GfCPKMX+Gd9138/u5ufLZIGDhwd5DUYp3nv8eXYHX+OS3j9dfPuBzJVox++vnNL9/2+wb9XOsqdu/Z7VbPRuz2q2eiTGu2q1O4o3ZNusgMcdsu9VRvP76ORwFbZb5P4yIiIiIiIgcob58iaCIiIiIiEivUoIlIiIiIiISIUqwREREREREIkQJloiIiIiISIQowRIREREREYkQJVgiEWZmjWa2vM10ZwSfu8DM4m88CBERiVlqt0QiKyHaAYj0QTXOuYnRDkJERKSL1G6JRJB6sER6iZkVmdnPzex9fxrprx9uZq+a2Qr/cZi/Ps/MFpjZR/50qv9UQTN7xMw+NrOXzCzZ3/8WM1vtP8+TUfoxRUSkj1C7JdI9SrBEIi+53aUWl7fZtsc5NxW4H/iNv+5+YK5zbgIwH7jPX38f8KZz7gRgMt6I5gCjgAecc2OB3cD/56+/E5jkP88NPfXDiYhIn6N2SySCzDkX7RhE+hQzq3TOpXWwvgg42zm3wcxCwHbnXI6ZlQL5zrl6f/0251yumZUAQ5xz+9o8RwHwsnNulL98BxByzv3EzF4EKoFngGecc5U9/KOKiEgfoHZLJLLUgyXSu1wn853t05F9beYbab2X8nzgAeBEYKmZ6R5LERE5Umq3RA6TEiyR3nV5m8d3/fl3gFn+/JXA2/78q8C/AJhZ0MzSO3tSMwsAQ51zrwP/CmQCB3wbKSIicpjUbokcJn1TIBJ5yWa2vM3yi8655pK3SWa2GO/LjSv8dbcAj5nZ94AS4Bv++luBh83sGrxv/P4F2NbJawaBP5pZBmDAfzvndkfsJxIRkb5M7ZZIBOkeLJFe4l/LPsU5VxrtWERERA5F7ZZI9+gSQRERERERkQhRD5aIiIiIiEiEqAdLREREREQkQpRgiYiIiIiIRIgSLBERERERkQhRgiUiIiIiIhIhSrBEREREREQi5P8HtefqaUyfS7MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d9c48b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_vis(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 42us/step\n",
      "0.0990454939289597 0.9777\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.5278 - acc: 0.8397 - val_loss: 0.1984 - val_acc: 0.9397\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2350 - acc: 0.9319 - val_loss: 0.1452 - val_acc: 0.9581\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1803 - acc: 0.9463 - val_loss: 0.1138 - val_acc: 0.9662\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1500 - acc: 0.9553 - val_loss: 0.1028 - val_acc: 0.9695\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1284 - acc: 0.9614 - val_loss: 0.0989 - val_acc: 0.9717\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1157 - acc: 0.9639 - val_loss: 0.0876 - val_acc: 0.9739\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1034 - acc: 0.9689 - val_loss: 0.0860 - val_acc: 0.9749\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0948 - acc: 0.9706 - val_loss: 0.0877 - val_acc: 0.9747\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0886 - acc: 0.9726 - val_loss: 0.0828 - val_acc: 0.9758\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0816 - acc: 0.9746 - val_loss: 0.0815 - val_acc: 0.9760\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0788 - acc: 0.9744 - val_loss: 0.0834 - val_acc: 0.9767\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0743 - acc: 0.9775 - val_loss: 0.0802 - val_acc: 0.9773\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0707 - acc: 0.9779 - val_loss: 0.0852 - val_acc: 0.9753\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0666 - acc: 0.9779 - val_loss: 0.0789 - val_acc: 0.9782\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0649 - acc: 0.9786 - val_loss: 0.0804 - val_acc: 0.9778\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0649 - acc: 0.9791 - val_loss: 0.0808 - val_acc: 0.9763\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0569 - acc: 0.9821 - val_loss: 0.0815 - val_acc: 0.9772\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0547 - acc: 0.9825 - val_loss: 0.0868 - val_acc: 0.9769\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.0549 - acc: 0.9816 - val_loss: 0.0863 - val_acc: 0.9756\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0549 - acc: 0.9823 - val_loss: 0.0818 - val_acc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER = Adam()\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "#model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = 20, verbose = VERBOSE, validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/step\n",
      "0.07139727771007093 0.9808\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(score[0],score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
